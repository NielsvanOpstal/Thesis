{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29739892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ec31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from get_data import get_data\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.tensorflow import balanced_batch_generator\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed = 31)\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce305bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import scipy.ndimage as ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3bf2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, ID = get_data(\"../Data/filled/grids/\", [2015,2016,2017,2018,2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d24f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658c7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w15.pickle', 'rb') as handle:\n",
    "    wt = pickle.load(handle)\n",
    "    \n",
    "with open('w16.pickle', 'rb') as handle:\n",
    "    wv = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0544fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN_samples(grid, block, dims = 39):\n",
    "    \n",
    "    nonzero = np.transpose(grid[:,:,-2].nonzero()) # Get indices of nonzero componetns\n",
    "\n",
    "    size = nonzero.shape[0]\n",
    "    width = block * 2 + 1 # calculate widht and height. Needed later on\n",
    "    \n",
    "    X = np.zeros((size, width, width, dims))\n",
    "    Y = np.zeros(size)\n",
    "    ID = np.zeros(size)\n",
    "    \n",
    "    for idx, i in enumerate(nonzero):\n",
    "        x, ID[idx], Y[idx] = get_neighbor_grid(grid, i, block)\n",
    "        X[idx] = x.reshape(width,width, 39)\n",
    "        \n",
    "    X = np.moveaxis(X, -1, 1) # order the indices correctly to make sure it works in CNN\n",
    "    X = torch.from_numpy(X).float()\n",
    "    Y = torch.from_numpy(Y).float()\n",
    "    \n",
    "    return X,ID,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37bf83dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_neighbor_grid(full, hw, block = 1):\n",
    "    \n",
    "    # get the nonzero (built) blocks by checking if they have a ID\n",
    "\n",
    "    h = hw[0]\n",
    "    w = hw[1]\n",
    "    \n",
    "    y = full[h,w,-1]\n",
    "    ID = full[h,w,-2]\n",
    "    \n",
    "    hu = h - block\n",
    "    hd = h + block\n",
    "    hshort, hextra, wshort, wextra = 0,0,0,0\n",
    "    if hu < 0:\n",
    "        hshort = 0 - hu\n",
    "        hu = 0\n",
    "    if hd >= full.shape[0]:\n",
    "        hextra = (hd - full.shape[0]) + 1\n",
    "        hd = full.shape[0]\n",
    "\n",
    "    wr = w + block\n",
    "    wl = w - block\n",
    "\n",
    "    if wr >= full.shape[1]:\n",
    "        wextra = (wr - full.shape[1]) + 1\n",
    "        wr = full.shape[1]\n",
    "    if wl < 0:\n",
    "        wshort = 0 - wl\n",
    "        wl = 0\n",
    "\n",
    "    nb = full[hu : hd + 1, wl : wr + 1, :]\n",
    "    nb = np.pad(nb, ((hshort, hextra), (wshort, wextra), (0,0)), mode = \"constant\", constant_values = 0)\n",
    "    return nb[:,:,:-2], ID, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f890120d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "ID_train = []\n",
    "for filename in os.listdir(\"../Data/filled/grids/2015/\"):\n",
    "    n = np.load(\"../Data/filled/grids/2015/\" + filename)\n",
    "    X, ID, Y = create_CNN_samples(n, 5)\n",
    "    X_train.append(X)\n",
    "    Y_train.append(Y)\n",
    "    ID_train.append(ID)\n",
    "    \n",
    "Y_train = np.concatenate(Y_train)\n",
    "ID_train = np.concatenate(ID_train)\n",
    "X_train = np.concatenate(X_train)\n",
    "\n",
    "X_train = np.moveaxis(X_train, 1, -1)\n",
    "X_train = X_train.reshape(-1, 39)\n",
    "\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_train = X_train.reshape(-1, 11, 11, 39)\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "oversampler = SMOTE()\n",
    "X_train_resamp, Y_train_resamp = oversampler.fit_resample(X_train, Y_train)\n",
    "X_train = X_train.reshape(-1, 11, 11, 39)\n",
    "X_train_resamp = X_train_resamp.reshape(-1, 11, 11, 39)\n",
    "\n",
    "X_train = np.moveaxis(X_train, -1, 1)\n",
    "X_train_resamp = np.moveaxis(X_train_resamp, -1, 1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_val = []\n",
    "Y_val = []\n",
    "ID_val = []\n",
    "\n",
    "for filename in os.listdir(\"../Data/filled/grids/2016/\"):\n",
    "    n = np.load(\"../Data/filled/grids/2016/\" + filename)\n",
    "    X, ID, Y = create_CNN_samples(n, 5)\n",
    "    X_val.append(X)\n",
    "    Y_val.append(Y)\n",
    "    ID_val.append(ID)\n",
    "    \n",
    "X_val = np.concatenate(X_val)\n",
    "X_val = np.moveaxis(X_val, 1, -1)\n",
    "X_val = X_val.reshape(-1, 39)\n",
    "\n",
    "X_val = ss.transform(X_val)\n",
    "\n",
    "\n",
    "X_val = X_val.reshape(-1, 11, 11, 39)\n",
    "X_val = np.moveaxis(X_val, -1, 1)\n",
    "Y_val = np.concatenate(Y_val)\n",
    "ID_val = np.concatenate(ID_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d315a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "import libpysal\n",
    "import geopandas as gpd\n",
    "\n",
    "def load_data(year):\n",
    "    os.getcwd()\n",
    "    df = pd.DataFrame()\n",
    "    path = \"../Data/filled/\" + str(year) + \"/\"\n",
    "    for filename in os.listdir(path):\n",
    "        df1 = pd.read_csv(path + filename)\n",
    "        if df1.geometry.isna().any():\n",
    "            print(filename)\n",
    "        df = pd.concat([df, df1])\n",
    "    \n",
    "    df = gpd.GeoDataFrame(df)\n",
    "    df.geometry = df.geometry.apply(shapely.wkt.loads)\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.drop([\"Unnamed: 0\", \"index\"], axis = 1)\n",
    "    return df\n",
    "\n",
    "df15 = load_data(2015)\n",
    "df16 = load_data(2016)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X15 = df15[df15.columns[:-4]]\n",
    "Y15 = df15.y\n",
    "\n",
    "X16 = df16[df16.columns[:-4]]\n",
    "Y16 = df16.y\n",
    "\n",
    "X15 = pd.DataFrame(scaler.fit_transform(X15))\n",
    "X16 = pd.DataFrame(scaler.transform(X16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ab3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y, batch_size = 32):\n",
    "    \n",
    "    idxs = rng.integers(X.shape[0], size = batch_size)\n",
    "    \n",
    "    X = torch.from_numpy(X[idxs]).float()\n",
    "    Y = torch.from_numpy(Y[idxs]).float()\n",
    "    \n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225c26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super(Model1, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 39, out_channels = 32, kernel_size = (3,3)), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (3,3)), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 8),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(8,1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "        self.name = name\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        out = self.final(out)\n",
    "        return out\n",
    "    \n",
    "    def get_no_activation(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3bc321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super(Model2, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 39, out_channels = 128, kernel_size = (3,3)), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size = (3,3)), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(576, 64),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "        self.name = name\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c333d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super(Model3, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 39, out_channels = 128, kernel_size = (3,3)), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size = (3,3)), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(576, 128),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "        self.name = name\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b5471cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super(Model4, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 39, out_channels = 64, kernel_size = (3,3)), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (3,3)), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 512),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid())\n",
    "            \n",
    "        self.name = name\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "216b1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model1(\"een\")\n",
    "model2 = Model2(\"twee\")\n",
    "model3 = Model3(\"drie\")\n",
    "model4 = Model4(\"vier\")\n",
    "models = [model1, model2, model3, model4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f836c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6219a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # optimizer = optim.RMSprop(model.parameters(), lr=0.001) \n",
    "# BCEloss = nn.BCELoss()\n",
    "# model.train()\n",
    "\n",
    "def train_model(model, X_train, Y_train, X_val, Y_val, num_epochs, batch_per_e = 500):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.0001) \n",
    "    BCEloss = nn.BCELoss()\n",
    "    train_loss = []\n",
    "    train_loss_history = []\n",
    "    acc_history = []\n",
    "    ROC_history = []\n",
    "    f1_score_history = []\n",
    "    cmc_best = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        print(\"epoch: {} of {}\".format(epoch, num_epochs))\n",
    "        for batch in range(batch_per_e):\n",
    "            \n",
    "            x_train_, y_train_ = get_batch(X_train, Y_train)\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train_)\n",
    "\n",
    "            loss = BCEloss(out.squeeze(), y_train_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 10 == 0:\n",
    "                train_loss.append(loss)\n",
    "                \n",
    "        model.eval()\n",
    "        \n",
    "        x_val_, y_val_ = get_batch(X_val, Y_val, batch_size = 1000)\n",
    "        predictions = model(x_val_).detach().numpy()\n",
    "        predictions = (predictions > 0.5).astype(int)\n",
    "        \n",
    "        acc = accuracy_score(y_val_, predictions)\n",
    "        ROC = roc_auc_score(y_val_, predictions)\n",
    "        f1 = f1_score(y_val_, predictions)\n",
    "        train_loss = (np.sum(train_loss) / (batch_per_e/10)).detach().item()\n",
    "\n",
    "        acc_history.append(acc)\n",
    "        ROC_history.append(ROC)\n",
    "        train_loss_history.append(train_loss)\n",
    "        f1_score_history.append(f1)\n",
    "        \n",
    "        \n",
    "        print(\"training_loss: {:.4f}, acc: {:.3f}, ROC: {:.3f}, f1: {:.3f}\".format(train_loss, acc, ROC , f1))\n",
    "        train_loss = []\n",
    "\n",
    "\n",
    "    return acc_history, ROC_history, train_loss_history, f1_score_history\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f6590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "588476f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_part(models, X, idxs, w, labels):\n",
    "    tot_preds = pd.DataFrame()\n",
    "    for i in labels15.index.unique():\n",
    "        x = X[labels.loc[i][\"X\"]]\n",
    "        IDs = idxs[labels.loc[i][\"X\"]]\n",
    "        preds = models[i](torch.from_numpy(x).float())\n",
    "        preds = pd.DataFrame(np.array(preds.detach())).set_index(IDs)\n",
    "        tot_preds = pd.concat([tot_preds, preds])\n",
    "    \n",
    "    \n",
    "    neighbors = [w.neighbors[x] for x in idxs]\n",
    "    transitions = [tot_preds.loc[x].values for x in neighbors]\n",
    "\n",
    "\n",
    "    n_function = np.zeros((len(tot_preds), w.max_neighbors + 1))\n",
    "    for i, (t, idx) in enumerate(zip(transitions, idxs)):\n",
    "        n_function[i, 1:len(t) + 1] = t.squeeze()\n",
    "        n_function[i, 0] = tot_preds.loc[idx]\n",
    "        \n",
    "    return n_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6e18ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Model1, Model2, Model3, Model4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4d825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9eef98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10784, 39, 11, 11)\n",
      "epoch: 0 of 250\n",
      "training_loss: 0.6071, acc: 0.637, ROC: 0.598, f1: 0.188\n",
      "epoch: 1 of 250\n",
      "training_loss: 0.5594, acc: 0.664, ROC: 0.491, f1: 0.116\n",
      "epoch: 2 of 250\n",
      "training_loss: 0.5088, acc: 0.746, ROC: 0.573, f1: 0.164\n",
      "epoch: 3 of 250\n",
      "training_loss: 0.4563, acc: 0.783, ROC: 0.496, f1: 0.100\n",
      "epoch: 4 of 250\n",
      "training_loss: 0.4515, acc: 0.773, ROC: 0.613, f1: 0.215\n",
      "epoch: 5 of 250\n",
      "training_loss: 0.4082, acc: 0.837, ROC: 0.548, f1: 0.155\n",
      "epoch: 6 of 250\n",
      "training_loss: 0.3781, acc: 0.787, ROC: 0.565, f1: 0.151\n",
      "epoch: 7 of 250\n",
      "training_loss: 0.3574, acc: 0.790, ROC: 0.579, f1: 0.198\n",
      "epoch: 8 of 250\n",
      "training_loss: 0.3500, acc: 0.827, ROC: 0.550, f1: 0.164\n",
      "epoch: 9 of 250\n",
      "training_loss: 0.3204, acc: 0.828, ROC: 0.531, f1: 0.149\n",
      "epoch: 10 of 250\n",
      "training_loss: 0.2779, acc: 0.835, ROC: 0.549, f1: 0.154\n",
      "epoch: 11 of 250\n",
      "training_loss: 0.2806, acc: 0.796, ROC: 0.579, f1: 0.177\n",
      "epoch: 12 of 250\n",
      "training_loss: 0.2487, acc: 0.884, ROC: 0.540, f1: 0.147\n",
      "epoch: 13 of 250\n",
      "training_loss: 0.2451, acc: 0.845, ROC: 0.511, f1: 0.104\n",
      "epoch: 14 of 250\n",
      "training_loss: 0.2226, acc: 0.824, ROC: 0.562, f1: 0.185\n",
      "epoch: 15 of 250\n",
      "training_loss: 0.2213, acc: 0.842, ROC: 0.508, f1: 0.102\n",
      "epoch: 16 of 250\n",
      "training_loss: 0.2145, acc: 0.882, ROC: 0.507, f1: 0.078\n",
      "epoch: 17 of 250\n",
      "training_loss: 0.2058, acc: 0.822, ROC: 0.525, f1: 0.136\n",
      "epoch: 18 of 250\n",
      "training_loss: 0.1951, acc: 0.842, ROC: 0.531, f1: 0.132\n",
      "epoch: 19 of 250\n",
      "training_loss: 0.1865, acc: 0.883, ROC: 0.501, f1: 0.064\n",
      "epoch: 20 of 250\n",
      "training_loss: 0.2059, acc: 0.824, ROC: 0.522, f1: 0.120\n",
      "epoch: 21 of 250\n",
      "training_loss: 0.1911, acc: 0.897, ROC: 0.555, f1: 0.163\n",
      "epoch: 22 of 250\n",
      "training_loss: 0.1619, acc: 0.840, ROC: 0.507, f1: 0.101\n",
      "epoch: 23 of 250\n",
      "training_loss: 0.1712, acc: 0.898, ROC: 0.498, f1: 0.038\n",
      "epoch: 24 of 250\n",
      "training_loss: 0.1616, acc: 0.868, ROC: 0.517, f1: 0.108\n",
      "epoch: 25 of 250\n",
      "training_loss: 0.1444, acc: 0.868, ROC: 0.519, f1: 0.108\n",
      "epoch: 26 of 250\n",
      "training_loss: 0.1433, acc: 0.898, ROC: 0.528, f1: 0.121\n",
      "epoch: 27 of 250\n",
      "training_loss: 0.1588, acc: 0.904, ROC: 0.549, f1: 0.172\n",
      "epoch: 28 of 250\n",
      "training_loss: 0.1448, acc: 0.876, ROC: 0.532, f1: 0.139\n",
      "epoch: 29 of 250\n",
      "training_loss: 0.1358, acc: 0.870, ROC: 0.571, f1: 0.188\n",
      "epoch: 30 of 250\n",
      "training_loss: 0.1177, acc: 0.832, ROC: 0.572, f1: 0.208\n",
      "epoch: 31 of 250\n",
      "training_loss: 0.1463, acc: 0.868, ROC: 0.513, f1: 0.096\n",
      "epoch: 32 of 250\n",
      "training_loss: 0.1197, acc: 0.821, ROC: 0.543, f1: 0.144\n",
      "epoch: 33 of 250\n",
      "training_loss: 0.1181, acc: 0.886, ROC: 0.539, f1: 0.149\n",
      "epoch: 34 of 250\n",
      "training_loss: 0.1143, acc: 0.883, ROC: 0.575, f1: 0.193\n",
      "epoch: 35 of 250\n",
      "training_loss: 0.1121, acc: 0.853, ROC: 0.530, f1: 0.130\n",
      "epoch: 36 of 250\n",
      "training_loss: 0.1280, acc: 0.893, ROC: 0.520, f1: 0.101\n",
      "epoch: 37 of 250\n",
      "training_loss: 0.1162, acc: 0.886, ROC: 0.581, f1: 0.230\n",
      "epoch: 38 of 250\n",
      "training_loss: 0.1000, acc: 0.820, ROC: 0.507, f1: 0.109\n",
      "epoch: 39 of 250\n",
      "training_loss: 0.1059, acc: 0.886, ROC: 0.509, f1: 0.081\n",
      "epoch: 40 of 250\n",
      "training_loss: 0.1095, acc: 0.897, ROC: 0.529, f1: 0.120\n",
      "epoch: 41 of 250\n",
      "training_loss: 0.1055, acc: 0.842, ROC: 0.508, f1: 0.102\n",
      "epoch: 42 of 250\n",
      "training_loss: 0.0875, acc: 0.881, ROC: 0.541, f1: 0.156\n",
      "epoch: 43 of 250\n",
      "training_loss: 0.0895, acc: 0.865, ROC: 0.538, f1: 0.151\n",
      "epoch: 44 of 250\n",
      "training_loss: 0.0900, acc: 0.859, ROC: 0.512, f1: 0.102\n",
      "epoch: 45 of 250\n",
      "training_loss: 0.0809, acc: 0.868, ROC: 0.539, f1: 0.154\n",
      "epoch: 46 of 250\n",
      "training_loss: 0.0854, acc: 0.879, ROC: 0.544, f1: 0.154\n",
      "epoch: 47 of 250\n",
      "training_loss: 0.0851, acc: 0.899, ROC: 0.544, f1: 0.151\n",
      "epoch: 48 of 250\n",
      "training_loss: 0.0819, acc: 0.853, ROC: 0.527, f1: 0.130\n",
      "epoch: 49 of 250\n",
      "training_loss: 0.0835, acc: 0.901, ROC: 0.531, f1: 0.124\n",
      "epoch: 50 of 250\n",
      "training_loss: 0.0847, acc: 0.899, ROC: 0.509, f1: 0.073\n",
      "epoch: 51 of 250\n",
      "training_loss: 0.1015, acc: 0.875, ROC: 0.533, f1: 0.138\n",
      "epoch: 52 of 250\n",
      "training_loss: 0.0734, acc: 0.895, ROC: 0.552, f1: 0.160\n",
      "epoch: 53 of 250\n",
      "training_loss: 0.0724, acc: 0.867, ROC: 0.517, f1: 0.107\n",
      "epoch: 54 of 250\n",
      "training_loss: 0.0642, acc: 0.871, ROC: 0.558, f1: 0.178\n",
      "epoch: 55 of 250\n",
      "training_loss: 0.0697, acc: 0.891, ROC: 0.530, f1: 0.128\n",
      "epoch: 56 of 250\n",
      "training_loss: 0.0770, acc: 0.905, ROC: 0.538, f1: 0.144\n",
      "epoch: 57 of 250\n",
      "training_loss: 0.0695, acc: 0.854, ROC: 0.531, f1: 0.141\n",
      "epoch: 58 of 250\n",
      "training_loss: 0.0643, acc: 0.884, ROC: 0.567, f1: 0.205\n",
      "epoch: 59 of 250\n",
      "training_loss: 0.0732, acc: 0.855, ROC: 0.546, f1: 0.152\n",
      "epoch: 60 of 250\n",
      "training_loss: 0.0628, acc: 0.877, ROC: 0.537, f1: 0.152\n",
      "epoch: 61 of 250\n",
      "training_loss: 0.0525, acc: 0.886, ROC: 0.509, f1: 0.081\n",
      "epoch: 62 of 250\n",
      "training_loss: 0.0630, acc: 0.913, ROC: 0.531, f1: 0.121\n",
      "epoch: 63 of 250\n",
      "training_loss: 0.0651, acc: 0.895, ROC: 0.535, f1: 0.132\n",
      "epoch: 64 of 250\n",
      "training_loss: 0.0587, acc: 0.887, ROC: 0.516, f1: 0.096\n",
      "epoch: 65 of 250\n",
      "training_loss: 0.0529, acc: 0.872, ROC: 0.489, f1: 0.045\n",
      "epoch: 66 of 250\n",
      "training_loss: 0.0554, acc: 0.859, ROC: 0.527, f1: 0.124\n",
      "epoch: 67 of 250\n",
      "training_loss: 0.0586, acc: 0.866, ROC: 0.505, f1: 0.082\n",
      "epoch: 68 of 250\n",
      "training_loss: 0.0677, acc: 0.887, ROC: 0.533, f1: 0.137\n",
      "epoch: 69 of 250\n",
      "training_loss: 0.0680, acc: 0.879, ROC: 0.524, f1: 0.117\n",
      "epoch: 70 of 250\n",
      "training_loss: 0.0510, acc: 0.857, ROC: 0.540, f1: 0.164\n",
      "epoch: 71 of 250\n",
      "training_loss: 0.0537, acc: 0.877, ROC: 0.551, f1: 0.174\n",
      "epoch: 72 of 250\n",
      "training_loss: 0.0536, acc: 0.902, ROC: 0.505, f1: 0.058\n",
      "epoch: 73 of 250\n",
      "training_loss: 0.0540, acc: 0.902, ROC: 0.565, f1: 0.183\n",
      "epoch: 74 of 250\n",
      "training_loss: 0.0462, acc: 0.862, ROC: 0.552, f1: 0.169\n",
      "epoch: 75 of 250\n",
      "training_loss: 0.0566, acc: 0.878, ROC: 0.558, f1: 0.187\n",
      "epoch: 76 of 250\n",
      "training_loss: 0.0558, acc: 0.891, ROC: 0.524, f1: 0.114\n",
      "epoch: 77 of 250\n",
      "training_loss: 0.0489, acc: 0.881, ROC: 0.560, f1: 0.179\n",
      "epoch: 78 of 250\n",
      "training_loss: 0.0489, acc: 0.903, ROC: 0.535, f1: 0.126\n",
      "epoch: 79 of 250\n",
      "training_loss: 0.0428, acc: 0.881, ROC: 0.557, f1: 0.179\n",
      "epoch: 80 of 250\n",
      "training_loss: 0.0458, acc: 0.884, ROC: 0.533, f1: 0.134\n",
      "epoch: 81 of 250\n",
      "training_loss: 0.0629, acc: 0.889, ROC: 0.556, f1: 0.165\n",
      "epoch: 82 of 250\n",
      "training_loss: 0.0507, acc: 0.873, ROC: 0.526, f1: 0.124\n",
      "epoch: 83 of 250\n",
      "training_loss: 0.0471, acc: 0.901, ROC: 0.525, f1: 0.108\n",
      "epoch: 84 of 250\n",
      "training_loss: 0.0429, acc: 0.880, ROC: 0.544, f1: 0.167\n",
      "epoch: 85 of 250\n",
      "training_loss: 0.0427, acc: 0.881, ROC: 0.554, f1: 0.179\n",
      "epoch: 86 of 250\n",
      "training_loss: 0.0505, acc: 0.894, ROC: 0.481, f1: 0.019\n",
      "epoch: 87 of 250\n",
      "training_loss: 0.0441, acc: 0.864, ROC: 0.520, f1: 0.117\n",
      "epoch: 88 of 250\n",
      "training_loss: 0.0439, acc: 0.887, ROC: 0.534, f1: 0.137\n",
      "epoch: 89 of 250\n",
      "training_loss: 0.0357, acc: 0.876, ROC: 0.551, f1: 0.173\n",
      "epoch: 90 of 250\n",
      "training_loss: 0.0350, acc: 0.855, ROC: 0.525, f1: 0.121\n",
      "epoch: 91 of 250\n",
      "training_loss: 0.0470, acc: 0.892, ROC: 0.519, f1: 0.100\n",
      "epoch: 92 of 250\n",
      "training_loss: 0.0494, acc: 0.868, ROC: 0.519, f1: 0.108\n",
      "epoch: 93 of 250\n",
      "training_loss: 0.0448, acc: 0.887, ROC: 0.523, f1: 0.110\n",
      "epoch: 94 of 250\n",
      "training_loss: 0.0348, acc: 0.893, ROC: 0.546, f1: 0.171\n",
      "epoch: 95 of 250\n",
      "training_loss: 0.0478, acc: 0.913, ROC: 0.510, f1: 0.065\n",
      "epoch: 96 of 250\n",
      "training_loss: 0.0312, acc: 0.866, ROC: 0.534, f1: 0.141\n",
      "epoch: 97 of 250\n",
      "training_loss: 0.0329, acc: 0.871, ROC: 0.495, f1: 0.044\n",
      "epoch: 98 of 250\n",
      "training_loss: 0.0416, acc: 0.888, ROC: 0.631, f1: 0.333\n",
      "epoch: 99 of 250\n",
      "training_loss: 0.0324, acc: 0.893, ROC: 0.561, f1: 0.171\n",
      "epoch: 100 of 250\n",
      "training_loss: 0.0298, acc: 0.872, ROC: 0.519, f1: 0.111\n",
      "epoch: 101 of 250\n",
      "training_loss: 0.0370, acc: 0.889, ROC: 0.511, f1: 0.083\n",
      "epoch: 102 of 250\n",
      "training_loss: 0.0405, acc: 0.891, ROC: 0.513, f1: 0.084\n",
      "epoch: 103 of 250\n",
      "training_loss: 0.0281, acc: 0.880, ROC: 0.544, f1: 0.155\n",
      "epoch: 104 of 250\n",
      "training_loss: 0.0286, acc: 0.864, ROC: 0.545, f1: 0.171\n",
      "epoch: 105 of 250\n",
      "training_loss: 0.0413, acc: 0.851, ROC: 0.503, f1: 0.086\n",
      "epoch: 106 of 250\n",
      "training_loss: 0.0286, acc: 0.882, ROC: 0.558, f1: 0.169\n",
      "epoch: 107 of 250\n",
      "training_loss: 0.0374, acc: 0.873, ROC: 0.520, f1: 0.112\n",
      "epoch: 108 of 250\n",
      "training_loss: 0.0244, acc: 0.865, ROC: 0.542, f1: 0.161\n",
      "epoch: 109 of 250\n",
      "training_loss: 0.0325, acc: 0.879, ROC: 0.524, f1: 0.117\n",
      "epoch: 110 of 250\n",
      "training_loss: 0.0269, acc: 0.898, ROC: 0.529, f1: 0.121\n",
      "epoch: 111 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.0327, acc: 0.881, ROC: 0.557, f1: 0.190\n",
      "epoch: 112 of 250\n",
      "training_loss: 0.0296, acc: 0.883, ROC: 0.537, f1: 0.146\n",
      "epoch: 113 of 250\n",
      "training_loss: 0.0329, acc: 0.905, ROC: 0.513, f1: 0.078\n",
      "epoch: 114 of 250\n",
      "training_loss: 0.0236, acc: 0.879, ROC: 0.539, f1: 0.154\n",
      "epoch: 115 of 250\n",
      "training_loss: 0.0280, acc: 0.866, ROC: 0.529, f1: 0.130\n",
      "epoch: 116 of 250\n",
      "training_loss: 0.0306, acc: 0.876, ROC: 0.521, f1: 0.114\n",
      "epoch: 117 of 250\n",
      "training_loss: 0.0412, acc: 0.875, ROC: 0.545, f1: 0.150\n",
      "epoch: 118 of 250\n",
      "training_loss: 0.0287, acc: 0.875, ROC: 0.559, f1: 0.183\n",
      "epoch: 119 of 250\n",
      "training_loss: 0.0295, acc: 0.889, ROC: 0.505, f1: 0.067\n",
      "epoch: 120 of 250\n",
      "training_loss: 0.0194, acc: 0.849, ROC: 0.515, f1: 0.107\n",
      "epoch: 121 of 250\n",
      "training_loss: 0.0300, acc: 0.863, ROC: 0.575, f1: 0.208\n",
      "epoch: 122 of 250\n",
      "training_loss: 0.0271, acc: 0.864, ROC: 0.534, f1: 0.150\n",
      "epoch: 123 of 250\n",
      "training_loss: 0.0242, acc: 0.900, ROC: 0.524, f1: 0.107\n",
      "epoch: 124 of 250\n",
      "training_loss: 0.0190, acc: 0.897, ROC: 0.562, f1: 0.189\n",
      "epoch: 125 of 250\n",
      "training_loss: 0.0288, acc: 0.885, ROC: 0.539, f1: 0.148\n",
      "epoch: 126 of 250\n",
      "training_loss: 0.0202, acc: 0.901, ROC: 0.505, f1: 0.057\n",
      "epoch: 127 of 250\n",
      "training_loss: 0.0230, acc: 0.888, ROC: 0.510, f1: 0.082\n",
      "epoch: 128 of 250\n",
      "training_loss: 0.0210, acc: 0.906, ROC: 0.532, f1: 0.130\n",
      "epoch: 129 of 250\n",
      "training_loss: 0.0248, acc: 0.897, ROC: 0.513, f1: 0.072\n",
      "epoch: 130 of 250\n",
      "training_loss: 0.0273, acc: 0.889, ROC: 0.541, f1: 0.153\n",
      "epoch: 131 of 250\n",
      "training_loss: 0.0317, acc: 0.888, ROC: 0.517, f1: 0.097\n",
      "epoch: 132 of 250\n",
      "training_loss: 0.0222, acc: 0.882, ROC: 0.525, f1: 0.119\n",
      "epoch: 133 of 250\n",
      "training_loss: 0.0195, acc: 0.875, ROC: 0.544, f1: 0.150\n",
      "epoch: 134 of 250\n",
      "training_loss: 0.0154, acc: 0.894, ROC: 0.551, f1: 0.172\n",
      "epoch: 135 of 250\n",
      "training_loss: 0.0238, acc: 0.901, ROC: 0.525, f1: 0.108\n",
      "epoch: 136 of 250\n",
      "training_loss: 0.0231, acc: 0.862, ROC: 0.537, f1: 0.159\n",
      "epoch: 137 of 250\n",
      "training_loss: 0.0262, acc: 0.875, ROC: 0.521, f1: 0.113\n",
      "epoch: 138 of 250\n",
      "training_loss: 0.0239, acc: 0.881, ROC: 0.519, f1: 0.105\n",
      "epoch: 139 of 250\n",
      "training_loss: 0.0224, acc: 0.893, ROC: 0.520, f1: 0.101\n",
      "epoch: 140 of 250\n",
      "training_loss: 0.0306, acc: 0.901, ROC: 0.549, f1: 0.154\n",
      "epoch: 141 of 250\n",
      "training_loss: 0.0229, acc: 0.879, ROC: 0.530, f1: 0.129\n",
      "epoch: 142 of 250\n",
      "training_loss: 0.0196, acc: 0.883, ROC: 0.556, f1: 0.193\n",
      "epoch: 143 of 250\n",
      "training_loss: 0.0179, acc: 0.879, ROC: 0.518, f1: 0.104\n",
      "epoch: 144 of 250\n",
      "training_loss: 0.0288, acc: 0.857, ROC: 0.542, f1: 0.144\n",
      "epoch: 145 of 250\n",
      "training_loss: 0.0232, acc: 0.889, ROC: 0.547, f1: 0.165\n",
      "epoch: 146 of 250\n",
      "training_loss: 0.0252, acc: 0.874, ROC: 0.535, f1: 0.149\n",
      "epoch: 147 of 250\n",
      "training_loss: 0.0239, acc: 0.881, ROC: 0.530, f1: 0.131\n",
      "epoch: 148 of 250\n",
      "training_loss: 0.0282, acc: 0.894, ROC: 0.558, f1: 0.185\n",
      "epoch: 149 of 250\n",
      "training_loss: 0.0141, acc: 0.892, ROC: 0.519, f1: 0.100\n",
      "epoch: 150 of 250\n",
      "training_loss: 0.0245, acc: 0.879, ROC: 0.538, f1: 0.154\n",
      "epoch: 151 of 250\n",
      "training_loss: 0.0208, acc: 0.868, ROC: 0.529, f1: 0.132\n",
      "epoch: 152 of 250\n",
      "training_loss: 0.0252, acc: 0.898, ROC: 0.554, f1: 0.177\n",
      "epoch: 153 of 250\n",
      "training_loss: 0.0417, acc: 0.880, ROC: 0.525, f1: 0.118\n",
      "epoch: 154 of 250\n",
      "training_loss: 0.0233, acc: 0.875, ROC: 0.521, f1: 0.113\n",
      "epoch: 155 of 250\n",
      "training_loss: 0.0205, acc: 0.874, ROC: 0.536, f1: 0.149\n",
      "epoch: 156 of 250\n",
      "training_loss: 0.0199, acc: 0.892, ROC: 0.572, f1: 0.217\n",
      "epoch: 157 of 250\n",
      "training_loss: 0.0212, acc: 0.898, ROC: 0.540, f1: 0.150\n",
      "epoch: 158 of 250\n",
      "training_loss: 0.0193, acc: 0.884, ROC: 0.515, f1: 0.094\n",
      "epoch: 159 of 250\n",
      "training_loss: 0.0193, acc: 0.894, ROC: 0.493, f1: 0.036\n",
      "epoch: 160 of 250\n",
      "training_loss: 0.0227, acc: 0.913, ROC: 0.530, f1: 0.121\n",
      "epoch: 161 of 250\n",
      "training_loss: 0.0133, acc: 0.907, ROC: 0.541, f1: 0.147\n",
      "epoch: 162 of 250\n",
      "training_loss: 0.0227, acc: 0.880, ROC: 0.524, f1: 0.118\n",
      "epoch: 163 of 250\n",
      "training_loss: 0.0160, acc: 0.856, ROC: 0.550, f1: 0.182\n",
      "epoch: 164 of 250\n",
      "training_loss: 0.0178, acc: 0.880, ROC: 0.527, f1: 0.118\n",
      "epoch: 165 of 250\n",
      "training_loss: 0.0134, acc: 0.898, ROC: 0.560, f1: 0.177\n",
      "epoch: 166 of 250\n",
      "training_loss: 0.0274, acc: 0.887, ROC: 0.528, f1: 0.124\n",
      "epoch: 167 of 250\n",
      "training_loss: 0.0146, acc: 0.883, ROC: 0.571, f1: 0.215\n",
      "epoch: 168 of 250\n",
      "training_loss: 0.0143, acc: 0.879, ROC: 0.567, f1: 0.199\n",
      "epoch: 169 of 250\n",
      "training_loss: 0.0180, acc: 0.863, ROC: 0.539, f1: 0.149\n",
      "epoch: 170 of 250\n",
      "training_loss: 0.0228, acc: 0.877, ROC: 0.523, f1: 0.115\n",
      "epoch: 171 of 250\n",
      "training_loss: 0.0217, acc: 0.875, ROC: 0.515, f1: 0.101\n",
      "epoch: 172 of 250\n",
      "training_loss: 0.0196, acc: 0.893, ROC: 0.527, f1: 0.116\n",
      "epoch: 173 of 250\n",
      "training_loss: 0.0085, acc: 0.893, ROC: 0.520, f1: 0.101\n",
      "epoch: 174 of 250\n",
      "training_loss: 0.0103, acc: 0.874, ROC: 0.526, f1: 0.125\n",
      "epoch: 175 of 250\n",
      "training_loss: 0.0309, acc: 0.860, ROC: 0.528, f1: 0.136\n",
      "epoch: 176 of 250\n",
      "training_loss: 0.0325, acc: 0.888, ROC: 0.528, f1: 0.125\n",
      "epoch: 177 of 250\n",
      "training_loss: 0.0097, acc: 0.895, ROC: 0.521, f1: 0.103\n",
      "epoch: 178 of 250\n",
      "training_loss: 0.0168, acc: 0.883, ROC: 0.536, f1: 0.146\n",
      "epoch: 179 of 250\n",
      "training_loss: 0.0190, acc: 0.861, ROC: 0.503, f1: 0.079\n",
      "epoch: 180 of 250\n",
      "training_loss: 0.0168, acc: 0.888, ROC: 0.528, f1: 0.125\n",
      "epoch: 181 of 250\n",
      "training_loss: 0.0212, acc: 0.906, ROC: 0.505, f1: 0.060\n",
      "epoch: 182 of 250\n",
      "training_loss: 0.0217, acc: 0.878, ROC: 0.528, f1: 0.129\n",
      "epoch: 183 of 250\n",
      "training_loss: 0.0191, acc: 0.887, ROC: 0.511, f1: 0.081\n",
      "epoch: 184 of 250\n",
      "training_loss: 0.0154, acc: 0.894, ROC: 0.547, f1: 0.159\n",
      "epoch: 185 of 250\n",
      "training_loss: 0.0169, acc: 0.855, ROC: 0.520, f1: 0.121\n",
      "epoch: 186 of 250\n",
      "training_loss: 0.0142, acc: 0.891, ROC: 0.518, f1: 0.099\n",
      "epoch: 187 of 250\n",
      "training_loss: 0.0113, acc: 0.881, ROC: 0.508, f1: 0.078\n",
      "epoch: 188 of 250\n",
      "training_loss: 0.0209, acc: 0.873, ROC: 0.531, f1: 0.136\n",
      "epoch: 189 of 250\n",
      "training_loss: 0.0218, acc: 0.894, ROC: 0.526, f1: 0.117\n",
      "epoch: 190 of 250\n",
      "training_loss: 0.0159, acc: 0.873, ROC: 0.521, f1: 0.112\n",
      "epoch: 191 of 250\n",
      "training_loss: 0.0165, acc: 0.885, ROC: 0.551, f1: 0.173\n",
      "epoch: 192 of 250\n",
      "training_loss: 0.0176, acc: 0.901, ROC: 0.510, f1: 0.075\n",
      "epoch: 193 of 250\n",
      "training_loss: 0.0171, acc: 0.891, ROC: 0.551, f1: 0.180\n",
      "epoch: 194 of 250\n",
      "training_loss: 0.0144, acc: 0.908, ROC: 0.522, f1: 0.098\n",
      "epoch: 195 of 250\n",
      "training_loss: 0.0153, acc: 0.887, ROC: 0.551, f1: 0.163\n",
      "epoch: 196 of 250\n",
      "training_loss: 0.0143, acc: 0.881, ROC: 0.564, f1: 0.212\n",
      "epoch: 197 of 250\n",
      "training_loss: 0.0146, acc: 0.877, ROC: 0.506, f1: 0.075\n",
      "epoch: 198 of 250\n",
      "training_loss: 0.0238, acc: 0.918, ROC: 0.546, f1: 0.163\n",
      "epoch: 199 of 250\n",
      "training_loss: 0.0267, acc: 0.902, ROC: 0.524, f1: 0.109\n",
      "epoch: 200 of 250\n",
      "training_loss: 0.0098, acc: 0.858, ROC: 0.509, f1: 0.090\n",
      "epoch: 201 of 250\n",
      "training_loss: 0.0130, acc: 0.882, ROC: 0.561, f1: 0.203\n",
      "epoch: 202 of 250\n",
      "training_loss: 0.0162, acc: 0.917, ROC: 0.555, f1: 0.178\n",
      "epoch: 203 of 250\n",
      "training_loss: 0.0103, acc: 0.900, ROC: 0.517, f1: 0.091\n",
      "epoch: 204 of 250\n",
      "training_loss: 0.0180, acc: 0.898, ROC: 0.523, f1: 0.105\n",
      "epoch: 205 of 250\n",
      "training_loss: 0.0208, acc: 0.883, ROC: 0.571, f1: 0.225\n",
      "epoch: 206 of 250\n",
      "training_loss: 0.0161, acc: 0.886, ROC: 0.597, f1: 0.240\n",
      "epoch: 207 of 250\n",
      "training_loss: 0.0191, acc: 0.883, ROC: 0.558, f1: 0.182\n",
      "epoch: 208 of 250\n",
      "training_loss: 0.0126, acc: 0.896, ROC: 0.544, f1: 0.161\n",
      "epoch: 209 of 250\n",
      "training_loss: 0.0128, acc: 0.884, ROC: 0.503, f1: 0.065\n",
      "epoch: 210 of 250\n",
      "training_loss: 0.0124, acc: 0.878, ROC: 0.523, f1: 0.116\n",
      "epoch: 211 of 250\n",
      "training_loss: 0.0142, acc: 0.899, ROC: 0.510, f1: 0.073\n",
      "epoch: 212 of 250\n",
      "training_loss: 0.0121, acc: 0.875, ROC: 0.538, f1: 0.150\n",
      "epoch: 213 of 250\n",
      "training_loss: 0.0236, acc: 0.875, ROC: 0.503, f1: 0.074\n",
      "epoch: 214 of 250\n",
      "training_loss: 0.0095, acc: 0.867, ROC: 0.536, f1: 0.142\n",
      "epoch: 215 of 250\n",
      "training_loss: 0.0133, acc: 0.850, ROC: 0.512, f1: 0.107\n",
      "epoch: 216 of 250\n",
      "training_loss: 0.0130, acc: 0.873, ROC: 0.526, f1: 0.124\n",
      "epoch: 217 of 250\n",
      "training_loss: 0.0180, acc: 0.887, ROC: 0.550, f1: 0.163\n",
      "epoch: 218 of 250\n",
      "training_loss: 0.0126, acc: 0.879, ROC: 0.512, f1: 0.090\n",
      "epoch: 219 of 250\n",
      "training_loss: 0.0079, acc: 0.863, ROC: 0.532, f1: 0.138\n",
      "epoch: 220 of 250\n",
      "training_loss: 0.0084, acc: 0.901, ROC: 0.537, f1: 0.139\n",
      "epoch: 221 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.0200, acc: 0.881, ROC: 0.563, f1: 0.179\n",
      "epoch: 222 of 250\n",
      "training_loss: 0.0212, acc: 0.884, ROC: 0.515, f1: 0.094\n",
      "epoch: 223 of 250\n",
      "training_loss: 0.0111, acc: 0.890, ROC: 0.531, f1: 0.127\n",
      "epoch: 224 of 250\n",
      "training_loss: 0.0108, acc: 0.900, ROC: 0.545, f1: 0.153\n",
      "epoch: 225 of 250\n",
      "training_loss: 0.0093, acc: 0.885, ROC: 0.516, f1: 0.094\n",
      "epoch: 226 of 250\n",
      "training_loss: 0.0114, acc: 0.901, ROC: 0.530, f1: 0.124\n",
      "epoch: 227 of 250\n",
      "training_loss: 0.0098, acc: 0.865, ROC: 0.561, f1: 0.192\n",
      "epoch: 228 of 250\n",
      "training_loss: 0.0089, acc: 0.872, ROC: 0.535, f1: 0.147\n",
      "epoch: 229 of 250\n",
      "training_loss: 0.0164, acc: 0.859, ROC: 0.553, f1: 0.194\n",
      "epoch: 230 of 250\n",
      "training_loss: 0.0115, acc: 0.890, ROC: 0.524, f1: 0.113\n",
      "epoch: 231 of 250\n",
      "training_loss: 0.0174, acc: 0.878, ROC: 0.503, f1: 0.062\n",
      "epoch: 232 of 250\n",
      "training_loss: 0.0118, acc: 0.903, ROC: 0.578, f1: 0.224\n",
      "epoch: 233 of 250\n",
      "training_loss: 0.0240, acc: 0.896, ROC: 0.560, f1: 0.188\n",
      "epoch: 234 of 250\n",
      "training_loss: 0.0121, acc: 0.899, ROC: 0.529, f1: 0.122\n",
      "epoch: 235 of 250\n",
      "training_loss: 0.0064, acc: 0.882, ROC: 0.541, f1: 0.157\n",
      "epoch: 236 of 250\n",
      "training_loss: 0.0138, acc: 0.896, ROC: 0.532, f1: 0.133\n",
      "epoch: 237 of 250\n",
      "training_loss: 0.0149, acc: 0.886, ROC: 0.556, f1: 0.174\n",
      "epoch: 238 of 250\n",
      "training_loss: 0.0120, acc: 0.883, ROC: 0.514, f1: 0.093\n",
      "epoch: 239 of 250\n",
      "training_loss: 0.0107, acc: 0.891, ROC: 0.540, f1: 0.142\n",
      "epoch: 240 of 250\n",
      "training_loss: 0.0096, acc: 0.901, ROC: 0.547, f1: 0.154\n",
      "epoch: 241 of 250\n",
      "training_loss: 0.0074, acc: 0.891, ROC: 0.543, f1: 0.155\n",
      "epoch: 242 of 250\n",
      "training_loss: 0.0116, acc: 0.890, ROC: 0.541, f1: 0.154\n",
      "epoch: 243 of 250\n",
      "training_loss: 0.0147, acc: 0.906, ROC: 0.536, f1: 0.130\n",
      "epoch: 244 of 250\n",
      "training_loss: 0.0111, acc: 0.903, ROC: 0.519, f1: 0.093\n",
      "epoch: 245 of 250\n",
      "training_loss: 0.0124, acc: 0.879, ROC: 0.508, f1: 0.076\n",
      "epoch: 246 of 250\n",
      "training_loss: 0.0145, acc: 0.866, ROC: 0.521, f1: 0.118\n",
      "epoch: 247 of 250\n",
      "training_loss: 0.0107, acc: 0.887, ROC: 0.532, f1: 0.137\n",
      "epoch: 248 of 250\n",
      "training_loss: 0.0085, acc: 0.879, ROC: 0.528, f1: 0.129\n",
      "epoch: 249 of 250\n",
      "training_loss: 0.0172, acc: 0.893, ROC: 0.544, f1: 0.144\n",
      "(26173, 39, 11, 11)\n",
      "epoch: 0 of 250\n",
      "training_loss: 0.6071, acc: 0.710, ROC: 0.587, f1: 0.190\n",
      "epoch: 1 of 250\n",
      "training_loss: 0.5689, acc: 0.715, ROC: 0.604, f1: 0.202\n",
      "epoch: 2 of 250\n",
      "training_loss: 0.5399, acc: 0.668, ROC: 0.597, f1: 0.166\n",
      "epoch: 3 of 250\n",
      "training_loss: 0.5174, acc: 0.741, ROC: 0.594, f1: 0.222\n",
      "epoch: 4 of 250\n",
      "training_loss: 0.5181, acc: 0.713, ROC: 0.570, f1: 0.168\n",
      "epoch: 5 of 250\n",
      "training_loss: 0.5095, acc: 0.741, ROC: 0.593, f1: 0.193\n",
      "epoch: 6 of 250\n",
      "training_loss: 0.4680, acc: 0.803, ROC: 0.523, f1: 0.132\n",
      "epoch: 7 of 250\n",
      "training_loss: 0.4685, acc: 0.733, ROC: 0.600, f1: 0.193\n",
      "epoch: 8 of 250\n",
      "training_loss: 0.4571, acc: 0.832, ROC: 0.499, f1: 0.087\n",
      "epoch: 9 of 250\n",
      "training_loss: 0.4344, acc: 0.830, ROC: 0.521, f1: 0.133\n",
      "epoch: 10 of 250\n",
      "training_loss: 0.4070, acc: 0.809, ROC: 0.567, f1: 0.151\n",
      "epoch: 11 of 250\n",
      "training_loss: 0.3910, acc: 0.725, ROC: 0.549, f1: 0.174\n",
      "epoch: 12 of 250\n",
      "training_loss: 0.3954, acc: 0.719, ROC: 0.566, f1: 0.135\n",
      "epoch: 13 of 250\n",
      "training_loss: 0.3958, acc: 0.812, ROC: 0.546, f1: 0.145\n",
      "epoch: 14 of 250\n",
      "training_loss: 0.4257, acc: 0.753, ROC: 0.513, f1: 0.127\n",
      "epoch: 15 of 250\n",
      "training_loss: 0.3658, acc: 0.811, ROC: 0.601, f1: 0.229\n",
      "epoch: 16 of 250\n",
      "training_loss: 0.3658, acc: 0.752, ROC: 0.572, f1: 0.173\n",
      "epoch: 17 of 250\n",
      "training_loss: 0.3247, acc: 0.886, ROC: 0.583, f1: 0.240\n",
      "epoch: 18 of 250\n",
      "training_loss: 0.3393, acc: 0.865, ROC: 0.533, f1: 0.140\n",
      "epoch: 19 of 250\n",
      "training_loss: 0.3239, acc: 0.831, ROC: 0.550, f1: 0.176\n",
      "epoch: 20 of 250\n",
      "training_loss: 0.3074, acc: 0.803, ROC: 0.569, f1: 0.189\n",
      "epoch: 21 of 250\n",
      "training_loss: 0.3240, acc: 0.875, ROC: 0.529, f1: 0.126\n",
      "epoch: 22 of 250\n",
      "training_loss: 0.3040, acc: 0.859, ROC: 0.518, f1: 0.113\n",
      "epoch: 23 of 250\n",
      "training_loss: 0.2895, acc: 0.841, ROC: 0.592, f1: 0.217\n",
      "epoch: 24 of 250\n",
      "training_loss: 0.2692, acc: 0.788, ROC: 0.509, f1: 0.117\n",
      "epoch: 25 of 250\n",
      "training_loss: 0.2920, acc: 0.832, ROC: 0.517, f1: 0.106\n",
      "epoch: 26 of 250\n",
      "training_loss: 0.2948, acc: 0.842, ROC: 0.561, f1: 0.177\n",
      "epoch: 27 of 250\n",
      "training_loss: 0.2623, acc: 0.832, ROC: 0.506, f1: 0.097\n",
      "epoch: 28 of 250\n",
      "training_loss: 0.3093, acc: 0.868, ROC: 0.530, f1: 0.132\n",
      "epoch: 29 of 250\n",
      "training_loss: 0.2959, acc: 0.842, ROC: 0.570, f1: 0.186\n",
      "epoch: 30 of 250\n",
      "training_loss: 0.2566, acc: 0.825, ROC: 0.546, f1: 0.171\n",
      "epoch: 31 of 250\n",
      "training_loss: 0.2791, acc: 0.840, ROC: 0.555, f1: 0.184\n",
      "epoch: 32 of 250\n",
      "training_loss: 0.2621, acc: 0.890, ROC: 0.603, f1: 0.257\n",
      "epoch: 33 of 250\n",
      "training_loss: 0.2496, acc: 0.855, ROC: 0.548, f1: 0.171\n",
      "epoch: 34 of 250\n",
      "training_loss: 0.2696, acc: 0.843, ROC: 0.520, f1: 0.113\n",
      "epoch: 35 of 250\n",
      "training_loss: 0.2342, acc: 0.874, ROC: 0.515, f1: 0.100\n",
      "epoch: 36 of 250\n",
      "training_loss: 0.2688, acc: 0.890, ROC: 0.567, f1: 0.191\n",
      "epoch: 37 of 250\n",
      "training_loss: 0.2407, acc: 0.877, ROC: 0.517, f1: 0.102\n",
      "epoch: 38 of 250\n",
      "training_loss: 0.2321, acc: 0.807, ROC: 0.538, f1: 0.165\n",
      "epoch: 39 of 250\n",
      "training_loss: 0.2598, acc: 0.868, ROC: 0.554, f1: 0.175\n",
      "epoch: 40 of 250\n",
      "training_loss: 0.2271, acc: 0.893, ROC: 0.520, f1: 0.101\n",
      "epoch: 41 of 250\n",
      "training_loss: 0.2476, acc: 0.854, ROC: 0.548, f1: 0.170\n",
      "epoch: 42 of 250\n",
      "training_loss: 0.2317, acc: 0.835, ROC: 0.583, f1: 0.225\n",
      "epoch: 43 of 250\n",
      "training_loss: 0.2156, acc: 0.880, ROC: 0.501, f1: 0.062\n",
      "epoch: 44 of 250\n",
      "training_loss: 0.2606, acc: 0.782, ROC: 0.592, f1: 0.180\n",
      "epoch: 45 of 250\n",
      "training_loss: 0.2373, acc: 0.843, ROC: 0.517, f1: 0.113\n",
      "epoch: 46 of 250\n",
      "training_loss: 0.2097, acc: 0.871, ROC: 0.545, f1: 0.168\n",
      "epoch: 47 of 250\n",
      "training_loss: 0.2209, acc: 0.859, ROC: 0.553, f1: 0.175\n",
      "epoch: 48 of 250\n",
      "training_loss: 0.2219, acc: 0.778, ROC: 0.583, f1: 0.172\n",
      "epoch: 49 of 250\n",
      "training_loss: 0.2444, acc: 0.834, ROC: 0.556, f1: 0.170\n",
      "epoch: 50 of 250\n",
      "training_loss: 0.2305, acc: 0.787, ROC: 0.557, f1: 0.171\n",
      "epoch: 51 of 250\n",
      "training_loss: 0.2258, acc: 0.819, ROC: 0.547, f1: 0.142\n",
      "epoch: 52 of 250\n",
      "training_loss: 0.2277, acc: 0.810, ROC: 0.512, f1: 0.120\n",
      "epoch: 53 of 250\n",
      "training_loss: 0.1988, acc: 0.850, ROC: 0.568, f1: 0.194\n",
      "epoch: 54 of 250\n",
      "training_loss: 0.1992, acc: 0.877, ROC: 0.511, f1: 0.089\n",
      "epoch: 55 of 250\n",
      "training_loss: 0.2008, acc: 0.868, ROC: 0.526, f1: 0.120\n",
      "epoch: 56 of 250\n",
      "training_loss: 0.1983, acc: 0.840, ROC: 0.619, f1: 0.231\n",
      "epoch: 57 of 250\n",
      "training_loss: 0.2190, acc: 0.796, ROC: 0.542, f1: 0.177\n",
      "epoch: 58 of 250\n",
      "training_loss: 0.2059, acc: 0.769, ROC: 0.607, f1: 0.178\n",
      "epoch: 59 of 250\n",
      "training_loss: 0.2082, acc: 0.853, ROC: 0.535, f1: 0.150\n",
      "epoch: 60 of 250\n",
      "training_loss: 0.1976, acc: 0.849, ROC: 0.507, f1: 0.096\n",
      "epoch: 61 of 250\n",
      "training_loss: 0.2094, acc: 0.875, ROC: 0.531, f1: 0.126\n",
      "epoch: 62 of 250\n",
      "training_loss: 0.2063, acc: 0.879, ROC: 0.533, f1: 0.142\n",
      "epoch: 63 of 250\n",
      "training_loss: 0.2072, acc: 0.881, ROC: 0.545, f1: 0.156\n",
      "epoch: 64 of 250\n",
      "training_loss: 0.1946, acc: 0.823, ROC: 0.606, f1: 0.227\n",
      "epoch: 65 of 250\n",
      "training_loss: 0.2017, acc: 0.804, ROC: 0.588, f1: 0.203\n",
      "epoch: 66 of 250\n",
      "training_loss: 0.1999, acc: 0.887, ROC: 0.538, f1: 0.137\n",
      "epoch: 67 of 250\n",
      "training_loss: 0.2063, acc: 0.870, ROC: 0.568, f1: 0.207\n",
      "epoch: 68 of 250\n",
      "training_loss: 0.1890, acc: 0.883, ROC: 0.556, f1: 0.170\n",
      "epoch: 69 of 250\n",
      "training_loss: 0.1894, acc: 0.833, ROC: 0.521, f1: 0.116\n",
      "epoch: 70 of 250\n",
      "training_loss: 0.1792, acc: 0.855, ROC: 0.536, f1: 0.142\n",
      "epoch: 71 of 250\n",
      "training_loss: 0.1899, acc: 0.883, ROC: 0.544, f1: 0.158\n",
      "epoch: 72 of 250\n",
      "training_loss: 0.1755, acc: 0.907, ROC: 0.521, f1: 0.097\n",
      "epoch: 73 of 250\n",
      "training_loss: 0.1893, acc: 0.896, ROC: 0.534, f1: 0.133\n",
      "epoch: 74 of 250\n",
      "training_loss: 0.1640, acc: 0.865, ROC: 0.545, f1: 0.161\n",
      "epoch: 75 of 250\n",
      "training_loss: 0.1832, acc: 0.793, ROC: 0.602, f1: 0.225\n",
      "epoch: 76 of 250\n",
      "training_loss: 0.1745, acc: 0.831, ROC: 0.574, f1: 0.191\n",
      "epoch: 77 of 250\n",
      "training_loss: 0.1725, acc: 0.841, ROC: 0.543, f1: 0.159\n",
      "epoch: 78 of 250\n",
      "training_loss: 0.1810, acc: 0.917, ROC: 0.520, f1: 0.088\n",
      "epoch: 79 of 250\n",
      "training_loss: 0.1765, acc: 0.815, ROC: 0.514, f1: 0.106\n",
      "epoch: 80 of 250\n",
      "training_loss: 0.2034, acc: 0.792, ROC: 0.569, f1: 0.181\n",
      "epoch: 81 of 250\n",
      "training_loss: 0.1816, acc: 0.856, ROC: 0.578, f1: 0.217\n",
      "epoch: 82 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.1614, acc: 0.892, ROC: 0.519, f1: 0.100\n",
      "epoch: 83 of 250\n",
      "training_loss: 0.1653, acc: 0.869, ROC: 0.549, f1: 0.166\n",
      "epoch: 84 of 250\n",
      "training_loss: 0.1630, acc: 0.878, ROC: 0.581, f1: 0.218\n",
      "epoch: 85 of 250\n",
      "training_loss: 0.1858, acc: 0.847, ROC: 0.553, f1: 0.155\n",
      "epoch: 86 of 250\n",
      "training_loss: 0.1778, acc: 0.872, ROC: 0.547, f1: 0.169\n",
      "epoch: 87 of 250\n",
      "training_loss: 0.1611, acc: 0.886, ROC: 0.517, f1: 0.095\n",
      "epoch: 88 of 250\n",
      "training_loss: 0.1635, acc: 0.739, ROC: 0.555, f1: 0.166\n",
      "epoch: 89 of 250\n",
      "training_loss: 0.1863, acc: 0.812, ROC: 0.618, f1: 0.260\n",
      "epoch: 90 of 250\n",
      "training_loss: 0.1759, acc: 0.844, ROC: 0.524, f1: 0.133\n",
      "epoch: 91 of 250\n",
      "training_loss: 0.1676, acc: 0.824, ROC: 0.531, f1: 0.146\n",
      "epoch: 92 of 250\n",
      "training_loss: 0.1424, acc: 0.880, ROC: 0.550, f1: 0.167\n",
      "epoch: 93 of 250\n",
      "training_loss: 0.1679, acc: 0.903, ROC: 0.525, f1: 0.110\n",
      "epoch: 94 of 250\n",
      "training_loss: 0.1808, acc: 0.860, ROC: 0.502, f1: 0.079\n",
      "epoch: 95 of 250\n",
      "training_loss: 0.1789, acc: 0.899, ROC: 0.517, f1: 0.090\n",
      "epoch: 96 of 250\n",
      "training_loss: 0.1684, acc: 0.873, ROC: 0.563, f1: 0.181\n",
      "epoch: 97 of 250\n",
      "training_loss: 0.1585, acc: 0.852, ROC: 0.557, f1: 0.159\n",
      "epoch: 98 of 250\n",
      "training_loss: 0.1572, acc: 0.862, ROC: 0.515, f1: 0.104\n",
      "epoch: 99 of 250\n",
      "training_loss: 0.1741, acc: 0.847, ROC: 0.552, f1: 0.182\n",
      "epoch: 100 of 250\n",
      "training_loss: 0.1491, acc: 0.879, ROC: 0.578, f1: 0.219\n",
      "epoch: 101 of 250\n",
      "training_loss: 0.1593, acc: 0.808, ROC: 0.553, f1: 0.158\n",
      "epoch: 102 of 250\n",
      "training_loss: 0.1594, acc: 0.780, ROC: 0.519, f1: 0.127\n",
      "epoch: 103 of 250\n",
      "training_loss: 0.1606, acc: 0.877, ROC: 0.548, f1: 0.163\n",
      "epoch: 104 of 250\n",
      "training_loss: 0.1664, acc: 0.862, ROC: 0.556, f1: 0.179\n",
      "epoch: 105 of 250\n",
      "training_loss: 0.1338, acc: 0.818, ROC: 0.544, f1: 0.165\n",
      "epoch: 106 of 250\n",
      "training_loss: 0.1524, acc: 0.905, ROC: 0.496, f1: 0.021\n",
      "epoch: 107 of 250\n",
      "training_loss: 0.1675, acc: 0.869, ROC: 0.506, f1: 0.084\n",
      "epoch: 108 of 250\n",
      "training_loss: 0.1443, acc: 0.879, ROC: 0.523, f1: 0.117\n",
      "epoch: 109 of 250\n",
      "training_loss: 0.1560, acc: 0.895, ROC: 0.534, f1: 0.132\n",
      "epoch: 110 of 250\n",
      "training_loss: 0.1446, acc: 0.870, ROC: 0.536, f1: 0.145\n",
      "epoch: 111 of 250\n",
      "training_loss: 0.1432, acc: 0.862, ROC: 0.556, f1: 0.169\n",
      "epoch: 112 of 250\n",
      "training_loss: 0.1662, acc: 0.876, ROC: 0.534, f1: 0.139\n",
      "epoch: 113 of 250\n",
      "training_loss: 0.1532, acc: 0.896, ROC: 0.510, f1: 0.071\n",
      "epoch: 114 of 250\n",
      "training_loss: 0.1446, acc: 0.876, ROC: 0.499, f1: 0.061\n",
      "epoch: 115 of 250\n",
      "training_loss: 0.1602, acc: 0.802, ROC: 0.548, f1: 0.161\n",
      "epoch: 116 of 250\n",
      "training_loss: 0.1472, acc: 0.896, ROC: 0.546, f1: 0.161\n",
      "epoch: 117 of 250\n",
      "training_loss: 0.1376, acc: 0.870, ROC: 0.523, f1: 0.122\n",
      "epoch: 118 of 250\n",
      "training_loss: 0.1556, acc: 0.871, ROC: 0.531, f1: 0.134\n",
      "epoch: 119 of 250\n",
      "training_loss: 0.1448, acc: 0.856, ROC: 0.574, f1: 0.209\n",
      "epoch: 120 of 250\n",
      "training_loss: 0.1505, acc: 0.865, ROC: 0.542, f1: 0.151\n",
      "epoch: 121 of 250\n",
      "training_loss: 0.1304, acc: 0.836, ROC: 0.553, f1: 0.172\n",
      "epoch: 122 of 250\n",
      "training_loss: 0.1470, acc: 0.863, ROC: 0.581, f1: 0.243\n",
      "epoch: 123 of 250\n",
      "training_loss: 0.1413, acc: 0.847, ROC: 0.524, f1: 0.126\n",
      "epoch: 124 of 250\n",
      "training_loss: 0.1587, acc: 0.879, ROC: 0.534, f1: 0.129\n",
      "epoch: 125 of 250\n",
      "training_loss: 0.1627, acc: 0.867, ROC: 0.528, f1: 0.131\n",
      "epoch: 126 of 250\n",
      "training_loss: 0.1376, acc: 0.846, ROC: 0.564, f1: 0.181\n",
      "epoch: 127 of 250\n",
      "training_loss: 0.1188, acc: 0.845, ROC: 0.561, f1: 0.188\n",
      "epoch: 128 of 250\n",
      "training_loss: 0.1215, acc: 0.819, ROC: 0.585, f1: 0.203\n",
      "epoch: 129 of 250\n",
      "training_loss: 0.1456, acc: 0.853, ROC: 0.512, f1: 0.098\n",
      "epoch: 130 of 250\n",
      "training_loss: 0.1336, acc: 0.884, ROC: 0.503, f1: 0.065\n",
      "epoch: 131 of 250\n",
      "training_loss: 0.1266, acc: 0.862, ROC: 0.514, f1: 0.104\n",
      "epoch: 132 of 250\n",
      "training_loss: 0.1486, acc: 0.843, ROC: 0.560, f1: 0.187\n",
      "epoch: 133 of 250\n",
      "training_loss: 0.1645, acc: 0.854, ROC: 0.535, f1: 0.141\n",
      "epoch: 134 of 250\n",
      "training_loss: 0.1255, acc: 0.899, ROC: 0.537, f1: 0.137\n",
      "epoch: 135 of 250\n",
      "training_loss: 0.1302, acc: 0.855, ROC: 0.571, f1: 0.216\n",
      "epoch: 136 of 250\n",
      "training_loss: 0.1223, acc: 0.850, ROC: 0.623, f1: 0.286\n",
      "epoch: 137 of 250\n",
      "training_loss: 0.1355, acc: 0.861, ROC: 0.559, f1: 0.187\n",
      "epoch: 138 of 250\n",
      "training_loss: 0.1368, acc: 0.881, ROC: 0.540, f1: 0.144\n",
      "epoch: 139 of 250\n",
      "training_loss: 0.1410, acc: 0.887, ROC: 0.538, f1: 0.137\n",
      "epoch: 140 of 250\n",
      "training_loss: 0.1229, acc: 0.858, ROC: 0.537, f1: 0.155\n",
      "epoch: 141 of 250\n",
      "training_loss: 0.1207, acc: 0.820, ROC: 0.591, f1: 0.204\n",
      "epoch: 142 of 250\n",
      "training_loss: 0.1287, acc: 0.876, ROC: 0.533, f1: 0.139\n",
      "epoch: 143 of 250\n",
      "training_loss: 0.1253, acc: 0.860, ROC: 0.565, f1: 0.195\n",
      "epoch: 144 of 250\n",
      "training_loss: 0.1227, acc: 0.875, ROC: 0.549, f1: 0.172\n",
      "epoch: 145 of 250\n",
      "training_loss: 0.1533, acc: 0.902, ROC: 0.532, f1: 0.125\n",
      "epoch: 146 of 250\n",
      "training_loss: 0.1059, acc: 0.891, ROC: 0.577, f1: 0.216\n",
      "epoch: 147 of 250\n",
      "training_loss: 0.1171, acc: 0.853, ROC: 0.576, f1: 0.197\n",
      "epoch: 148 of 250\n",
      "training_loss: 0.1396, acc: 0.868, ROC: 0.566, f1: 0.205\n",
      "epoch: 149 of 250\n",
      "training_loss: 0.1401, acc: 0.837, ROC: 0.560, f1: 0.181\n",
      "epoch: 150 of 250\n",
      "training_loss: 0.1172, acc: 0.859, ROC: 0.568, f1: 0.166\n",
      "epoch: 151 of 250\n",
      "training_loss: 0.1113, acc: 0.895, ROC: 0.515, f1: 0.087\n",
      "epoch: 152 of 250\n",
      "training_loss: 0.1224, acc: 0.873, ROC: 0.520, f1: 0.112\n",
      "epoch: 153 of 250\n",
      "training_loss: 0.1279, acc: 0.808, ROC: 0.521, f1: 0.111\n",
      "epoch: 154 of 250\n",
      "training_loss: 0.1383, acc: 0.888, ROC: 0.554, f1: 0.176\n",
      "epoch: 155 of 250\n",
      "training_loss: 0.1187, acc: 0.871, ROC: 0.546, f1: 0.168\n",
      "epoch: 156 of 250\n",
      "training_loss: 0.1151, acc: 0.874, ROC: 0.566, f1: 0.192\n",
      "epoch: 157 of 250\n",
      "training_loss: 0.1346, acc: 0.862, ROC: 0.522, f1: 0.115\n",
      "epoch: 158 of 250\n",
      "training_loss: 0.1118, acc: 0.884, ROC: 0.571, f1: 0.216\n",
      "epoch: 159 of 250\n",
      "training_loss: 0.1105, acc: 0.872, ROC: 0.541, f1: 0.158\n",
      "epoch: 160 of 250\n",
      "training_loss: 0.1167, acc: 0.874, ROC: 0.541, f1: 0.137\n",
      "epoch: 161 of 250\n",
      "training_loss: 0.1144, acc: 0.886, ROC: 0.558, f1: 0.174\n",
      "epoch: 162 of 250\n",
      "training_loss: 0.1094, acc: 0.874, ROC: 0.557, f1: 0.182\n",
      "epoch: 163 of 250\n",
      "training_loss: 0.1188, acc: 0.892, ROC: 0.571, f1: 0.206\n",
      "epoch: 164 of 250\n",
      "training_loss: 0.1234, acc: 0.908, ROC: 0.561, f1: 0.193\n",
      "epoch: 165 of 250\n",
      "training_loss: 0.1250, acc: 0.865, ROC: 0.544, f1: 0.172\n",
      "epoch: 166 of 250\n",
      "training_loss: 0.1258, acc: 0.906, ROC: 0.527, f1: 0.113\n",
      "epoch: 167 of 250\n",
      "training_loss: 0.1005, acc: 0.893, ROC: 0.532, f1: 0.130\n",
      "epoch: 168 of 250\n",
      "training_loss: 0.1073, acc: 0.865, ROC: 0.532, f1: 0.140\n",
      "epoch: 169 of 250\n",
      "training_loss: 0.1306, acc: 0.881, ROC: 0.533, f1: 0.131\n",
      "epoch: 170 of 250\n",
      "training_loss: 0.0986, acc: 0.887, ROC: 0.583, f1: 0.210\n",
      "epoch: 171 of 250\n",
      "training_loss: 0.1000, acc: 0.855, ROC: 0.551, f1: 0.162\n",
      "epoch: 172 of 250\n",
      "training_loss: 0.1020, acc: 0.840, ROC: 0.525, f1: 0.130\n",
      "epoch: 173 of 250\n",
      "training_loss: 0.1125, acc: 0.876, ROC: 0.571, f1: 0.225\n",
      "epoch: 174 of 250\n",
      "training_loss: 0.1230, acc: 0.846, ROC: 0.574, f1: 0.198\n",
      "epoch: 175 of 250\n",
      "training_loss: 0.1131, acc: 0.834, ROC: 0.550, f1: 0.162\n",
      "epoch: 176 of 250\n",
      "training_loss: 0.1255, acc: 0.877, ROC: 0.546, f1: 0.152\n",
      "epoch: 177 of 250\n",
      "training_loss: 0.1009, acc: 0.823, ROC: 0.577, f1: 0.199\n",
      "epoch: 178 of 250\n",
      "training_loss: 0.1154, acc: 0.870, ROC: 0.524, f1: 0.122\n",
      "epoch: 179 of 250\n",
      "training_loss: 0.1017, acc: 0.869, ROC: 0.568, f1: 0.206\n",
      "epoch: 180 of 250\n",
      "training_loss: 0.1035, acc: 0.867, ROC: 0.570, f1: 0.204\n",
      "epoch: 181 of 250\n",
      "training_loss: 0.0953, acc: 0.854, ROC: 0.567, f1: 0.198\n",
      "epoch: 182 of 250\n",
      "training_loss: 0.1077, acc: 0.905, ROC: 0.538, f1: 0.144\n",
      "epoch: 183 of 250\n",
      "training_loss: 0.1325, acc: 0.837, ROC: 0.507, f1: 0.099\n",
      "epoch: 184 of 250\n",
      "training_loss: 0.1079, acc: 0.859, ROC: 0.562, f1: 0.194\n",
      "epoch: 185 of 250\n",
      "training_loss: 0.1029, acc: 0.869, ROC: 0.566, f1: 0.206\n",
      "epoch: 186 of 250\n",
      "training_loss: 0.1053, acc: 0.879, ROC: 0.506, f1: 0.076\n",
      "epoch: 187 of 250\n",
      "training_loss: 0.1186, acc: 0.835, ROC: 0.561, f1: 0.162\n",
      "epoch: 188 of 250\n",
      "training_loss: 0.1142, acc: 0.872, ROC: 0.577, f1: 0.210\n",
      "epoch: 189 of 250\n",
      "training_loss: 0.1123, acc: 0.864, ROC: 0.517, f1: 0.105\n",
      "epoch: 190 of 250\n",
      "training_loss: 0.1060, acc: 0.871, ROC: 0.584, f1: 0.228\n",
      "epoch: 191 of 250\n",
      "training_loss: 0.1008, acc: 0.879, ROC: 0.539, f1: 0.154\n",
      "epoch: 192 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.0907, acc: 0.866, ROC: 0.555, f1: 0.173\n",
      "epoch: 193 of 250\n",
      "training_loss: 0.1018, acc: 0.859, ROC: 0.575, f1: 0.185\n",
      "epoch: 194 of 250\n",
      "training_loss: 0.0875, acc: 0.866, ROC: 0.568, f1: 0.183\n",
      "epoch: 195 of 250\n",
      "training_loss: 0.1082, acc: 0.869, ROC: 0.581, f1: 0.225\n",
      "epoch: 196 of 250\n",
      "training_loss: 0.1001, acc: 0.852, ROC: 0.564, f1: 0.169\n",
      "epoch: 197 of 250\n",
      "training_loss: 0.1031, acc: 0.903, ROC: 0.553, f1: 0.171\n",
      "epoch: 198 of 250\n",
      "training_loss: 0.1122, acc: 0.855, ROC: 0.561, f1: 0.208\n",
      "epoch: 199 of 250\n",
      "training_loss: 0.0949, acc: 0.845, ROC: 0.601, f1: 0.236\n",
      "epoch: 200 of 250\n",
      "training_loss: 0.1079, acc: 0.896, ROC: 0.546, f1: 0.161\n",
      "epoch: 201 of 250\n",
      "training_loss: 0.1098, acc: 0.861, ROC: 0.502, f1: 0.079\n",
      "epoch: 202 of 250\n",
      "training_loss: 0.1150, acc: 0.844, ROC: 0.587, f1: 0.204\n",
      "epoch: 203 of 250\n",
      "training_loss: 0.1061, acc: 0.883, ROC: 0.538, f1: 0.146\n",
      "epoch: 204 of 250\n",
      "training_loss: 0.1238, acc: 0.874, ROC: 0.503, f1: 0.074\n",
      "epoch: 205 of 250\n",
      "training_loss: 0.1150, acc: 0.890, ROC: 0.545, f1: 0.154\n",
      "epoch: 206 of 250\n",
      "training_loss: 0.1087, acc: 0.889, ROC: 0.556, f1: 0.165\n",
      "epoch: 207 of 250\n",
      "training_loss: 0.0728, acc: 0.900, ROC: 0.550, f1: 0.167\n",
      "epoch: 208 of 250\n",
      "training_loss: 0.0959, acc: 0.863, ROC: 0.557, f1: 0.160\n",
      "epoch: 209 of 250\n",
      "training_loss: 0.0905, acc: 0.869, ROC: 0.558, f1: 0.176\n",
      "epoch: 210 of 250\n",
      "training_loss: 0.1022, acc: 0.849, ROC: 0.533, f1: 0.147\n",
      "epoch: 211 of 250\n",
      "training_loss: 0.0984, acc: 0.867, ROC: 0.557, f1: 0.164\n",
      "epoch: 212 of 250\n",
      "training_loss: 0.1023, acc: 0.861, ROC: 0.519, f1: 0.115\n",
      "epoch: 213 of 250\n",
      "training_loss: 0.1241, acc: 0.871, ROC: 0.538, f1: 0.146\n",
      "epoch: 214 of 250\n",
      "training_loss: 0.0909, acc: 0.882, ROC: 0.533, f1: 0.132\n",
      "epoch: 215 of 250\n",
      "training_loss: 0.0992, acc: 0.871, ROC: 0.565, f1: 0.199\n",
      "epoch: 216 of 250\n",
      "training_loss: 0.1080, acc: 0.874, ROC: 0.548, f1: 0.160\n",
      "epoch: 217 of 250\n",
      "training_loss: 0.0878, acc: 0.873, ROC: 0.526, f1: 0.124\n",
      "epoch: 218 of 250\n",
      "training_loss: 0.0967, acc: 0.895, ROC: 0.546, f1: 0.160\n",
      "epoch: 219 of 250\n",
      "training_loss: 0.0854, acc: 0.850, ROC: 0.523, f1: 0.128\n",
      "epoch: 220 of 250\n",
      "training_loss: 0.0769, acc: 0.832, ROC: 0.553, f1: 0.168\n",
      "epoch: 221 of 250\n",
      "training_loss: 0.0833, acc: 0.886, ROC: 0.523, f1: 0.109\n",
      "epoch: 222 of 250\n",
      "training_loss: 0.1019, acc: 0.886, ROC: 0.571, f1: 0.197\n",
      "epoch: 223 of 250\n",
      "training_loss: 0.0946, acc: 0.853, ROC: 0.536, f1: 0.160\n",
      "epoch: 224 of 250\n",
      "training_loss: 0.1002, acc: 0.797, ROC: 0.575, f1: 0.185\n",
      "epoch: 225 of 250\n",
      "training_loss: 0.1099, acc: 0.888, ROC: 0.566, f1: 0.200\n",
      "epoch: 226 of 250\n",
      "training_loss: 0.0923, acc: 0.843, ROC: 0.498, f1: 0.082\n",
      "epoch: 227 of 250\n",
      "training_loss: 0.1010, acc: 0.897, ROC: 0.501, f1: 0.055\n",
      "epoch: 228 of 250\n",
      "training_loss: 0.0826, acc: 0.861, ROC: 0.572, f1: 0.232\n",
      "epoch: 229 of 250\n",
      "training_loss: 0.0790, acc: 0.895, ROC: 0.573, f1: 0.198\n",
      "epoch: 230 of 250\n",
      "training_loss: 0.0920, acc: 0.860, ROC: 0.543, f1: 0.157\n",
      "epoch: 231 of 250\n",
      "training_loss: 0.0810, acc: 0.892, ROC: 0.587, f1: 0.217\n",
      "epoch: 232 of 250\n",
      "training_loss: 0.0899, acc: 0.914, ROC: 0.551, f1: 0.157\n",
      "epoch: 233 of 250\n",
      "training_loss: 0.0802, acc: 0.882, ROC: 0.525, f1: 0.119\n",
      "epoch: 234 of 250\n",
      "training_loss: 0.0984, acc: 0.856, ROC: 0.550, f1: 0.172\n",
      "epoch: 235 of 250\n",
      "training_loss: 0.0798, acc: 0.850, ROC: 0.591, f1: 0.211\n",
      "epoch: 236 of 250\n",
      "training_loss: 0.0834, acc: 0.865, ROC: 0.523, f1: 0.118\n",
      "epoch: 237 of 250\n",
      "training_loss: 0.0954, acc: 0.851, ROC: 0.514, f1: 0.108\n",
      "epoch: 238 of 250\n",
      "training_loss: 0.0870, acc: 0.861, ROC: 0.606, f1: 0.265\n",
      "epoch: 239 of 250\n",
      "training_loss: 0.0974, acc: 0.889, ROC: 0.530, f1: 0.126\n",
      "epoch: 240 of 250\n",
      "training_loss: 0.0830, acc: 0.878, ROC: 0.523, f1: 0.116\n",
      "epoch: 241 of 250\n",
      "training_loss: 0.0911, acc: 0.862, ROC: 0.539, f1: 0.148\n",
      "epoch: 242 of 250\n",
      "training_loss: 0.0791, acc: 0.893, ROC: 0.513, f1: 0.085\n",
      "epoch: 243 of 250\n",
      "training_loss: 0.0894, acc: 0.897, ROC: 0.557, f1: 0.176\n",
      "epoch: 244 of 250\n",
      "training_loss: 0.0857, acc: 0.823, ROC: 0.520, f1: 0.137\n",
      "epoch: 245 of 250\n",
      "training_loss: 0.0712, acc: 0.867, ROC: 0.538, f1: 0.142\n",
      "epoch: 246 of 250\n",
      "training_loss: 0.1028, acc: 0.874, ROC: 0.525, f1: 0.125\n",
      "epoch: 247 of 250\n",
      "training_loss: 0.0935, acc: 0.746, ROC: 0.607, f1: 0.201\n",
      "epoch: 248 of 250\n",
      "training_loss: 0.0850, acc: 0.863, ROC: 0.562, f1: 0.170\n",
      "epoch: 249 of 250\n",
      "training_loss: 0.0689, acc: 0.882, ROC: 0.548, f1: 0.169\n",
      "(19410, 39, 11, 11)\n",
      "epoch: 0 of 250\n",
      "training_loss: 0.6179, acc: 0.737, ROC: 0.631, f1: 0.263\n",
      "epoch: 1 of 250\n",
      "training_loss: 0.5657, acc: 0.815, ROC: 0.560, f1: 0.163\n",
      "epoch: 2 of 250\n",
      "training_loss: 0.5378, acc: 0.794, ROC: 0.574, f1: 0.202\n",
      "epoch: 3 of 250\n",
      "training_loss: 0.5173, acc: 0.741, ROC: 0.608, f1: 0.203\n",
      "epoch: 4 of 250\n",
      "training_loss: 0.4986, acc: 0.760, ROC: 0.577, f1: 0.200\n",
      "epoch: 5 of 250\n",
      "training_loss: 0.4597, acc: 0.782, ROC: 0.605, f1: 0.253\n",
      "epoch: 6 of 250\n",
      "training_loss: 0.4461, acc: 0.652, ROC: 0.561, f1: 0.191\n",
      "epoch: 7 of 250\n",
      "training_loss: 0.4549, acc: 0.797, ROC: 0.580, f1: 0.204\n",
      "epoch: 8 of 250\n",
      "training_loss: 0.4218, acc: 0.743, ROC: 0.569, f1: 0.224\n",
      "epoch: 9 of 250\n",
      "training_loss: 0.4249, acc: 0.781, ROC: 0.592, f1: 0.204\n",
      "epoch: 10 of 250\n",
      "training_loss: 0.4171, acc: 0.823, ROC: 0.583, f1: 0.220\n",
      "epoch: 11 of 250\n",
      "training_loss: 0.3876, acc: 0.810, ROC: 0.570, f1: 0.208\n",
      "epoch: 12 of 250\n",
      "training_loss: 0.3676, acc: 0.758, ROC: 0.606, f1: 0.229\n",
      "epoch: 13 of 250\n",
      "training_loss: 0.3710, acc: 0.861, ROC: 0.575, f1: 0.215\n",
      "epoch: 14 of 250\n",
      "training_loss: 0.3817, acc: 0.870, ROC: 0.608, f1: 0.278\n",
      "epoch: 15 of 250\n",
      "training_loss: 0.3385, acc: 0.797, ROC: 0.587, f1: 0.210\n",
      "epoch: 16 of 250\n",
      "training_loss: 0.3296, acc: 0.749, ROC: 0.599, f1: 0.237\n",
      "epoch: 17 of 250\n",
      "training_loss: 0.3372, acc: 0.779, ROC: 0.577, f1: 0.225\n",
      "epoch: 18 of 250\n",
      "training_loss: 0.3553, acc: 0.825, ROC: 0.547, f1: 0.178\n",
      "epoch: 19 of 250\n",
      "training_loss: 0.3291, acc: 0.814, ROC: 0.563, f1: 0.198\n",
      "epoch: 20 of 250\n",
      "training_loss: 0.3187, acc: 0.819, ROC: 0.547, f1: 0.181\n",
      "epoch: 21 of 250\n",
      "training_loss: 0.3199, acc: 0.831, ROC: 0.536, f1: 0.151\n",
      "epoch: 22 of 250\n",
      "training_loss: 0.3154, acc: 0.831, ROC: 0.578, f1: 0.235\n",
      "epoch: 23 of 250\n",
      "training_loss: 0.2736, acc: 0.822, ROC: 0.583, f1: 0.219\n",
      "epoch: 24 of 250\n",
      "training_loss: 0.2831, acc: 0.868, ROC: 0.537, f1: 0.154\n",
      "epoch: 25 of 250\n",
      "training_loss: 0.2920, acc: 0.794, ROC: 0.551, f1: 0.169\n",
      "epoch: 26 of 250\n",
      "training_loss: 0.2843, acc: 0.798, ROC: 0.556, f1: 0.185\n",
      "epoch: 27 of 250\n",
      "training_loss: 0.2933, acc: 0.846, ROC: 0.520, f1: 0.125\n",
      "epoch: 28 of 250\n",
      "training_loss: 0.2696, acc: 0.854, ROC: 0.580, f1: 0.247\n",
      "epoch: 29 of 250\n",
      "training_loss: 0.2715, acc: 0.871, ROC: 0.582, f1: 0.237\n",
      "epoch: 30 of 250\n",
      "training_loss: 0.2773, acc: 0.858, ROC: 0.540, f1: 0.155\n",
      "epoch: 31 of 250\n",
      "training_loss: 0.2516, acc: 0.839, ROC: 0.565, f1: 0.207\n",
      "epoch: 32 of 250\n",
      "training_loss: 0.2431, acc: 0.839, ROC: 0.563, f1: 0.199\n",
      "epoch: 33 of 250\n",
      "training_loss: 0.2678, acc: 0.881, ROC: 0.541, f1: 0.156\n",
      "epoch: 34 of 250\n",
      "training_loss: 0.2520, acc: 0.816, ROC: 0.479, f1: 0.061\n",
      "epoch: 35 of 250\n",
      "training_loss: 0.2454, acc: 0.820, ROC: 0.551, f1: 0.182\n",
      "epoch: 36 of 250\n",
      "training_loss: 0.2697, acc: 0.841, ROC: 0.575, f1: 0.201\n",
      "epoch: 37 of 250\n",
      "training_loss: 0.2338, acc: 0.753, ROC: 0.589, f1: 0.235\n",
      "epoch: 38 of 250\n",
      "training_loss: 0.2480, acc: 0.862, ROC: 0.538, f1: 0.148\n",
      "epoch: 39 of 250\n",
      "training_loss: 0.2543, acc: 0.878, ROC: 0.513, f1: 0.090\n",
      "epoch: 40 of 250\n",
      "training_loss: 0.2547, acc: 0.841, ROC: 0.564, f1: 0.209\n",
      "epoch: 41 of 250\n",
      "training_loss: 0.2373, acc: 0.882, ROC: 0.600, f1: 0.263\n",
      "epoch: 42 of 250\n",
      "training_loss: 0.2353, acc: 0.802, ROC: 0.591, f1: 0.227\n",
      "epoch: 43 of 250\n",
      "training_loss: 0.2240, acc: 0.789, ROC: 0.555, f1: 0.179\n",
      "epoch: 44 of 250\n",
      "training_loss: 0.2403, acc: 0.796, ROC: 0.618, f1: 0.233\n",
      "epoch: 45 of 250\n",
      "training_loss: 0.2222, acc: 0.878, ROC: 0.568, f1: 0.218\n",
      "epoch: 46 of 250\n",
      "training_loss: 0.1963, acc: 0.864, ROC: 0.557, f1: 0.181\n",
      "epoch: 47 of 250\n",
      "training_loss: 0.2367, acc: 0.814, ROC: 0.580, f1: 0.191\n",
      "epoch: 48 of 250\n",
      "training_loss: 0.2148, acc: 0.876, ROC: 0.529, f1: 0.127\n",
      "epoch: 49 of 250\n",
      "training_loss: 0.2348, acc: 0.809, ROC: 0.549, f1: 0.173\n",
      "epoch: 50 of 250\n",
      "training_loss: 0.1925, acc: 0.800, ROC: 0.580, f1: 0.219\n",
      "epoch: 51 of 250\n",
      "training_loss: 0.2259, acc: 0.812, ROC: 0.603, f1: 0.260\n",
      "epoch: 52 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.2104, acc: 0.810, ROC: 0.554, f1: 0.159\n",
      "epoch: 53 of 250\n",
      "training_loss: 0.1983, acc: 0.824, ROC: 0.609, f1: 0.279\n",
      "epoch: 54 of 250\n",
      "training_loss: 0.1972, acc: 0.851, ROC: 0.492, f1: 0.063\n",
      "epoch: 55 of 250\n",
      "training_loss: 0.1893, acc: 0.856, ROC: 0.539, f1: 0.153\n",
      "epoch: 56 of 250\n",
      "training_loss: 0.2136, acc: 0.825, ROC: 0.592, f1: 0.236\n",
      "epoch: 57 of 250\n",
      "training_loss: 0.1877, acc: 0.818, ROC: 0.542, f1: 0.165\n",
      "epoch: 58 of 250\n",
      "training_loss: 0.1934, acc: 0.812, ROC: 0.576, f1: 0.203\n",
      "epoch: 59 of 250\n",
      "training_loss: 0.2185, acc: 0.749, ROC: 0.597, f1: 0.223\n",
      "epoch: 60 of 250\n",
      "training_loss: 0.1891, acc: 0.853, ROC: 0.588, f1: 0.222\n",
      "epoch: 61 of 250\n",
      "training_loss: 0.1834, acc: 0.743, ROC: 0.546, f1: 0.189\n",
      "epoch: 62 of 250\n",
      "training_loss: 0.1876, acc: 0.856, ROC: 0.525, f1: 0.122\n",
      "epoch: 63 of 250\n",
      "training_loss: 0.2011, acc: 0.826, ROC: 0.550, f1: 0.179\n",
      "epoch: 64 of 250\n",
      "training_loss: 0.1850, acc: 0.808, ROC: 0.592, f1: 0.220\n",
      "epoch: 65 of 250\n",
      "training_loss: 0.1853, acc: 0.865, ROC: 0.543, f1: 0.161\n",
      "epoch: 66 of 250\n",
      "training_loss: 0.1809, acc: 0.828, ROC: 0.516, f1: 0.122\n",
      "epoch: 67 of 250\n",
      "training_loss: 0.1685, acc: 0.884, ROC: 0.541, f1: 0.147\n",
      "epoch: 68 of 250\n",
      "training_loss: 0.1743, acc: 0.845, ROC: 0.603, f1: 0.265\n",
      "epoch: 69 of 250\n",
      "training_loss: 0.1826, acc: 0.730, ROC: 0.570, f1: 0.187\n",
      "epoch: 70 of 250\n",
      "training_loss: 0.1863, acc: 0.857, ROC: 0.533, f1: 0.144\n",
      "epoch: 71 of 250\n",
      "training_loss: 0.1621, acc: 0.820, ROC: 0.570, f1: 0.224\n",
      "epoch: 72 of 250\n",
      "training_loss: 0.1773, acc: 0.862, ROC: 0.532, f1: 0.137\n",
      "epoch: 73 of 250\n",
      "training_loss: 0.1759, acc: 0.871, ROC: 0.536, f1: 0.146\n",
      "epoch: 74 of 250\n",
      "training_loss: 0.1721, acc: 0.801, ROC: 0.620, f1: 0.226\n",
      "epoch: 75 of 250\n",
      "training_loss: 0.1609, acc: 0.838, ROC: 0.588, f1: 0.229\n",
      "epoch: 76 of 250\n",
      "training_loss: 0.1561, acc: 0.789, ROC: 0.581, f1: 0.227\n",
      "epoch: 77 of 250\n",
      "training_loss: 0.1497, acc: 0.865, ROC: 0.498, f1: 0.069\n",
      "epoch: 78 of 250\n",
      "training_loss: 0.1657, acc: 0.766, ROC: 0.595, f1: 0.220\n",
      "epoch: 79 of 250\n",
      "training_loss: 0.1674, acc: 0.849, ROC: 0.546, f1: 0.166\n",
      "epoch: 80 of 250\n",
      "training_loss: 0.1557, acc: 0.845, ROC: 0.512, f1: 0.104\n",
      "epoch: 81 of 250\n",
      "training_loss: 0.1618, acc: 0.873, ROC: 0.541, f1: 0.159\n",
      "epoch: 82 of 250\n",
      "training_loss: 0.1687, acc: 0.872, ROC: 0.554, f1: 0.179\n",
      "epoch: 83 of 250\n",
      "training_loss: 0.1545, acc: 0.829, ROC: 0.528, f1: 0.149\n",
      "epoch: 84 of 250\n",
      "training_loss: 0.1728, acc: 0.821, ROC: 0.527, f1: 0.152\n",
      "epoch: 85 of 250\n",
      "training_loss: 0.1595, acc: 0.805, ROC: 0.564, f1: 0.191\n",
      "epoch: 86 of 250\n",
      "training_loss: 0.1372, acc: 0.845, ROC: 0.582, f1: 0.244\n",
      "epoch: 87 of 250\n",
      "training_loss: 0.1613, acc: 0.892, ROC: 0.556, f1: 0.182\n",
      "epoch: 88 of 250\n",
      "training_loss: 0.1567, acc: 0.877, ROC: 0.560, f1: 0.196\n",
      "epoch: 89 of 250\n",
      "training_loss: 0.1293, acc: 0.847, ROC: 0.581, f1: 0.223\n",
      "epoch: 90 of 250\n",
      "training_loss: 0.1568, acc: 0.844, ROC: 0.589, f1: 0.228\n",
      "epoch: 91 of 250\n",
      "training_loss: 0.1489, acc: 0.814, ROC: 0.536, f1: 0.170\n",
      "epoch: 92 of 250\n",
      "training_loss: 0.1455, acc: 0.846, ROC: 0.592, f1: 0.245\n",
      "epoch: 93 of 250\n",
      "training_loss: 0.1381, acc: 0.875, ROC: 0.538, f1: 0.150\n",
      "epoch: 94 of 250\n",
      "training_loss: 0.1528, acc: 0.851, ROC: 0.563, f1: 0.203\n",
      "epoch: 95 of 250\n",
      "training_loss: 0.1602, acc: 0.857, ROC: 0.601, f1: 0.251\n",
      "epoch: 96 of 250\n",
      "training_loss: 0.1390, acc: 0.785, ROC: 0.568, f1: 0.207\n",
      "epoch: 97 of 250\n",
      "training_loss: 0.1552, acc: 0.831, ROC: 0.603, f1: 0.256\n",
      "epoch: 98 of 250\n",
      "training_loss: 0.1440, acc: 0.819, ROC: 0.572, f1: 0.223\n",
      "epoch: 99 of 250\n",
      "training_loss: 0.1442, acc: 0.841, ROC: 0.580, f1: 0.224\n",
      "epoch: 100 of 250\n",
      "training_loss: 0.1394, acc: 0.871, ROC: 0.627, f1: 0.271\n",
      "epoch: 101 of 250\n",
      "training_loss: 0.1384, acc: 0.818, ROC: 0.573, f1: 0.222\n",
      "epoch: 102 of 250\n",
      "training_loss: 0.1331, acc: 0.803, ROC: 0.564, f1: 0.189\n",
      "epoch: 103 of 250\n",
      "training_loss: 0.1391, acc: 0.775, ROC: 0.609, f1: 0.247\n",
      "epoch: 104 of 250\n",
      "training_loss: 0.1437, acc: 0.847, ROC: 0.568, f1: 0.207\n",
      "epoch: 105 of 250\n",
      "training_loss: 0.1367, acc: 0.826, ROC: 0.590, f1: 0.216\n",
      "epoch: 106 of 250\n",
      "training_loss: 0.1170, acc: 0.814, ROC: 0.558, f1: 0.177\n",
      "epoch: 107 of 250\n",
      "training_loss: 0.1298, acc: 0.870, ROC: 0.588, f1: 0.244\n",
      "epoch: 108 of 250\n",
      "training_loss: 0.1125, acc: 0.780, ROC: 0.624, f1: 0.220\n",
      "epoch: 109 of 250\n",
      "training_loss: 0.1478, acc: 0.799, ROC: 0.556, f1: 0.186\n",
      "epoch: 110 of 250\n",
      "training_loss: 0.1399, acc: 0.867, ROC: 0.593, f1: 0.257\n",
      "epoch: 111 of 250\n",
      "training_loss: 0.1285, acc: 0.800, ROC: 0.535, f1: 0.145\n",
      "epoch: 112 of 250\n",
      "training_loss: 0.1196, acc: 0.857, ROC: 0.534, f1: 0.144\n",
      "epoch: 113 of 250\n",
      "training_loss: 0.1413, acc: 0.856, ROC: 0.556, f1: 0.172\n",
      "epoch: 114 of 250\n",
      "training_loss: 0.1180, acc: 0.854, ROC: 0.562, f1: 0.207\n",
      "epoch: 115 of 250\n",
      "training_loss: 0.1337, acc: 0.843, ROC: 0.544, f1: 0.178\n",
      "epoch: 116 of 250\n",
      "training_loss: 0.1150, acc: 0.870, ROC: 0.528, f1: 0.133\n",
      "epoch: 117 of 250\n",
      "training_loss: 0.1169, acc: 0.842, ROC: 0.544, f1: 0.168\n",
      "epoch: 118 of 250\n",
      "training_loss: 0.1244, acc: 0.790, ROC: 0.541, f1: 0.167\n",
      "epoch: 119 of 250\n",
      "training_loss: 0.1083, acc: 0.868, ROC: 0.577, f1: 0.224\n",
      "epoch: 120 of 250\n",
      "training_loss: 0.1166, acc: 0.856, ROC: 0.534, f1: 0.143\n",
      "epoch: 121 of 250\n",
      "training_loss: 0.1373, acc: 0.819, ROC: 0.615, f1: 0.279\n",
      "epoch: 122 of 250\n",
      "training_loss: 0.1166, acc: 0.856, ROC: 0.559, f1: 0.200\n",
      "epoch: 123 of 250\n",
      "training_loss: 0.1418, acc: 0.856, ROC: 0.527, f1: 0.122\n",
      "epoch: 124 of 250\n",
      "training_loss: 0.1187, acc: 0.800, ROC: 0.633, f1: 0.254\n",
      "epoch: 125 of 250\n",
      "training_loss: 0.1090, acc: 0.824, ROC: 0.516, f1: 0.129\n",
      "epoch: 126 of 250\n",
      "training_loss: 0.1353, acc: 0.792, ROC: 0.533, f1: 0.148\n",
      "epoch: 127 of 250\n",
      "training_loss: 0.1047, acc: 0.855, ROC: 0.533, f1: 0.142\n",
      "epoch: 128 of 250\n",
      "training_loss: 0.1263, acc: 0.862, ROC: 0.580, f1: 0.233\n",
      "epoch: 129 of 250\n",
      "training_loss: 0.1147, acc: 0.842, ROC: 0.580, f1: 0.218\n",
      "epoch: 130 of 250\n",
      "training_loss: 0.1282, acc: 0.849, ROC: 0.554, f1: 0.166\n",
      "epoch: 131 of 250\n",
      "training_loss: 0.1174, acc: 0.848, ROC: 0.556, f1: 0.174\n",
      "epoch: 132 of 250\n",
      "training_loss: 0.1201, acc: 0.827, ROC: 0.593, f1: 0.258\n",
      "epoch: 133 of 250\n",
      "training_loss: 0.1124, acc: 0.874, ROC: 0.547, f1: 0.171\n",
      "epoch: 134 of 250\n",
      "training_loss: 0.1225, acc: 0.811, ROC: 0.520, f1: 0.137\n",
      "epoch: 135 of 250\n",
      "training_loss: 0.1101, acc: 0.789, ROC: 0.562, f1: 0.185\n",
      "epoch: 136 of 250\n",
      "training_loss: 0.1331, acc: 0.810, ROC: 0.540, f1: 0.167\n",
      "epoch: 137 of 250\n",
      "training_loss: 0.1243, acc: 0.870, ROC: 0.523, f1: 0.122\n",
      "epoch: 138 of 250\n",
      "training_loss: 0.1138, acc: 0.864, ROC: 0.541, f1: 0.160\n",
      "epoch: 139 of 250\n",
      "training_loss: 0.1115, acc: 0.874, ROC: 0.560, f1: 0.203\n",
      "epoch: 140 of 250\n",
      "training_loss: 0.1161, acc: 0.891, ROC: 0.508, f1: 0.068\n",
      "epoch: 141 of 250\n",
      "training_loss: 0.1092, acc: 0.838, ROC: 0.547, f1: 0.182\n",
      "epoch: 142 of 250\n",
      "training_loss: 0.1004, acc: 0.863, ROC: 0.571, f1: 0.226\n",
      "epoch: 143 of 250\n",
      "training_loss: 0.1010, acc: 0.853, ROC: 0.571, f1: 0.197\n",
      "epoch: 144 of 250\n",
      "training_loss: 0.0986, acc: 0.818, ROC: 0.555, f1: 0.188\n",
      "epoch: 145 of 250\n",
      "training_loss: 0.1043, acc: 0.818, ROC: 0.553, f1: 0.202\n",
      "epoch: 146 of 250\n",
      "training_loss: 0.1007, acc: 0.876, ROC: 0.507, f1: 0.075\n",
      "epoch: 147 of 250\n",
      "training_loss: 0.0894, acc: 0.832, ROC: 0.549, f1: 0.168\n",
      "epoch: 148 of 250\n",
      "training_loss: 0.0965, acc: 0.858, ROC: 0.588, f1: 0.245\n",
      "epoch: 149 of 250\n",
      "training_loss: 0.1212, acc: 0.852, ROC: 0.513, f1: 0.108\n",
      "epoch: 150 of 250\n",
      "training_loss: 0.1024, acc: 0.857, ROC: 0.574, f1: 0.227\n",
      "epoch: 151 of 250\n",
      "training_loss: 0.1003, acc: 0.819, ROC: 0.587, f1: 0.249\n",
      "epoch: 152 of 250\n",
      "training_loss: 0.1095, acc: 0.844, ROC: 0.524, f1: 0.133\n",
      "epoch: 153 of 250\n",
      "training_loss: 0.0954, acc: 0.863, ROC: 0.562, f1: 0.208\n",
      "epoch: 154 of 250\n",
      "training_loss: 0.0956, acc: 0.811, ROC: 0.559, f1: 0.189\n",
      "epoch: 155 of 250\n",
      "training_loss: 0.1049, acc: 0.849, ROC: 0.529, f1: 0.147\n",
      "epoch: 156 of 250\n",
      "training_loss: 0.1138, acc: 0.874, ROC: 0.570, f1: 0.203\n",
      "epoch: 157 of 250\n",
      "training_loss: 0.1007, acc: 0.824, ROC: 0.556, f1: 0.193\n",
      "epoch: 158 of 250\n",
      "training_loss: 0.1040, acc: 0.763, ROC: 0.562, f1: 0.174\n",
      "epoch: 159 of 250\n",
      "training_loss: 0.1138, acc: 0.857, ROC: 0.565, f1: 0.210\n",
      "epoch: 160 of 250\n",
      "training_loss: 0.1095, acc: 0.808, ROC: 0.556, f1: 0.193\n",
      "epoch: 161 of 250\n",
      "training_loss: 0.1150, acc: 0.868, ROC: 0.588, f1: 0.241\n",
      "epoch: 162 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.0942, acc: 0.819, ROC: 0.588, f1: 0.210\n",
      "epoch: 163 of 250\n",
      "training_loss: 0.0943, acc: 0.872, ROC: 0.525, f1: 0.123\n",
      "epoch: 164 of 250\n",
      "training_loss: 0.1265, acc: 0.855, ROC: 0.571, f1: 0.225\n",
      "epoch: 165 of 250\n",
      "training_loss: 0.0910, acc: 0.860, ROC: 0.533, f1: 0.146\n",
      "epoch: 166 of 250\n",
      "training_loss: 0.1059, acc: 0.883, ROC: 0.568, f1: 0.215\n",
      "epoch: 167 of 250\n",
      "training_loss: 0.1207, acc: 0.836, ROC: 0.553, f1: 0.188\n",
      "epoch: 168 of 250\n",
      "training_loss: 0.1089, acc: 0.824, ROC: 0.554, f1: 0.193\n",
      "epoch: 169 of 250\n",
      "training_loss: 0.0955, acc: 0.789, ROC: 0.554, f1: 0.185\n",
      "epoch: 170 of 250\n",
      "training_loss: 0.0917, acc: 0.846, ROC: 0.548, f1: 0.181\n",
      "epoch: 171 of 250\n",
      "training_loss: 0.0817, acc: 0.878, ROC: 0.530, f1: 0.129\n",
      "epoch: 172 of 250\n",
      "training_loss: 0.0883, acc: 0.843, ROC: 0.519, f1: 0.123\n",
      "epoch: 173 of 250\n",
      "training_loss: 0.1142, acc: 0.873, ROC: 0.509, f1: 0.086\n",
      "epoch: 174 of 250\n",
      "training_loss: 0.0919, acc: 0.863, ROC: 0.556, f1: 0.199\n",
      "epoch: 175 of 250\n",
      "training_loss: 0.0918, acc: 0.867, ROC: 0.558, f1: 0.204\n",
      "epoch: 176 of 250\n",
      "training_loss: 0.1067, acc: 0.870, ROC: 0.582, f1: 0.244\n",
      "epoch: 177 of 250\n",
      "training_loss: 0.1217, acc: 0.855, ROC: 0.544, f1: 0.152\n",
      "epoch: 178 of 250\n",
      "training_loss: 0.0850, acc: 0.859, ROC: 0.571, f1: 0.221\n",
      "epoch: 179 of 250\n",
      "training_loss: 0.0878, acc: 0.819, ROC: 0.562, f1: 0.203\n",
      "epoch: 180 of 250\n",
      "training_loss: 0.0982, acc: 0.857, ROC: 0.556, f1: 0.201\n",
      "epoch: 181 of 250\n",
      "training_loss: 0.1028, acc: 0.822, ROC: 0.563, f1: 0.212\n",
      "epoch: 182 of 250\n",
      "training_loss: 0.0854, acc: 0.815, ROC: 0.578, f1: 0.192\n",
      "epoch: 183 of 250\n",
      "training_loss: 0.0966, acc: 0.873, ROC: 0.590, f1: 0.221\n",
      "epoch: 184 of 250\n",
      "training_loss: 0.1048, acc: 0.812, ROC: 0.581, f1: 0.203\n",
      "epoch: 185 of 250\n",
      "training_loss: 0.0895, acc: 0.903, ROC: 0.525, f1: 0.110\n",
      "epoch: 186 of 250\n",
      "training_loss: 0.0868, acc: 0.849, ROC: 0.567, f1: 0.209\n",
      "epoch: 187 of 250\n",
      "training_loss: 0.0904, acc: 0.868, ROC: 0.565, f1: 0.195\n",
      "epoch: 188 of 250\n",
      "training_loss: 0.0975, acc: 0.854, ROC: 0.528, f1: 0.141\n",
      "epoch: 189 of 250\n",
      "training_loss: 0.0805, acc: 0.840, ROC: 0.582, f1: 0.231\n",
      "epoch: 190 of 250\n",
      "training_loss: 0.0916, acc: 0.811, ROC: 0.584, f1: 0.265\n",
      "epoch: 191 of 250\n",
      "training_loss: 0.0765, acc: 0.857, ROC: 0.563, f1: 0.201\n",
      "epoch: 192 of 250\n",
      "training_loss: 0.0797, acc: 0.861, ROC: 0.522, f1: 0.115\n",
      "epoch: 193 of 250\n",
      "training_loss: 0.0850, acc: 0.818, ROC: 0.557, f1: 0.195\n",
      "epoch: 194 of 250\n",
      "training_loss: 0.0819, acc: 0.881, ROC: 0.548, f1: 0.179\n",
      "epoch: 195 of 250\n",
      "training_loss: 0.0785, acc: 0.824, ROC: 0.577, f1: 0.221\n",
      "epoch: 196 of 250\n",
      "training_loss: 0.0869, acc: 0.841, ROC: 0.562, f1: 0.185\n",
      "epoch: 197 of 250\n",
      "training_loss: 0.0760, acc: 0.842, ROC: 0.576, f1: 0.218\n",
      "epoch: 198 of 250\n",
      "training_loss: 0.0659, acc: 0.808, ROC: 0.554, f1: 0.193\n",
      "epoch: 199 of 250\n",
      "training_loss: 0.0822, acc: 0.851, ROC: 0.513, f1: 0.108\n",
      "epoch: 200 of 250\n",
      "training_loss: 0.0996, acc: 0.781, ROC: 0.559, f1: 0.186\n",
      "epoch: 201 of 250\n",
      "training_loss: 0.0780, acc: 0.854, ROC: 0.581, f1: 0.240\n",
      "epoch: 202 of 250\n",
      "training_loss: 0.0739, acc: 0.859, ROC: 0.559, f1: 0.194\n",
      "epoch: 203 of 250\n",
      "training_loss: 0.0807, acc: 0.840, ROC: 0.567, f1: 0.200\n",
      "epoch: 204 of 250\n",
      "training_loss: 0.0883, acc: 0.854, ROC: 0.529, f1: 0.141\n",
      "epoch: 205 of 250\n",
      "training_loss: 0.1000, acc: 0.826, ROC: 0.550, f1: 0.187\n",
      "epoch: 206 of 250\n",
      "training_loss: 0.0792, acc: 0.876, ROC: 0.533, f1: 0.139\n",
      "epoch: 207 of 250\n",
      "training_loss: 0.0674, acc: 0.894, ROC: 0.574, f1: 0.197\n",
      "epoch: 208 of 250\n",
      "training_loss: 0.0854, acc: 0.865, ROC: 0.599, f1: 0.237\n",
      "epoch: 209 of 250\n",
      "training_loss: 0.0991, acc: 0.882, ROC: 0.550, f1: 0.169\n",
      "epoch: 210 of 250\n",
      "training_loss: 0.0814, acc: 0.862, ROC: 0.567, f1: 0.216\n",
      "epoch: 211 of 250\n",
      "training_loss: 0.0747, acc: 0.814, ROC: 0.534, f1: 0.162\n",
      "epoch: 212 of 250\n",
      "training_loss: 0.0780, acc: 0.844, ROC: 0.556, f1: 0.196\n",
      "epoch: 213 of 250\n",
      "training_loss: 0.0806, acc: 0.853, ROC: 0.524, f1: 0.130\n",
      "epoch: 214 of 250\n",
      "training_loss: 0.0807, acc: 0.835, ROC: 0.528, f1: 0.127\n",
      "epoch: 215 of 250\n",
      "training_loss: 0.0780, acc: 0.858, ROC: 0.543, f1: 0.165\n",
      "epoch: 216 of 250\n",
      "training_loss: 0.0914, acc: 0.861, ROC: 0.537, f1: 0.147\n",
      "epoch: 217 of 250\n",
      "training_loss: 0.0691, acc: 0.836, ROC: 0.564, f1: 0.212\n",
      "epoch: 218 of 250\n",
      "training_loss: 0.0770, acc: 0.856, ROC: 0.569, f1: 0.209\n",
      "epoch: 219 of 250\n",
      "training_loss: 0.0772, acc: 0.863, ROC: 0.566, f1: 0.208\n",
      "epoch: 220 of 250\n",
      "training_loss: 0.0729, acc: 0.876, ROC: 0.589, f1: 0.244\n",
      "epoch: 221 of 250\n",
      "training_loss: 0.0773, acc: 0.871, ROC: 0.549, f1: 0.178\n",
      "epoch: 222 of 250\n",
      "training_loss: 0.0764, acc: 0.862, ROC: 0.530, f1: 0.137\n",
      "epoch: 223 of 250\n",
      "training_loss: 0.0823, acc: 0.873, ROC: 0.562, f1: 0.211\n",
      "epoch: 224 of 250\n",
      "training_loss: 0.0780, acc: 0.860, ROC: 0.556, f1: 0.195\n",
      "epoch: 225 of 250\n",
      "training_loss: 0.0760, acc: 0.867, ROC: 0.562, f1: 0.213\n",
      "epoch: 226 of 250\n",
      "training_loss: 0.0579, acc: 0.808, ROC: 0.588, f1: 0.226\n",
      "epoch: 227 of 250\n",
      "training_loss: 0.0880, acc: 0.861, ROC: 0.553, f1: 0.178\n",
      "epoch: 228 of 250\n",
      "training_loss: 0.0770, acc: 0.830, ROC: 0.583, f1: 0.227\n",
      "epoch: 229 of 250\n",
      "training_loss: 0.0702, acc: 0.837, ROC: 0.550, f1: 0.181\n",
      "epoch: 230 of 250\n",
      "training_loss: 0.0696, acc: 0.864, ROC: 0.551, f1: 0.181\n",
      "epoch: 231 of 250\n",
      "training_loss: 0.0743, acc: 0.840, ROC: 0.595, f1: 0.238\n",
      "epoch: 232 of 250\n",
      "training_loss: 0.0647, acc: 0.889, ROC: 0.528, f1: 0.126\n",
      "epoch: 233 of 250\n",
      "training_loss: 0.0842, acc: 0.879, ROC: 0.570, f1: 0.229\n",
      "epoch: 234 of 250\n",
      "training_loss: 0.0697, acc: 0.836, ROC: 0.549, f1: 0.155\n",
      "epoch: 235 of 250\n",
      "training_loss: 0.0704, acc: 0.863, ROC: 0.597, f1: 0.235\n",
      "epoch: 236 of 250\n",
      "training_loss: 0.0748, acc: 0.849, ROC: 0.572, f1: 0.201\n",
      "epoch: 237 of 250\n",
      "training_loss: 0.0663, acc: 0.815, ROC: 0.555, f1: 0.185\n",
      "epoch: 238 of 250\n",
      "training_loss: 0.0620, acc: 0.881, ROC: 0.524, f1: 0.119\n",
      "epoch: 239 of 250\n",
      "training_loss: 0.0754, acc: 0.865, ROC: 0.584, f1: 0.237\n",
      "epoch: 240 of 250\n",
      "training_loss: 0.0714, acc: 0.829, ROC: 0.618, f1: 0.253\n",
      "epoch: 241 of 250\n",
      "training_loss: 0.0672, acc: 0.857, ROC: 0.528, f1: 0.133\n",
      "epoch: 242 of 250\n",
      "training_loss: 0.0609, acc: 0.878, ROC: 0.559, f1: 0.197\n",
      "epoch: 243 of 250\n",
      "training_loss: 0.0787, acc: 0.881, ROC: 0.598, f1: 0.242\n",
      "epoch: 244 of 250\n",
      "training_loss: 0.0685, acc: 0.835, ROC: 0.517, f1: 0.127\n",
      "epoch: 245 of 250\n",
      "training_loss: 0.0846, acc: 0.803, ROC: 0.603, f1: 0.245\n",
      "epoch: 246 of 250\n",
      "training_loss: 0.0710, acc: 0.851, ROC: 0.535, f1: 0.149\n",
      "epoch: 247 of 250\n",
      "training_loss: 0.0758, acc: 0.856, ROC: 0.538, f1: 0.153\n",
      "epoch: 248 of 250\n",
      "training_loss: 0.0767, acc: 0.837, ROC: 0.537, f1: 0.147\n",
      "epoch: 249 of 250\n",
      "training_loss: 0.0920, acc: 0.864, ROC: 0.544, f1: 0.171\n",
      "{1: Model1(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(39, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Flatten()\n",
      "    (8): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (9): ReLU()\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "), 0: Model1(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(39, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Flatten()\n",
      "    (8): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (9): ReLU()\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "), 2: Model1(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(39, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Flatten()\n",
      "    (8): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (9): ReLU()\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10784, 39, 11, 11)\n",
      "epoch: 0 of 250\n",
      "training_loss: 0.5819, acc: 0.829, ROC: 0.527, f1: 0.132\n",
      "epoch: 1 of 250\n",
      "training_loss: 0.4639, acc: 0.763, ROC: 0.547, f1: 0.163\n",
      "epoch: 2 of 250\n",
      "training_loss: 0.4162, acc: 0.876, ROC: 0.528, f1: 0.114\n",
      "epoch: 3 of 250\n",
      "training_loss: 0.3278, acc: 0.858, ROC: 0.518, f1: 0.101\n",
      "epoch: 4 of 250\n",
      "training_loss: 0.2822, acc: 0.886, ROC: 0.496, f1: 0.050\n",
      "epoch: 5 of 250\n",
      "training_loss: 0.2155, acc: 0.922, ROC: 0.517, f1: 0.071\n",
      "epoch: 6 of 250\n",
      "training_loss: 0.2093, acc: 0.823, ROC: 0.523, f1: 0.128\n",
      "epoch: 7 of 250\n",
      "training_loss: 0.1494, acc: 0.915, ROC: 0.546, f1: 0.158\n",
      "epoch: 8 of 250\n",
      "training_loss: 0.1423, acc: 0.866, ROC: 0.547, f1: 0.163\n",
      "epoch: 9 of 250\n",
      "training_loss: 0.1110, acc: 0.892, ROC: 0.538, f1: 0.143\n",
      "epoch: 10 of 250\n",
      "training_loss: 0.0874, acc: 0.868, ROC: 0.535, f1: 0.143\n",
      "epoch: 11 of 250\n",
      "training_loss: 0.0600, acc: 0.873, ROC: 0.527, f1: 0.124\n",
      "epoch: 12 of 250\n",
      "training_loss: 0.0512, acc: 0.896, ROC: 0.583, f1: 0.212\n",
      "epoch: 13 of 250\n",
      "training_loss: 0.0517, acc: 0.872, ROC: 0.544, f1: 0.169\n",
      "epoch: 14 of 250\n",
      "training_loss: 0.0361, acc: 0.877, ROC: 0.584, f1: 0.245\n",
      "epoch: 15 of 250\n",
      "training_loss: 0.0425, acc: 0.884, ROC: 0.526, f1: 0.121\n",
      "epoch: 16 of 250\n",
      "training_loss: 0.0225, acc: 0.890, ROC: 0.559, f1: 0.191\n",
      "epoch: 17 of 250\n",
      "training_loss: 0.0173, acc: 0.880, ROC: 0.555, f1: 0.178\n",
      "epoch: 18 of 250\n",
      "training_loss: 0.0216, acc: 0.881, ROC: 0.538, f1: 0.144\n",
      "epoch: 19 of 250\n",
      "training_loss: 0.0222, acc: 0.894, ROC: 0.526, f1: 0.117\n",
      "epoch: 20 of 250\n",
      "training_loss: 0.0207, acc: 0.894, ROC: 0.539, f1: 0.145\n",
      "epoch: 21 of 250\n",
      "training_loss: 0.0175, acc: 0.894, ROC: 0.542, f1: 0.145\n",
      "epoch: 22 of 250\n",
      "training_loss: 0.0178, acc: 0.868, ROC: 0.534, f1: 0.143\n",
      "epoch: 23 of 250\n",
      "training_loss: 0.0124, acc: 0.901, ROC: 0.537, f1: 0.139\n",
      "epoch: 24 of 250\n",
      "training_loss: 0.0069, acc: 0.893, ROC: 0.542, f1: 0.157\n",
      "epoch: 25 of 250\n",
      "training_loss: 0.0079, acc: 0.900, ROC: 0.560, f1: 0.194\n",
      "epoch: 26 of 250\n",
      "training_loss: 0.0047, acc: 0.884, ROC: 0.575, f1: 0.216\n",
      "epoch: 27 of 250\n",
      "training_loss: 0.0071, acc: 0.892, ROC: 0.525, f1: 0.115\n",
      "epoch: 28 of 250\n",
      "training_loss: 0.0174, acc: 0.882, ROC: 0.519, f1: 0.106\n",
      "epoch: 29 of 250\n",
      "training_loss: 0.0056, acc: 0.877, ROC: 0.511, f1: 0.089\n",
      "epoch: 30 of 250\n",
      "training_loss: 0.0114, acc: 0.881, ROC: 0.535, f1: 0.144\n",
      "epoch: 31 of 250\n",
      "training_loss: 0.0072, acc: 0.892, ROC: 0.559, f1: 0.182\n",
      "epoch: 32 of 250\n",
      "training_loss: 0.0071, acc: 0.906, ROC: 0.544, f1: 0.161\n",
      "epoch: 33 of 250\n",
      "training_loss: 0.0046, acc: 0.882, ROC: 0.530, f1: 0.132\n",
      "epoch: 34 of 250\n",
      "training_loss: 0.0040, acc: 0.874, ROC: 0.562, f1: 0.203\n",
      "epoch: 35 of 250\n",
      "training_loss: 0.0059, acc: 0.909, ROC: 0.529, f1: 0.117\n",
      "epoch: 36 of 250\n",
      "training_loss: 0.0116, acc: 0.885, ROC: 0.527, f1: 0.122\n",
      "epoch: 37 of 250\n",
      "training_loss: 0.0032, acc: 0.915, ROC: 0.533, f1: 0.124\n",
      "epoch: 38 of 250\n",
      "training_loss: 0.0038, acc: 0.899, ROC: 0.547, f1: 0.165\n",
      "epoch: 39 of 250\n",
      "training_loss: 0.0028, acc: 0.897, ROC: 0.535, f1: 0.134\n",
      "epoch: 40 of 250\n",
      "training_loss: 0.0011, acc: 0.912, ROC: 0.530, f1: 0.120\n",
      "epoch: 41 of 250\n",
      "training_loss: 0.0034, acc: 0.891, ROC: 0.537, f1: 0.142\n",
      "epoch: 42 of 250\n",
      "training_loss: 0.0020, acc: 0.888, ROC: 0.544, f1: 0.164\n",
      "epoch: 43 of 250\n",
      "training_loss: 0.0096, acc: 0.893, ROC: 0.602, f1: 0.262\n",
      "epoch: 44 of 250\n",
      "training_loss: 0.0049, acc: 0.907, ROC: 0.527, f1: 0.114\n",
      "epoch: 45 of 250\n",
      "training_loss: 0.0046, acc: 0.899, ROC: 0.547, f1: 0.165\n",
      "epoch: 46 of 250\n",
      "training_loss: 0.0018, acc: 0.885, ROC: 0.540, f1: 0.148\n",
      "epoch: 47 of 250\n",
      "training_loss: 0.0017, acc: 0.882, ROC: 0.514, f1: 0.092\n",
      "epoch: 48 of 250\n",
      "training_loss: 0.0090, acc: 0.899, ROC: 0.501, f1: 0.056\n",
      "epoch: 49 of 250\n",
      "training_loss: 0.0035, acc: 0.887, ROC: 0.528, f1: 0.124\n",
      "epoch: 50 of 250\n",
      "training_loss: 0.0016, acc: 0.888, ROC: 0.545, f1: 0.164\n",
      "epoch: 51 of 250\n",
      "training_loss: 0.0016, acc: 0.891, ROC: 0.524, f1: 0.114\n",
      "epoch: 52 of 250\n",
      "training_loss: 0.0028, acc: 0.885, ROC: 0.509, f1: 0.080\n",
      "epoch: 53 of 250\n",
      "training_loss: 0.0046, acc: 0.897, ROC: 0.554, f1: 0.176\n",
      "epoch: 54 of 250\n",
      "training_loss: 0.0037, acc: 0.893, ROC: 0.538, f1: 0.144\n",
      "epoch: 55 of 250\n",
      "training_loss: 0.0018, acc: 0.898, ROC: 0.544, f1: 0.164\n",
      "epoch: 56 of 250\n",
      "training_loss: 0.0020, acc: 0.882, ROC: 0.533, f1: 0.132\n",
      "epoch: 57 of 250\n",
      "training_loss: 0.0036, acc: 0.897, ROC: 0.573, f1: 0.226\n",
      "epoch: 58 of 250\n",
      "training_loss: 0.0025, acc: 0.895, ROC: 0.533, f1: 0.132\n",
      "epoch: 59 of 250\n",
      "training_loss: 0.0016, acc: 0.895, ROC: 0.556, f1: 0.173\n",
      "epoch: 60 of 250\n",
      "training_loss: 0.0025, acc: 0.881, ROC: 0.526, f1: 0.119\n",
      "epoch: 61 of 250\n",
      "training_loss: 0.0043, acc: 0.855, ROC: 0.534, f1: 0.152\n",
      "epoch: 62 of 250\n",
      "training_loss: 0.0027, acc: 0.869, ROC: 0.502, f1: 0.071\n",
      "epoch: 63 of 250\n",
      "training_loss: 0.0023, acc: 0.899, ROC: 0.540, f1: 0.151\n",
      "epoch: 64 of 250\n",
      "training_loss: 0.0013, acc: 0.870, ROC: 0.519, f1: 0.110\n",
      "epoch: 65 of 250\n",
      "training_loss: 0.0050, acc: 0.881, ROC: 0.513, f1: 0.092\n",
      "epoch: 66 of 250\n",
      "training_loss: 0.0012, acc: 0.890, ROC: 0.532, f1: 0.127\n",
      "epoch: 67 of 250\n",
      "training_loss: 0.0027, acc: 0.890, ROC: 0.518, f1: 0.098\n",
      "epoch: 68 of 250\n",
      "training_loss: 0.0022, acc: 0.876, ROC: 0.542, f1: 0.151\n",
      "epoch: 69 of 250\n",
      "training_loss: 0.0028, acc: 0.886, ROC: 0.537, f1: 0.149\n",
      "epoch: 70 of 250\n",
      "training_loss: 0.0144, acc: 0.890, ROC: 0.536, f1: 0.141\n",
      "epoch: 71 of 250\n",
      "training_loss: 0.0005, acc: 0.890, ROC: 0.548, f1: 0.167\n",
      "epoch: 72 of 250\n",
      "training_loss: 0.0003, acc: 0.906, ROC: 0.561, f1: 0.190\n",
      "epoch: 73 of 250\n",
      "training_loss: 0.0053, acc: 0.887, ROC: 0.536, f1: 0.137\n",
      "epoch: 74 of 250\n",
      "training_loss: 0.0073, acc: 0.893, ROC: 0.575, f1: 0.230\n",
      "epoch: 75 of 250\n",
      "training_loss: 0.0028, acc: 0.866, ROC: 0.516, f1: 0.107\n",
      "epoch: 76 of 250\n",
      "training_loss: 0.0011, acc: 0.856, ROC: 0.520, f1: 0.122\n",
      "epoch: 77 of 250\n",
      "training_loss: 0.0006, acc: 0.897, ROC: 0.528, f1: 0.120\n",
      "epoch: 78 of 250\n",
      "training_loss: 0.0021, acc: 0.881, ROC: 0.534, f1: 0.144\n",
      "epoch: 79 of 250\n",
      "training_loss: 0.0005, acc: 0.874, ROC: 0.515, f1: 0.100\n",
      "epoch: 80 of 250\n",
      "training_loss: 0.0032, acc: 0.885, ROC: 0.515, f1: 0.094\n",
      "epoch: 81 of 250\n",
      "training_loss: 0.0009, acc: 0.883, ROC: 0.509, f1: 0.079\n",
      "epoch: 82 of 250\n",
      "training_loss: 0.0012, acc: 0.897, ROC: 0.548, f1: 0.176\n",
      "epoch: 83 of 250\n",
      "training_loss: 0.0053, acc: 0.895, ROC: 0.549, f1: 0.173\n",
      "epoch: 84 of 250\n",
      "training_loss: 0.0022, acc: 0.872, ROC: 0.515, f1: 0.099\n",
      "epoch: 85 of 250\n",
      "training_loss: 0.0023, acc: 0.891, ROC: 0.553, f1: 0.180\n",
      "epoch: 86 of 250\n",
      "training_loss: 0.0006, acc: 0.896, ROC: 0.535, f1: 0.133\n",
      "epoch: 87 of 250\n",
      "training_loss: 0.0031, acc: 0.880, ROC: 0.548, f1: 0.167\n",
      "epoch: 88 of 250\n",
      "training_loss: 0.0008, acc: 0.889, ROC: 0.553, f1: 0.190\n",
      "epoch: 89 of 250\n",
      "training_loss: 0.0039, acc: 0.899, ROC: 0.531, f1: 0.122\n",
      "epoch: 90 of 250\n",
      "training_loss: 0.0004, acc: 0.880, ROC: 0.526, f1: 0.118\n",
      "epoch: 91 of 250\n",
      "training_loss: 0.0059, acc: 0.900, ROC: 0.561, f1: 0.194\n",
      "epoch: 92 of 250\n",
      "training_loss: 0.0017, acc: 0.892, ROC: 0.547, f1: 0.169\n",
      "epoch: 93 of 250\n",
      "training_loss: 0.0082, acc: 0.884, ROC: 0.531, f1: 0.134\n",
      "epoch: 94 of 250\n",
      "training_loss: 0.0037, acc: 0.853, ROC: 0.526, f1: 0.120\n",
      "epoch: 95 of 250\n",
      "training_loss: 0.0032, acc: 0.876, ROC: 0.521, f1: 0.114\n",
      "epoch: 96 of 250\n",
      "training_loss: 0.0036, acc: 0.872, ROC: 0.533, f1: 0.135\n",
      "epoch: 97 of 250\n",
      "training_loss: 0.0009, acc: 0.886, ROC: 0.528, f1: 0.123\n",
      "epoch: 98 of 250\n",
      "training_loss: 0.0015, acc: 0.893, ROC: 0.582, f1: 0.241\n",
      "epoch: 99 of 250\n",
      "training_loss: 0.0024, acc: 0.895, ROC: 0.550, f1: 0.173\n",
      "epoch: 100 of 250\n",
      "training_loss: 0.0020, acc: 0.887, ROC: 0.577, f1: 0.221\n",
      "epoch: 101 of 250\n",
      "training_loss: 0.0007, acc: 0.906, ROC: 0.513, f1: 0.078\n",
      "epoch: 102 of 250\n",
      "training_loss: 0.0015, acc: 0.907, ROC: 0.559, f1: 0.191\n",
      "epoch: 103 of 250\n",
      "training_loss: 0.0019, acc: 0.904, ROC: 0.512, f1: 0.077\n",
      "epoch: 104 of 250\n",
      "training_loss: 0.0065, acc: 0.904, ROC: 0.526, f1: 0.111\n",
      "epoch: 105 of 250\n",
      "training_loss: 0.0049, acc: 0.900, ROC: 0.566, f1: 0.180\n",
      "epoch: 106 of 250\n",
      "training_loss: 0.0006, acc: 0.906, ROC: 0.559, f1: 0.175\n",
      "epoch: 107 of 250\n",
      "training_loss: 0.0045, acc: 0.917, ROC: 0.550, f1: 0.162\n",
      "epoch: 108 of 250\n",
      "training_loss: 0.0020, acc: 0.884, ROC: 0.531, f1: 0.134\n",
      "epoch: 109 of 250\n",
      "training_loss: 0.0011, acc: 0.891, ROC: 0.547, f1: 0.168\n",
      "epoch: 110 of 250\n",
      "training_loss: 0.0039, acc: 0.880, ROC: 0.507, f1: 0.077\n",
      "epoch: 111 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.0033, acc: 0.903, ROC: 0.525, f1: 0.110\n",
      "epoch: 112 of 250\n",
      "training_loss: 0.0026, acc: 0.877, ROC: 0.501, f1: 0.061\n",
      "epoch: 113 of 250\n",
      "training_loss: 0.0005, acc: 0.894, ROC: 0.539, f1: 0.145\n",
      "epoch: 114 of 250\n",
      "training_loss: 0.0001, acc: 0.895, ROC: 0.538, f1: 0.146\n",
      "epoch: 115 of 250\n",
      "training_loss: 0.0049, acc: 0.872, ROC: 0.524, f1: 0.123\n",
      "epoch: 116 of 250\n",
      "training_loss: 0.0034, acc: 0.902, ROC: 0.525, f1: 0.109\n",
      "epoch: 117 of 250\n",
      "training_loss: 0.0018, acc: 0.891, ROC: 0.564, f1: 0.216\n",
      "epoch: 118 of 250\n",
      "training_loss: 0.0003, acc: 0.888, ROC: 0.554, f1: 0.188\n",
      "epoch: 119 of 250\n",
      "training_loss: 0.0022, acc: 0.902, ROC: 0.549, f1: 0.169\n",
      "epoch: 120 of 250\n",
      "training_loss: 0.0029, acc: 0.904, ROC: 0.564, f1: 0.186\n",
      "epoch: 121 of 250\n",
      "training_loss: 0.0005, acc: 0.875, ROC: 0.531, f1: 0.138\n",
      "epoch: 122 of 250\n",
      "training_loss: 0.0067, acc: 0.891, ROC: 0.542, f1: 0.155\n",
      "epoch: 123 of 250\n",
      "training_loss: 0.0024, acc: 0.913, ROC: 0.565, f1: 0.202\n",
      "epoch: 124 of 250\n",
      "training_loss: 0.0001, acc: 0.890, ROC: 0.512, f1: 0.083\n",
      "epoch: 125 of 250\n",
      "training_loss: 0.0009, acc: 0.913, ROC: 0.545, f1: 0.155\n",
      "epoch: 126 of 250\n",
      "training_loss: 0.0006, acc: 0.884, ROC: 0.521, f1: 0.108\n",
      "epoch: 127 of 250\n",
      "training_loss: 0.0008, acc: 0.906, ROC: 0.539, f1: 0.145\n",
      "epoch: 128 of 250\n",
      "training_loss: 0.0005, acc: 0.893, ROC: 0.563, f1: 0.207\n",
      "epoch: 129 of 250\n",
      "training_loss: 0.0018, acc: 0.882, ROC: 0.525, f1: 0.119\n",
      "epoch: 130 of 250\n",
      "training_loss: 0.0047, acc: 0.895, ROC: 0.552, f1: 0.173\n",
      "epoch: 131 of 250\n",
      "training_loss: 0.0008, acc: 0.890, ROC: 0.569, f1: 0.191\n",
      "epoch: 132 of 250\n",
      "training_loss: 0.0019, acc: 0.885, ROC: 0.516, f1: 0.094\n",
      "epoch: 133 of 250\n",
      "training_loss: 0.0072, acc: 0.901, ROC: 0.525, f1: 0.108\n",
      "epoch: 134 of 250\n",
      "training_loss: 0.0001, acc: 0.910, ROC: 0.563, f1: 0.211\n",
      "epoch: 135 of 250\n",
      "training_loss: 0.0008, acc: 0.904, ROC: 0.513, f1: 0.077\n",
      "epoch: 136 of 250\n",
      "training_loss: 0.0037, acc: 0.889, ROC: 0.540, f1: 0.153\n",
      "epoch: 137 of 250\n",
      "training_loss: 0.0008, acc: 0.903, ROC: 0.560, f1: 0.198\n",
      "epoch: 138 of 250\n",
      "training_loss: 0.0015, acc: 0.893, ROC: 0.532, f1: 0.130\n",
      "epoch: 139 of 250\n",
      "training_loss: 0.0026, acc: 0.902, ROC: 0.553, f1: 0.169\n",
      "epoch: 140 of 250\n",
      "training_loss: 0.0017, acc: 0.890, ROC: 0.577, f1: 0.225\n",
      "epoch: 141 of 250\n",
      "training_loss: 0.0025, acc: 0.893, ROC: 0.553, f1: 0.183\n",
      "epoch: 142 of 250\n",
      "training_loss: 0.0043, acc: 0.921, ROC: 0.578, f1: 0.218\n",
      "epoch: 143 of 250\n",
      "training_loss: 0.0013, acc: 0.887, ROC: 0.546, f1: 0.163\n",
      "epoch: 144 of 250\n",
      "training_loss: 0.0628, acc: 0.898, ROC: 0.575, f1: 0.215\n",
      "epoch: 145 of 250\n",
      "training_loss: 0.0003, acc: 0.909, ROC: 0.507, f1: 0.062\n",
      "epoch: 146 of 250\n",
      "training_loss: 0.0025, acc: 0.881, ROC: 0.531, f1: 0.131\n",
      "epoch: 147 of 250\n",
      "training_loss: 0.0025, acc: 0.892, ROC: 0.526, f1: 0.115\n",
      "epoch: 148 of 250\n",
      "training_loss: 0.0032, acc: 0.871, ROC: 0.532, f1: 0.134\n",
      "epoch: 149 of 250\n",
      "training_loss: 0.0021, acc: 0.896, ROC: 0.533, f1: 0.133\n",
      "epoch: 150 of 250\n",
      "training_loss: 0.0027, acc: 0.895, ROC: 0.546, f1: 0.160\n",
      "epoch: 151 of 250\n",
      "training_loss: 0.0019, acc: 0.884, ROC: 0.543, f1: 0.159\n",
      "epoch: 152 of 250\n",
      "training_loss: 0.0031, acc: 0.885, ROC: 0.515, f1: 0.094\n",
      "epoch: 153 of 250\n",
      "training_loss: 0.0014, acc: 0.906, ROC: 0.547, f1: 0.161\n",
      "epoch: 154 of 250\n",
      "training_loss: 0.0019, acc: 0.871, ROC: 0.553, f1: 0.168\n",
      "epoch: 155 of 250\n",
      "training_loss: 0.0011, acc: 0.883, ROC: 0.508, f1: 0.079\n",
      "epoch: 156 of 250\n",
      "training_loss: 0.0006, acc: 0.897, ROC: 0.537, f1: 0.134\n",
      "epoch: 157 of 250\n",
      "training_loss: 0.0011, acc: 0.903, ROC: 0.519, f1: 0.093\n",
      "epoch: 158 of 250\n",
      "training_loss: 0.0002, acc: 0.890, ROC: 0.529, f1: 0.127\n",
      "epoch: 159 of 250\n",
      "training_loss: 0.0029, acc: 0.874, ROC: 0.541, f1: 0.160\n",
      "epoch: 160 of 250\n",
      "training_loss: 0.0007, acc: 0.900, ROC: 0.554, f1: 0.180\n",
      "epoch: 161 of 250\n",
      "training_loss: 0.0002, acc: 0.876, ROC: 0.521, f1: 0.114\n",
      "epoch: 162 of 250\n",
      "training_loss: 0.0035, acc: 0.910, ROC: 0.544, f1: 0.151\n",
      "epoch: 163 of 250\n",
      "training_loss: 0.0003, acc: 0.883, ROC: 0.514, f1: 0.093\n",
      "epoch: 164 of 250\n",
      "training_loss: 0.0012, acc: 0.876, ROC: 0.495, f1: 0.046\n",
      "epoch: 165 of 250\n",
      "training_loss: 0.0018, acc: 0.883, ROC: 0.502, f1: 0.064\n",
      "epoch: 166 of 250\n",
      "training_loss: 0.0011, acc: 0.886, ROC: 0.512, f1: 0.081\n",
      "epoch: 167 of 250\n",
      "training_loss: 0.0014, acc: 0.886, ROC: 0.532, f1: 0.136\n",
      "epoch: 168 of 250\n",
      "training_loss: 0.0045, acc: 0.905, ROC: 0.539, f1: 0.144\n",
      "epoch: 169 of 250\n",
      "training_loss: 0.0002, acc: 0.896, ROC: 0.548, f1: 0.161\n",
      "epoch: 170 of 250\n",
      "training_loss: 0.0005, acc: 0.885, ROC: 0.527, f1: 0.122\n",
      "epoch: 171 of 250\n",
      "training_loss: 0.0004, acc: 0.889, ROC: 0.529, f1: 0.126\n",
      "epoch: 172 of 250\n",
      "training_loss: 0.0004, acc: 0.906, ROC: 0.553, f1: 0.175\n",
      "epoch: 173 of 250\n",
      "training_loss: 0.0012, acc: 0.892, ROC: 0.525, f1: 0.115\n",
      "epoch: 174 of 250\n",
      "training_loss: 0.0015, acc: 0.886, ROC: 0.522, f1: 0.109\n",
      "epoch: 175 of 250\n",
      "training_loss: 0.0006, acc: 0.881, ROC: 0.548, f1: 0.168\n",
      "epoch: 176 of 250\n",
      "training_loss: 0.0002, acc: 0.904, ROC: 0.559, f1: 0.186\n",
      "epoch: 177 of 250\n",
      "training_loss: 0.0008, acc: 0.877, ROC: 0.527, f1: 0.128\n",
      "epoch: 178 of 250\n",
      "training_loss: 0.0007, acc: 0.901, ROC: 0.539, f1: 0.139\n",
      "epoch: 179 of 250\n",
      "training_loss: 0.0015, acc: 0.884, ROC: 0.509, f1: 0.079\n",
      "epoch: 180 of 250\n",
      "training_loss: 0.0053, acc: 0.889, ROC: 0.546, f1: 0.165\n",
      "epoch: 181 of 250\n",
      "training_loss: 0.0003, acc: 0.887, ROC: 0.527, f1: 0.124\n",
      "epoch: 182 of 250\n",
      "training_loss: 0.0038, acc: 0.899, ROC: 0.517, f1: 0.090\n",
      "epoch: 183 of 250\n",
      "training_loss: 0.0019, acc: 0.879, ROC: 0.528, f1: 0.129\n",
      "epoch: 184 of 250\n",
      "training_loss: 0.0274, acc: 0.890, ROC: 0.557, f1: 0.191\n",
      "epoch: 185 of 250\n",
      "training_loss: 0.0006, acc: 0.897, ROC: 0.515, f1: 0.088\n",
      "epoch: 186 of 250\n",
      "training_loss: 0.0003, acc: 0.885, ROC: 0.521, f1: 0.109\n",
      "epoch: 187 of 250\n",
      "training_loss: 0.0154, acc: 0.877, ROC: 0.506, f1: 0.075\n",
      "epoch: 188 of 250\n",
      "training_loss: 0.0005, acc: 0.871, ROC: 0.513, f1: 0.098\n",
      "epoch: 189 of 250\n",
      "training_loss: 0.0004, acc: 0.891, ROC: 0.512, f1: 0.084\n",
      "epoch: 190 of 250\n",
      "training_loss: 0.0011, acc: 0.901, ROC: 0.525, f1: 0.108\n",
      "epoch: 191 of 250\n",
      "training_loss: 0.0010, acc: 0.894, ROC: 0.554, f1: 0.185\n",
      "epoch: 192 of 250\n",
      "training_loss: 0.0033, acc: 0.888, ROC: 0.523, f1: 0.111\n",
      "epoch: 193 of 250\n",
      "training_loss: 0.0001, acc: 0.869, ROC: 0.501, f1: 0.071\n",
      "epoch: 194 of 250\n",
      "training_loss: 0.0068, acc: 0.893, ROC: 0.542, f1: 0.157\n",
      "epoch: 195 of 250\n",
      "training_loss: 0.0000, acc: 0.900, ROC: 0.547, f1: 0.167\n",
      "epoch: 196 of 250\n",
      "training_loss: 0.0050, acc: 0.891, ROC: 0.543, f1: 0.155\n",
      "epoch: 197 of 250\n",
      "training_loss: 0.0003, acc: 0.890, ROC: 0.529, f1: 0.127\n",
      "epoch: 198 of 250\n",
      "training_loss: 0.0046, acc: 0.900, ROC: 0.532, f1: 0.123\n",
      "epoch: 199 of 250\n",
      "training_loss: 0.0012, acc: 0.893, ROC: 0.525, f1: 0.116\n",
      "epoch: 200 of 250\n",
      "training_loss: 0.0018, acc: 0.877, ROC: 0.532, f1: 0.140\n",
      "epoch: 201 of 250\n",
      "training_loss: 0.0005, acc: 0.899, ROC: 0.542, f1: 0.151\n",
      "epoch: 202 of 250\n",
      "training_loss: 0.0001, acc: 0.883, ROC: 0.496, f1: 0.049\n",
      "epoch: 203 of 250\n",
      "training_loss: 0.0037, acc: 0.901, ROC: 0.525, f1: 0.108\n",
      "epoch: 204 of 250\n",
      "training_loss: 0.0039, acc: 0.905, ROC: 0.556, f1: 0.188\n",
      "epoch: 205 of 250\n",
      "training_loss: 0.0002, acc: 0.897, ROC: 0.566, f1: 0.202\n",
      "epoch: 206 of 250\n",
      "training_loss: 0.0003, acc: 0.895, ROC: 0.532, f1: 0.132\n",
      "epoch: 207 of 250\n",
      "training_loss: 0.0033, acc: 0.904, ROC: 0.537, f1: 0.143\n",
      "epoch: 208 of 250\n",
      "training_loss: 0.0066, acc: 0.860, ROC: 0.574, f1: 0.222\n",
      "epoch: 209 of 250\n",
      "training_loss: 0.0012, acc: 0.903, ROC: 0.519, f1: 0.093\n",
      "epoch: 210 of 250\n",
      "training_loss: 0.0001, acc: 0.889, ROC: 0.568, f1: 0.213\n",
      "epoch: 211 of 250\n",
      "training_loss: 0.0020, acc: 0.901, ROC: 0.518, f1: 0.092\n",
      "epoch: 212 of 250\n",
      "training_loss: 0.0008, acc: 0.873, ROC: 0.492, f1: 0.045\n",
      "epoch: 213 of 250\n",
      "training_loss: 0.0050, acc: 0.903, ROC: 0.519, f1: 0.093\n",
      "epoch: 214 of 250\n",
      "training_loss: 0.0074, acc: 0.894, ROC: 0.563, f1: 0.197\n",
      "epoch: 215 of 250\n",
      "training_loss: 0.0001, acc: 0.894, ROC: 0.559, f1: 0.197\n",
      "epoch: 216 of 250\n",
      "training_loss: 0.0032, acc: 0.879, ROC: 0.538, f1: 0.154\n",
      "epoch: 217 of 250\n",
      "training_loss: 0.0008, acc: 0.872, ROC: 0.530, f1: 0.135\n",
      "epoch: 218 of 250\n",
      "training_loss: 0.0003, acc: 0.904, ROC: 0.537, f1: 0.143\n",
      "epoch: 219 of 250\n",
      "training_loss: 0.0001, acc: 0.900, ROC: 0.524, f1: 0.107\n",
      "epoch: 220 of 250\n",
      "training_loss: 0.0012, acc: 0.886, ROC: 0.509, f1: 0.081\n",
      "epoch: 221 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.0007, acc: 0.872, ROC: 0.502, f1: 0.072\n",
      "epoch: 222 of 250\n",
      "training_loss: 0.0002, acc: 0.896, ROC: 0.546, f1: 0.161\n",
      "epoch: 223 of 250\n",
      "training_loss: 0.0017, acc: 0.884, ROC: 0.558, f1: 0.183\n",
      "epoch: 224 of 250\n",
      "training_loss: 0.0002, acc: 0.892, ROC: 0.525, f1: 0.115\n",
      "epoch: 225 of 250\n",
      "training_loss: 0.0001, acc: 0.873, ROC: 0.520, f1: 0.112\n",
      "epoch: 226 of 250\n",
      "training_loss: 0.0001, acc: 0.881, ROC: 0.507, f1: 0.078\n",
      "epoch: 227 of 250\n",
      "training_loss: 0.0041, acc: 0.887, ROC: 0.510, f1: 0.081\n",
      "epoch: 228 of 250\n",
      "training_loss: 0.0002, acc: 0.886, ROC: 0.527, f1: 0.123\n",
      "epoch: 229 of 250\n",
      "training_loss: 0.0002, acc: 0.899, ROC: 0.535, f1: 0.137\n",
      "epoch: 230 of 250\n",
      "training_loss: 0.0003, acc: 0.868, ROC: 0.522, f1: 0.120\n",
      "epoch: 231 of 250\n",
      "training_loss: 0.0003, acc: 0.904, ROC: 0.529, f1: 0.111\n",
      "epoch: 232 of 250\n",
      "training_loss: 0.0075, acc: 0.893, ROC: 0.543, f1: 0.157\n",
      "epoch: 233 of 250\n",
      "training_loss: 0.0007, acc: 0.898, ROC: 0.522, f1: 0.105\n",
      "epoch: 234 of 250\n",
      "training_loss: 0.0021, acc: 0.898, ROC: 0.566, f1: 0.203\n",
      "epoch: 235 of 250\n",
      "training_loss: 0.0062, acc: 0.889, ROC: 0.519, f1: 0.098\n",
      "epoch: 236 of 250\n",
      "training_loss: 0.0006, acc: 0.888, ROC: 0.511, f1: 0.082\n",
      "epoch: 237 of 250\n",
      "training_loss: 0.0001, acc: 0.900, ROC: 0.550, f1: 0.167\n",
      "epoch: 238 of 250\n",
      "training_loss: 0.0039, acc: 0.898, ROC: 0.542, f1: 0.150\n",
      "epoch: 239 of 250\n",
      "training_loss: 0.0673, acc: 0.892, ROC: 0.513, f1: 0.085\n",
      "epoch: 240 of 250\n",
      "training_loss: 0.0015, acc: 0.864, ROC: 0.520, f1: 0.117\n",
      "epoch: 241 of 250\n",
      "training_loss: 0.0001, acc: 0.908, ROC: 0.605, f1: 0.258\n",
      "epoch: 242 of 250\n",
      "training_loss: 0.0032, acc: 0.904, ROC: 0.541, f1: 0.143\n",
      "epoch: 243 of 250\n",
      "training_loss: 0.0001, acc: 0.880, ROC: 0.551, f1: 0.178\n",
      "epoch: 244 of 250\n",
      "training_loss: 0.0000, acc: 0.889, ROC: 0.555, f1: 0.190\n",
      "epoch: 245 of 250\n",
      "training_loss: 0.0024, acc: 0.896, ROC: 0.539, f1: 0.148\n",
      "epoch: 246 of 250\n",
      "training_loss: 0.0010, acc: 0.896, ROC: 0.534, f1: 0.133\n",
      "epoch: 247 of 250\n",
      "training_loss: 0.0009, acc: 0.885, ROC: 0.539, f1: 0.148\n",
      "epoch: 248 of 250\n",
      "training_loss: 0.0041, acc: 0.887, ROC: 0.534, f1: 0.137\n",
      "epoch: 249 of 250\n",
      "training_loss: 0.0003, acc: 0.887, ROC: 0.522, f1: 0.110\n",
      "(26173, 39, 11, 11)\n",
      "epoch: 0 of 250\n",
      "training_loss: 0.5923, acc: 0.737, ROC: 0.576, f1: 0.181\n",
      "epoch: 1 of 250\n",
      "training_loss: 0.5393, acc: 0.736, ROC: 0.567, f1: 0.180\n",
      "epoch: 2 of 250\n",
      "training_loss: 0.5011, acc: 0.811, ROC: 0.585, f1: 0.209\n",
      "epoch: 3 of 250\n",
      "training_loss: 0.4797, acc: 0.846, ROC: 0.545, f1: 0.154\n",
      "epoch: 4 of 250\n",
      "training_loss: 0.4261, acc: 0.655, ROC: 0.553, f1: 0.177\n",
      "epoch: 5 of 250\n",
      "training_loss: 0.3862, acc: 0.887, ROC: 0.486, f1: 0.017\n",
      "epoch: 6 of 250\n",
      "training_loss: 0.3741, acc: 0.840, ROC: 0.515, f1: 0.101\n",
      "epoch: 7 of 250\n",
      "training_loss: 0.3504, acc: 0.778, ROC: 0.531, f1: 0.140\n",
      "epoch: 8 of 250\n",
      "training_loss: 0.3174, acc: 0.828, ROC: 0.485, f1: 0.065\n",
      "epoch: 9 of 250\n",
      "training_loss: 0.2888, acc: 0.901, ROC: 0.512, f1: 0.075\n",
      "epoch: 10 of 250\n",
      "training_loss: 0.2744, acc: 0.841, ROC: 0.541, f1: 0.150\n",
      "epoch: 11 of 250\n",
      "training_loss: 0.2525, acc: 0.860, ROC: 0.571, f1: 0.205\n",
      "epoch: 12 of 250\n",
      "training_loss: 0.2155, acc: 0.738, ROC: 0.575, f1: 0.171\n",
      "epoch: 13 of 250\n",
      "training_loss: 0.2037, acc: 0.883, ROC: 0.541, f1: 0.146\n",
      "epoch: 14 of 250\n",
      "training_loss: 0.1996, acc: 0.777, ROC: 0.557, f1: 0.183\n",
      "epoch: 15 of 250\n",
      "training_loss: 0.1914, acc: 0.822, ROC: 0.579, f1: 0.191\n",
      "epoch: 16 of 250\n",
      "training_loss: 0.1792, acc: 0.792, ROC: 0.516, f1: 0.119\n",
      "epoch: 17 of 250\n",
      "training_loss: 0.1659, acc: 0.835, ROC: 0.510, f1: 0.108\n",
      "epoch: 18 of 250\n",
      "training_loss: 0.1528, acc: 0.910, ROC: 0.508, f1: 0.062\n",
      "epoch: 19 of 250\n",
      "training_loss: 0.1553, acc: 0.834, ROC: 0.550, f1: 0.162\n",
      "epoch: 20 of 250\n",
      "training_loss: 0.1241, acc: 0.909, ROC: 0.551, f1: 0.165\n",
      "epoch: 21 of 250\n",
      "training_loss: 0.1298, acc: 0.855, ROC: 0.505, f1: 0.088\n",
      "epoch: 22 of 250\n",
      "training_loss: 0.1282, acc: 0.879, ROC: 0.534, f1: 0.142\n",
      "epoch: 23 of 250\n",
      "training_loss: 0.1079, acc: 0.888, ROC: 0.553, f1: 0.176\n",
      "epoch: 24 of 250\n",
      "training_loss: 0.0929, acc: 0.872, ROC: 0.533, f1: 0.135\n",
      "epoch: 25 of 250\n",
      "training_loss: 0.0902, acc: 0.883, ROC: 0.561, f1: 0.204\n",
      "epoch: 26 of 250\n",
      "training_loss: 0.0989, acc: 0.839, ROC: 0.599, f1: 0.222\n",
      "epoch: 27 of 250\n",
      "training_loss: 0.0852, acc: 0.887, ROC: 0.539, f1: 0.150\n",
      "epoch: 28 of 250\n",
      "training_loss: 0.0809, acc: 0.863, ROC: 0.527, f1: 0.127\n",
      "epoch: 29 of 250\n",
      "training_loss: 0.0784, acc: 0.881, ROC: 0.530, f1: 0.131\n",
      "epoch: 30 of 250\n",
      "training_loss: 0.0790, acc: 0.868, ROC: 0.540, f1: 0.154\n",
      "epoch: 31 of 250\n",
      "training_loss: 0.0673, acc: 0.881, ROC: 0.589, f1: 0.212\n",
      "epoch: 32 of 250\n",
      "training_loss: 0.0583, acc: 0.739, ROC: 0.534, f1: 0.139\n",
      "epoch: 33 of 250\n",
      "training_loss: 0.0595, acc: 0.908, ROC: 0.506, f1: 0.061\n",
      "epoch: 34 of 250\n",
      "training_loss: 0.0432, acc: 0.871, ROC: 0.596, f1: 0.246\n",
      "epoch: 35 of 250\n",
      "training_loss: 0.0579, acc: 0.877, ROC: 0.540, f1: 0.152\n",
      "epoch: 36 of 250\n",
      "training_loss: 0.0541, acc: 0.883, ROC: 0.540, f1: 0.158\n",
      "epoch: 37 of 250\n",
      "training_loss: 0.0388, acc: 0.855, ROC: 0.535, f1: 0.152\n",
      "epoch: 38 of 250\n",
      "training_loss: 0.0437, acc: 0.857, ROC: 0.511, f1: 0.101\n",
      "epoch: 39 of 250\n",
      "training_loss: 0.0355, acc: 0.866, ROC: 0.492, f1: 0.056\n",
      "epoch: 40 of 250\n",
      "training_loss: 0.0443, acc: 0.885, ROC: 0.559, f1: 0.184\n",
      "epoch: 41 of 250\n",
      "training_loss: 0.0373, acc: 0.878, ROC: 0.528, f1: 0.129\n",
      "epoch: 42 of 250\n",
      "training_loss: 0.0408, acc: 0.854, ROC: 0.530, f1: 0.141\n",
      "epoch: 43 of 250\n",
      "training_loss: 0.0282, acc: 0.857, ROC: 0.511, f1: 0.101\n",
      "epoch: 44 of 250\n",
      "training_loss: 0.0356, acc: 0.853, ROC: 0.517, f1: 0.109\n",
      "epoch: 45 of 250\n",
      "training_loss: 0.0348, acc: 0.881, ROC: 0.528, f1: 0.119\n",
      "epoch: 46 of 250\n",
      "training_loss: 0.0231, acc: 0.889, ROC: 0.559, f1: 0.178\n",
      "epoch: 47 of 250\n",
      "training_loss: 0.0286, acc: 0.887, ROC: 0.528, f1: 0.124\n",
      "epoch: 48 of 250\n",
      "training_loss: 0.0305, acc: 0.878, ROC: 0.532, f1: 0.129\n",
      "epoch: 49 of 250\n",
      "training_loss: 0.0287, acc: 0.896, ROC: 0.563, f1: 0.200\n",
      "epoch: 50 of 250\n",
      "training_loss: 0.0257, acc: 0.873, ROC: 0.551, f1: 0.159\n",
      "epoch: 51 of 250\n",
      "training_loss: 0.0229, acc: 0.856, ROC: 0.517, f1: 0.111\n",
      "epoch: 52 of 250\n",
      "training_loss: 0.0268, acc: 0.857, ROC: 0.495, f1: 0.065\n",
      "epoch: 53 of 250\n",
      "training_loss: 0.0292, acc: 0.849, ROC: 0.541, f1: 0.156\n",
      "epoch: 54 of 250\n",
      "training_loss: 0.0316, acc: 0.875, ROC: 0.527, f1: 0.113\n",
      "epoch: 55 of 250\n",
      "training_loss: 0.0274, acc: 0.865, ROC: 0.549, f1: 0.161\n",
      "epoch: 56 of 250\n",
      "training_loss: 0.0254, acc: 0.863, ROC: 0.556, f1: 0.170\n",
      "epoch: 57 of 250\n",
      "training_loss: 0.0258, acc: 0.865, ROC: 0.511, f1: 0.094\n",
      "epoch: 58 of 250\n",
      "training_loss: 0.0095, acc: 0.853, ROC: 0.516, f1: 0.109\n",
      "epoch: 59 of 250\n",
      "training_loss: 0.0275, acc: 0.856, ROC: 0.520, f1: 0.122\n",
      "epoch: 60 of 250\n",
      "training_loss: 0.0230, acc: 0.883, ROC: 0.538, f1: 0.146\n",
      "epoch: 61 of 250\n",
      "training_loss: 0.0159, acc: 0.853, ROC: 0.535, f1: 0.140\n",
      "epoch: 62 of 250\n",
      "training_loss: 0.0145, acc: 0.872, ROC: 0.549, f1: 0.169\n",
      "epoch: 63 of 250\n",
      "training_loss: 0.0123, acc: 0.870, ROC: 0.538, f1: 0.156\n",
      "epoch: 64 of 250\n",
      "training_loss: 0.0155, acc: 0.883, ROC: 0.516, f1: 0.093\n",
      "epoch: 65 of 250\n",
      "training_loss: 0.0163, acc: 0.868, ROC: 0.512, f1: 0.096\n",
      "epoch: 66 of 250\n",
      "training_loss: 0.0126, acc: 0.881, ROC: 0.579, f1: 0.232\n",
      "epoch: 67 of 250\n",
      "training_loss: 0.0131, acc: 0.886, ROC: 0.530, f1: 0.123\n",
      "epoch: 68 of 250\n",
      "training_loss: 0.0192, acc: 0.882, ROC: 0.520, f1: 0.106\n",
      "epoch: 69 of 250\n",
      "training_loss: 0.0119, acc: 0.878, ROC: 0.523, f1: 0.116\n",
      "epoch: 70 of 250\n",
      "training_loss: 0.0138, acc: 0.843, ROC: 0.533, f1: 0.151\n",
      "epoch: 71 of 250\n",
      "training_loss: 0.0103, acc: 0.866, ROC: 0.511, f1: 0.095\n",
      "epoch: 72 of 250\n",
      "training_loss: 0.0136, acc: 0.869, ROC: 0.564, f1: 0.176\n",
      "epoch: 73 of 250\n",
      "training_loss: 0.0100, acc: 0.889, ROC: 0.579, f1: 0.224\n",
      "epoch: 74 of 250\n",
      "training_loss: 0.0114, acc: 0.904, ROC: 0.532, f1: 0.127\n",
      "epoch: 75 of 250\n",
      "training_loss: 0.0201, acc: 0.891, ROC: 0.536, f1: 0.142\n",
      "epoch: 76 of 250\n",
      "training_loss: 0.0226, acc: 0.884, ROC: 0.540, f1: 0.147\n",
      "epoch: 77 of 250\n",
      "training_loss: 0.0173, acc: 0.869, ROC: 0.530, f1: 0.132\n",
      "epoch: 78 of 250\n",
      "training_loss: 0.0129, acc: 0.863, ROC: 0.529, f1: 0.138\n",
      "epoch: 79 of 250\n",
      "training_loss: 0.0076, acc: 0.859, ROC: 0.541, f1: 0.156\n",
      "epoch: 80 of 250\n",
      "training_loss: 0.0174, acc: 0.894, ROC: 0.535, f1: 0.131\n",
      "epoch: 81 of 250\n",
      "training_loss: 0.0102, acc: 0.875, ROC: 0.516, f1: 0.101\n",
      "epoch: 82 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.0073, acc: 0.881, ROC: 0.529, f1: 0.131\n",
      "epoch: 83 of 250\n",
      "training_loss: 0.0109, acc: 0.857, ROC: 0.526, f1: 0.123\n",
      "epoch: 84 of 250\n",
      "training_loss: 0.0184, acc: 0.892, ROC: 0.552, f1: 0.169\n",
      "epoch: 85 of 250\n",
      "training_loss: 0.0106, acc: 0.884, ROC: 0.526, f1: 0.121\n",
      "epoch: 86 of 250\n",
      "training_loss: 0.0106, acc: 0.841, ROC: 0.531, f1: 0.141\n",
      "epoch: 87 of 250\n",
      "training_loss: 0.0101, acc: 0.863, ROC: 0.521, f1: 0.116\n",
      "epoch: 88 of 250\n",
      "training_loss: 0.0167, acc: 0.863, ROC: 0.540, f1: 0.160\n",
      "epoch: 89 of 250\n",
      "training_loss: 0.0052, acc: 0.851, ROC: 0.555, f1: 0.186\n",
      "epoch: 90 of 250\n",
      "training_loss: 0.0195, acc: 0.881, ROC: 0.568, f1: 0.190\n",
      "epoch: 91 of 250\n",
      "training_loss: 0.0048, acc: 0.855, ROC: 0.546, f1: 0.162\n",
      "epoch: 92 of 250\n",
      "training_loss: 0.0172, acc: 0.871, ROC: 0.514, f1: 0.098\n",
      "epoch: 93 of 250\n",
      "training_loss: 0.0061, acc: 0.868, ROC: 0.543, f1: 0.154\n",
      "epoch: 94 of 250\n",
      "training_loss: 0.0134, acc: 0.841, ROC: 0.514, f1: 0.112\n",
      "epoch: 95 of 250\n",
      "training_loss: 0.0051, acc: 0.874, ROC: 0.526, f1: 0.125\n",
      "epoch: 96 of 250\n",
      "training_loss: 0.0195, acc: 0.856, ROC: 0.540, f1: 0.153\n",
      "epoch: 97 of 250\n",
      "training_loss: 0.0064, acc: 0.854, ROC: 0.528, f1: 0.141\n",
      "epoch: 98 of 250\n",
      "training_loss: 0.0112, acc: 0.870, ROC: 0.546, f1: 0.167\n",
      "epoch: 99 of 250\n",
      "training_loss: 0.0116, acc: 0.891, ROC: 0.559, f1: 0.193\n",
      "epoch: 100 of 250\n",
      "training_loss: 0.0159, acc: 0.883, ROC: 0.546, f1: 0.158\n",
      "epoch: 101 of 250\n",
      "training_loss: 0.0138, acc: 0.877, ROC: 0.546, f1: 0.152\n",
      "epoch: 102 of 250\n",
      "training_loss: 0.0215, acc: 0.863, ROC: 0.477, f1: 0.028\n",
      "epoch: 103 of 250\n",
      "training_loss: 0.0072, acc: 0.855, ROC: 0.521, f1: 0.121\n",
      "epoch: 104 of 250\n",
      "training_loss: 0.0089, acc: 0.892, ROC: 0.565, f1: 0.194\n",
      "epoch: 105 of 250\n",
      "training_loss: 0.0138, acc: 0.865, ROC: 0.540, f1: 0.151\n",
      "epoch: 106 of 250\n",
      "training_loss: 0.0069, acc: 0.892, ROC: 0.538, f1: 0.143\n",
      "epoch: 107 of 250\n",
      "training_loss: 0.0151, acc: 0.866, ROC: 0.540, f1: 0.141\n",
      "epoch: 108 of 250\n",
      "training_loss: 0.0101, acc: 0.875, ROC: 0.530, f1: 0.126\n",
      "epoch: 109 of 250\n",
      "training_loss: 0.0087, acc: 0.871, ROC: 0.533, f1: 0.134\n",
      "epoch: 110 of 250\n",
      "training_loss: 0.0071, acc: 0.864, ROC: 0.546, f1: 0.150\n",
      "epoch: 111 of 250\n",
      "training_loss: 0.0108, acc: 0.869, ROC: 0.546, f1: 0.166\n",
      "epoch: 112 of 250\n",
      "training_loss: 0.0114, acc: 0.866, ROC: 0.511, f1: 0.095\n",
      "epoch: 113 of 250\n",
      "training_loss: 0.0128, acc: 0.859, ROC: 0.517, f1: 0.113\n",
      "epoch: 114 of 250\n",
      "training_loss: 0.0085, acc: 0.860, ROC: 0.560, f1: 0.186\n",
      "epoch: 115 of 250\n",
      "training_loss: 0.0134, acc: 0.860, ROC: 0.522, f1: 0.125\n",
      "epoch: 116 of 250\n",
      "training_loss: 0.0116, acc: 0.859, ROC: 0.539, f1: 0.135\n",
      "epoch: 117 of 250\n",
      "training_loss: 0.0166, acc: 0.861, ROC: 0.535, f1: 0.147\n",
      "epoch: 118 of 250\n",
      "training_loss: 0.0179, acc: 0.883, ROC: 0.521, f1: 0.107\n",
      "epoch: 119 of 250\n",
      "training_loss: 0.0087, acc: 0.902, ROC: 0.576, f1: 0.222\n",
      "epoch: 120 of 250\n",
      "training_loss: 0.0083, acc: 0.873, ROC: 0.534, f1: 0.136\n",
      "epoch: 121 of 250\n",
      "training_loss: 0.0121, acc: 0.873, ROC: 0.533, f1: 0.136\n",
      "epoch: 122 of 250\n",
      "training_loss: 0.0052, acc: 0.874, ROC: 0.592, f1: 0.241\n",
      "epoch: 123 of 250\n",
      "training_loss: 0.0119, acc: 0.885, ROC: 0.537, f1: 0.135\n",
      "epoch: 124 of 250\n",
      "training_loss: 0.0116, acc: 0.864, ROC: 0.551, f1: 0.171\n",
      "epoch: 125 of 250\n",
      "training_loss: 0.0061, acc: 0.878, ROC: 0.540, f1: 0.153\n",
      "epoch: 126 of 250\n",
      "training_loss: 0.0229, acc: 0.876, ROC: 0.541, f1: 0.151\n",
      "epoch: 127 of 250\n",
      "training_loss: 0.0072, acc: 0.874, ROC: 0.543, f1: 0.149\n",
      "epoch: 128 of 250\n",
      "training_loss: 0.0047, acc: 0.884, ROC: 0.543, f1: 0.159\n",
      "epoch: 129 of 250\n",
      "training_loss: 0.0130, acc: 0.890, ROC: 0.571, f1: 0.191\n",
      "epoch: 130 of 250\n",
      "training_loss: 0.0055, acc: 0.872, ROC: 0.581, f1: 0.238\n",
      "epoch: 131 of 250\n",
      "training_loss: 0.0029, acc: 0.887, ROC: 0.557, f1: 0.187\n",
      "epoch: 132 of 250\n",
      "training_loss: 0.0177, acc: 0.884, ROC: 0.530, f1: 0.121\n",
      "epoch: 133 of 250\n",
      "training_loss: 0.0103, acc: 0.869, ROC: 0.553, f1: 0.176\n",
      "epoch: 134 of 250\n",
      "training_loss: 0.0067, acc: 0.848, ROC: 0.533, f1: 0.156\n",
      "epoch: 135 of 250\n",
      "training_loss: 0.0162, acc: 0.864, ROC: 0.558, f1: 0.190\n",
      "epoch: 136 of 250\n",
      "training_loss: 0.0147, acc: 0.870, ROC: 0.562, f1: 0.198\n",
      "epoch: 137 of 250\n",
      "training_loss: 0.0110, acc: 0.874, ROC: 0.509, f1: 0.087\n",
      "epoch: 138 of 250\n",
      "training_loss: 0.0145, acc: 0.871, ROC: 0.565, f1: 0.199\n",
      "epoch: 139 of 250\n",
      "training_loss: 0.0138, acc: 0.896, ROC: 0.573, f1: 0.212\n",
      "epoch: 140 of 250\n",
      "training_loss: 0.0106, acc: 0.872, ROC: 0.553, f1: 0.169\n",
      "epoch: 141 of 250\n",
      "training_loss: 0.0035, acc: 0.859, ROC: 0.503, f1: 0.078\n",
      "epoch: 142 of 250\n",
      "training_loss: 0.0046, acc: 0.896, ROC: 0.508, f1: 0.071\n",
      "epoch: 143 of 250\n",
      "training_loss: 0.0149, acc: 0.873, ROC: 0.532, f1: 0.136\n",
      "epoch: 144 of 250\n",
      "training_loss: 0.0079, acc: 0.858, ROC: 0.523, f1: 0.123\n",
      "epoch: 145 of 250\n",
      "training_loss: 0.0089, acc: 0.890, ROC: 0.532, f1: 0.127\n",
      "epoch: 146 of 250\n",
      "training_loss: 0.0102, acc: 0.896, ROC: 0.553, f1: 0.175\n",
      "epoch: 147 of 250\n",
      "training_loss: 0.0029, acc: 0.863, ROC: 0.514, f1: 0.105\n",
      "epoch: 148 of 250\n",
      "training_loss: 0.0072, acc: 0.867, ROC: 0.527, f1: 0.131\n",
      "epoch: 149 of 250\n",
      "training_loss: 0.0103, acc: 0.870, ROC: 0.526, f1: 0.122\n",
      "epoch: 150 of 250\n",
      "training_loss: 0.0103, acc: 0.873, ROC: 0.557, f1: 0.181\n",
      "epoch: 151 of 250\n",
      "training_loss: 0.0051, acc: 0.864, ROC: 0.545, f1: 0.160\n",
      "epoch: 152 of 250\n",
      "training_loss: 0.0058, acc: 0.894, ROC: 0.521, f1: 0.102\n",
      "epoch: 153 of 250\n",
      "training_loss: 0.0102, acc: 0.884, ROC: 0.515, f1: 0.094\n",
      "epoch: 154 of 250\n",
      "training_loss: 0.0139, acc: 0.866, ROC: 0.536, f1: 0.152\n",
      "epoch: 155 of 250\n",
      "training_loss: 0.0049, acc: 0.858, ROC: 0.567, f1: 0.193\n",
      "epoch: 156 of 250\n",
      "training_loss: 0.0088, acc: 0.901, ROC: 0.550, f1: 0.168\n",
      "epoch: 157 of 250\n",
      "training_loss: 0.0087, acc: 0.877, ROC: 0.518, f1: 0.102\n",
      "epoch: 158 of 250\n",
      "training_loss: 0.0060, acc: 0.867, ROC: 0.524, f1: 0.119\n",
      "epoch: 159 of 250\n",
      "training_loss: 0.0091, acc: 0.874, ROC: 0.532, f1: 0.137\n",
      "epoch: 160 of 250\n",
      "training_loss: 0.0081, acc: 0.867, ROC: 0.522, f1: 0.119\n",
      "epoch: 161 of 250\n",
      "training_loss: 0.0072, acc: 0.878, ROC: 0.542, f1: 0.153\n",
      "epoch: 162 of 250\n",
      "training_loss: 0.0026, acc: 0.893, ROC: 0.555, f1: 0.171\n",
      "epoch: 163 of 250\n",
      "training_loss: 0.0095, acc: 0.876, ROC: 0.577, f1: 0.215\n",
      "epoch: 164 of 250\n",
      "training_loss: 0.0051, acc: 0.864, ROC: 0.539, f1: 0.139\n",
      "epoch: 165 of 250\n",
      "training_loss: 0.0064, acc: 0.866, ROC: 0.552, f1: 0.173\n",
      "epoch: 166 of 250\n",
      "training_loss: 0.0060, acc: 0.879, ROC: 0.535, f1: 0.142\n",
      "epoch: 167 of 250\n",
      "training_loss: 0.0223, acc: 0.857, ROC: 0.584, f1: 0.235\n",
      "epoch: 168 of 250\n",
      "training_loss: 0.0074, acc: 0.862, ROC: 0.538, f1: 0.148\n",
      "epoch: 169 of 250\n",
      "training_loss: 0.0045, acc: 0.878, ROC: 0.533, f1: 0.129\n",
      "epoch: 170 of 250\n",
      "training_loss: 0.0110, acc: 0.865, ROC: 0.513, f1: 0.094\n",
      "epoch: 171 of 250\n",
      "training_loss: 0.0056, acc: 0.875, ROC: 0.522, f1: 0.113\n",
      "epoch: 172 of 250\n",
      "training_loss: 0.0074, acc: 0.870, ROC: 0.540, f1: 0.145\n",
      "epoch: 173 of 250\n",
      "training_loss: 0.0018, acc: 0.860, ROC: 0.547, f1: 0.167\n",
      "epoch: 174 of 250\n",
      "training_loss: 0.0070, acc: 0.877, ROC: 0.512, f1: 0.089\n",
      "epoch: 175 of 250\n",
      "training_loss: 0.0046, acc: 0.859, ROC: 0.559, f1: 0.175\n",
      "epoch: 176 of 250\n",
      "training_loss: 0.0046, acc: 0.875, ROC: 0.534, f1: 0.126\n",
      "epoch: 177 of 250\n",
      "training_loss: 0.0071, acc: 0.880, ROC: 0.568, f1: 0.178\n",
      "epoch: 178 of 250\n",
      "training_loss: 0.0054, acc: 0.871, ROC: 0.536, f1: 0.146\n",
      "epoch: 179 of 250\n",
      "training_loss: 0.0086, acc: 0.869, ROC: 0.538, f1: 0.155\n",
      "epoch: 180 of 250\n",
      "training_loss: 0.0016, acc: 0.882, ROC: 0.555, f1: 0.181\n",
      "epoch: 181 of 250\n",
      "training_loss: 0.0058, acc: 0.888, ROC: 0.562, f1: 0.176\n",
      "epoch: 182 of 250\n",
      "training_loss: 0.0046, acc: 0.878, ROC: 0.529, f1: 0.129\n",
      "epoch: 183 of 250\n",
      "training_loss: 0.0052, acc: 0.862, ROC: 0.515, f1: 0.104\n",
      "epoch: 184 of 250\n",
      "training_loss: 0.0072, acc: 0.869, ROC: 0.540, f1: 0.155\n",
      "epoch: 185 of 250\n",
      "training_loss: 0.0127, acc: 0.863, ROC: 0.514, f1: 0.105\n",
      "epoch: 186 of 250\n",
      "training_loss: 0.0032, acc: 0.889, ROC: 0.542, f1: 0.153\n",
      "epoch: 187 of 250\n",
      "training_loss: 0.0021, acc: 0.878, ROC: 0.553, f1: 0.176\n",
      "epoch: 188 of 250\n",
      "training_loss: 0.0041, acc: 0.875, ROC: 0.544, f1: 0.161\n",
      "epoch: 189 of 250\n",
      "training_loss: 0.0052, acc: 0.864, ROC: 0.512, f1: 0.093\n",
      "epoch: 190 of 250\n",
      "training_loss: 0.0127, acc: 0.883, ROC: 0.553, f1: 0.170\n",
      "epoch: 191 of 250\n",
      "training_loss: 0.0101, acc: 0.862, ROC: 0.496, f1: 0.068\n",
      "epoch: 192 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.0061, acc: 0.863, ROC: 0.548, f1: 0.180\n",
      "epoch: 193 of 250\n",
      "training_loss: 0.0082, acc: 0.879, ROC: 0.526, f1: 0.117\n",
      "epoch: 194 of 250\n",
      "training_loss: 0.0024, acc: 0.895, ROC: 0.534, f1: 0.132\n",
      "epoch: 195 of 250\n",
      "training_loss: 0.0047, acc: 0.884, ROC: 0.533, f1: 0.134\n",
      "epoch: 196 of 250\n",
      "training_loss: 0.0044, acc: 0.869, ROC: 0.540, f1: 0.155\n",
      "epoch: 197 of 250\n",
      "training_loss: 0.0054, acc: 0.875, ROC: 0.522, f1: 0.113\n",
      "epoch: 198 of 250\n",
      "training_loss: 0.0011, acc: 0.883, ROC: 0.527, f1: 0.120\n",
      "epoch: 199 of 250\n",
      "training_loss: 0.0127, acc: 0.889, ROC: 0.545, f1: 0.153\n",
      "epoch: 200 of 250\n",
      "training_loss: 0.0108, acc: 0.869, ROC: 0.534, f1: 0.144\n",
      "epoch: 201 of 250\n",
      "training_loss: 0.0148, acc: 0.872, ROC: 0.554, f1: 0.179\n",
      "epoch: 202 of 250\n",
      "training_loss: 0.0029, acc: 0.837, ROC: 0.552, f1: 0.189\n",
      "epoch: 203 of 250\n",
      "training_loss: 0.0086, acc: 0.869, ROC: 0.551, f1: 0.166\n",
      "epoch: 204 of 250\n",
      "training_loss: 0.0074, acc: 0.894, ROC: 0.559, f1: 0.185\n",
      "epoch: 205 of 250\n",
      "training_loss: 0.0066, acc: 0.853, ROC: 0.585, f1: 0.230\n",
      "epoch: 206 of 250\n",
      "training_loss: 0.0056, acc: 0.878, ROC: 0.505, f1: 0.076\n",
      "epoch: 207 of 250\n",
      "training_loss: 0.0116, acc: 0.868, ROC: 0.570, f1: 0.214\n",
      "epoch: 208 of 250\n",
      "training_loss: 0.0030, acc: 0.885, ROC: 0.589, f1: 0.228\n",
      "epoch: 209 of 250\n",
      "training_loss: 0.0051, acc: 0.862, ROC: 0.537, f1: 0.148\n",
      "epoch: 210 of 250\n",
      "training_loss: 0.0016, acc: 0.868, ROC: 0.523, f1: 0.120\n",
      "epoch: 211 of 250\n",
      "training_loss: 0.0057, acc: 0.888, ROC: 0.547, f1: 0.164\n",
      "epoch: 212 of 250\n",
      "training_loss: 0.0087, acc: 0.873, ROC: 0.540, f1: 0.159\n",
      "epoch: 213 of 250\n",
      "training_loss: 0.0074, acc: 0.884, ROC: 0.547, f1: 0.159\n",
      "epoch: 214 of 250\n",
      "training_loss: 0.0114, acc: 0.878, ROC: 0.557, f1: 0.187\n",
      "epoch: 215 of 250\n",
      "training_loss: 0.0041, acc: 0.892, ROC: 0.549, f1: 0.169\n",
      "epoch: 216 of 250\n",
      "training_loss: 0.0101, acc: 0.867, ROC: 0.523, f1: 0.119\n",
      "epoch: 217 of 250\n",
      "training_loss: 0.0084, acc: 0.878, ROC: 0.543, f1: 0.153\n",
      "epoch: 218 of 250\n",
      "training_loss: 0.0041, acc: 0.878, ROC: 0.556, f1: 0.187\n",
      "epoch: 219 of 250\n",
      "training_loss: 0.0037, acc: 0.868, ROC: 0.553, f1: 0.175\n",
      "epoch: 220 of 250\n",
      "training_loss: 0.0049, acc: 0.891, ROC: 0.545, f1: 0.155\n",
      "epoch: 221 of 250\n",
      "training_loss: 0.0081, acc: 0.886, ROC: 0.575, f1: 0.219\n",
      "epoch: 222 of 250\n",
      "training_loss: 0.0163, acc: 0.844, ROC: 0.504, f1: 0.093\n",
      "epoch: 223 of 250\n",
      "training_loss: 0.0030, acc: 0.879, ROC: 0.557, f1: 0.199\n",
      "epoch: 224 of 250\n",
      "training_loss: 0.0088, acc: 0.871, ROC: 0.541, f1: 0.134\n",
      "epoch: 225 of 250\n",
      "training_loss: 0.0022, acc: 0.870, ROC: 0.563, f1: 0.198\n",
      "epoch: 226 of 250\n",
      "training_loss: 0.0033, acc: 0.890, ROC: 0.578, f1: 0.191\n",
      "epoch: 227 of 250\n",
      "training_loss: 0.0058, acc: 0.843, ROC: 0.530, f1: 0.142\n",
      "epoch: 228 of 250\n",
      "training_loss: 0.0093, acc: 0.904, ROC: 0.599, f1: 0.273\n",
      "epoch: 229 of 250\n",
      "training_loss: 0.0051, acc: 0.855, ROC: 0.529, f1: 0.132\n",
      "epoch: 230 of 250\n",
      "training_loss: 0.0023, acc: 0.889, ROC: 0.549, f1: 0.165\n",
      "epoch: 231 of 250\n",
      "training_loss: 0.0038, acc: 0.880, ROC: 0.538, f1: 0.143\n",
      "epoch: 232 of 250\n",
      "training_loss: 0.0424, acc: 0.862, ROC: 0.497, f1: 0.068\n",
      "epoch: 233 of 250\n",
      "training_loss: 0.0011, acc: 0.876, ROC: 0.540, f1: 0.151\n",
      "epoch: 234 of 250\n",
      "training_loss: 0.0024, acc: 0.878, ROC: 0.553, f1: 0.176\n",
      "epoch: 235 of 250\n",
      "training_loss: 0.0155, acc: 0.886, ROC: 0.559, f1: 0.186\n",
      "epoch: 236 of 250\n",
      "training_loss: 0.0031, acc: 0.814, ROC: 0.530, f1: 0.155\n",
      "epoch: 237 of 250\n",
      "training_loss: 0.0075, acc: 0.873, ROC: 0.559, f1: 0.191\n",
      "epoch: 238 of 250\n",
      "training_loss: 0.0057, acc: 0.856, ROC: 0.528, f1: 0.133\n",
      "epoch: 239 of 250\n",
      "training_loss: 0.0113, acc: 0.896, ROC: 0.592, f1: 0.246\n",
      "epoch: 240 of 250\n",
      "training_loss: 0.0034, acc: 0.874, ROC: 0.537, f1: 0.149\n",
      "epoch: 241 of 250\n",
      "training_loss: 0.0060, acc: 0.888, ROC: 0.541, f1: 0.138\n",
      "epoch: 242 of 250\n",
      "training_loss: 0.0068, acc: 0.888, ROC: 0.556, f1: 0.176\n",
      "epoch: 243 of 250\n",
      "training_loss: 0.0164, acc: 0.892, ROC: 0.545, f1: 0.156\n",
      "epoch: 244 of 250\n",
      "training_loss: 0.0019, acc: 0.878, ROC: 0.527, f1: 0.116\n",
      "epoch: 245 of 250\n",
      "training_loss: 0.0170, acc: 0.877, ROC: 0.589, f1: 0.245\n",
      "epoch: 246 of 250\n",
      "training_loss: 0.0020, acc: 0.859, ROC: 0.507, f1: 0.090\n",
      "epoch: 247 of 250\n",
      "training_loss: 0.0029, acc: 0.870, ROC: 0.502, f1: 0.071\n",
      "epoch: 248 of 250\n",
      "training_loss: 0.0122, acc: 0.897, ROC: 0.543, f1: 0.149\n",
      "epoch: 249 of 250\n",
      "training_loss: 0.0036, acc: 0.869, ROC: 0.528, f1: 0.132\n",
      "(19410, 39, 11, 11)\n",
      "epoch: 0 of 250\n",
      "training_loss: 0.5743, acc: 0.759, ROC: 0.554, f1: 0.199\n",
      "epoch: 1 of 250\n",
      "training_loss: 0.5033, acc: 0.743, ROC: 0.591, f1: 0.199\n",
      "epoch: 2 of 250\n",
      "training_loss: 0.4680, acc: 0.663, ROC: 0.549, f1: 0.192\n",
      "epoch: 3 of 250\n",
      "training_loss: 0.3886, acc: 0.835, ROC: 0.564, f1: 0.179\n",
      "epoch: 4 of 250\n",
      "training_loss: 0.4015, acc: 0.736, ROC: 0.579, f1: 0.195\n",
      "epoch: 5 of 250\n",
      "training_loss: 0.3700, acc: 0.836, ROC: 0.557, f1: 0.196\n",
      "epoch: 6 of 250\n",
      "training_loss: 0.3408, acc: 0.839, ROC: 0.523, f1: 0.130\n",
      "epoch: 7 of 250\n",
      "training_loss: 0.3012, acc: 0.862, ROC: 0.524, f1: 0.127\n",
      "epoch: 8 of 250\n",
      "training_loss: 0.2647, acc: 0.831, ROC: 0.570, f1: 0.214\n",
      "epoch: 9 of 250\n",
      "training_loss: 0.2674, acc: 0.661, ROC: 0.623, f1: 0.206\n",
      "epoch: 10 of 250\n",
      "training_loss: 0.2327, acc: 0.761, ROC: 0.613, f1: 0.282\n",
      "epoch: 11 of 250\n",
      "training_loss: 0.2118, acc: 0.751, ROC: 0.611, f1: 0.234\n",
      "epoch: 12 of 250\n",
      "training_loss: 0.1967, acc: 0.832, ROC: 0.546, f1: 0.143\n",
      "epoch: 13 of 250\n",
      "training_loss: 0.1742, acc: 0.797, ROC: 0.558, f1: 0.171\n",
      "epoch: 14 of 250\n",
      "training_loss: 0.1784, acc: 0.858, ROC: 0.540, f1: 0.155\n",
      "epoch: 15 of 250\n",
      "training_loss: 0.1437, acc: 0.819, ROC: 0.584, f1: 0.223\n",
      "epoch: 16 of 250\n",
      "training_loss: 0.1304, acc: 0.861, ROC: 0.509, f1: 0.092\n",
      "epoch: 17 of 250\n",
      "training_loss: 0.1143, acc: 0.831, ROC: 0.569, f1: 0.207\n",
      "epoch: 18 of 250\n",
      "training_loss: 0.1058, acc: 0.847, ROC: 0.545, f1: 0.182\n",
      "epoch: 19 of 250\n",
      "training_loss: 0.0973, acc: 0.888, ROC: 0.553, f1: 0.176\n",
      "epoch: 20 of 250\n",
      "training_loss: 0.0880, acc: 0.882, ROC: 0.552, f1: 0.181\n",
      "epoch: 21 of 250\n",
      "training_loss: 0.0772, acc: 0.810, ROC: 0.519, f1: 0.128\n",
      "epoch: 22 of 250\n",
      "training_loss: 0.0689, acc: 0.850, ROC: 0.512, f1: 0.107\n",
      "epoch: 23 of 250\n",
      "training_loss: 0.0752, acc: 0.885, ROC: 0.556, f1: 0.196\n",
      "epoch: 24 of 250\n",
      "training_loss: 0.0519, acc: 0.878, ROC: 0.536, f1: 0.141\n",
      "epoch: 25 of 250\n",
      "training_loss: 0.0563, acc: 0.871, ROC: 0.527, f1: 0.122\n",
      "epoch: 26 of 250\n",
      "training_loss: 0.0513, acc: 0.866, ROC: 0.530, f1: 0.141\n",
      "epoch: 27 of 250\n",
      "training_loss: 0.0558, acc: 0.818, ROC: 0.573, f1: 0.222\n",
      "epoch: 28 of 250\n",
      "training_loss: 0.0387, acc: 0.804, ROC: 0.557, f1: 0.210\n",
      "epoch: 29 of 250\n",
      "training_loss: 0.0557, acc: 0.879, ROC: 0.594, f1: 0.239\n",
      "epoch: 30 of 250\n",
      "training_loss: 0.0369, acc: 0.841, ROC: 0.551, f1: 0.185\n",
      "epoch: 31 of 250\n",
      "training_loss: 0.0324, acc: 0.867, ROC: 0.541, f1: 0.164\n",
      "epoch: 32 of 250\n",
      "training_loss: 0.0330, acc: 0.887, ROC: 0.548, f1: 0.175\n",
      "epoch: 33 of 250\n",
      "training_loss: 0.0298, acc: 0.855, ROC: 0.524, f1: 0.132\n",
      "epoch: 34 of 250\n",
      "training_loss: 0.0282, acc: 0.870, ROC: 0.518, f1: 0.110\n",
      "epoch: 35 of 250\n",
      "training_loss: 0.0198, acc: 0.848, ROC: 0.564, f1: 0.216\n",
      "epoch: 36 of 250\n",
      "training_loss: 0.0218, acc: 0.868, ROC: 0.574, f1: 0.214\n",
      "epoch: 37 of 250\n",
      "training_loss: 0.0196, acc: 0.867, ROC: 0.562, f1: 0.204\n",
      "epoch: 38 of 250\n",
      "training_loss: 0.0192, acc: 0.855, ROC: 0.565, f1: 0.208\n",
      "epoch: 39 of 250\n",
      "training_loss: 0.0201, acc: 0.851, ROC: 0.566, f1: 0.186\n",
      "epoch: 40 of 250\n",
      "training_loss: 0.0280, acc: 0.853, ROC: 0.544, f1: 0.169\n",
      "epoch: 41 of 250\n",
      "training_loss: 0.0184, acc: 0.886, ROC: 0.573, f1: 0.219\n",
      "epoch: 42 of 250\n",
      "training_loss: 0.0141, acc: 0.842, ROC: 0.563, f1: 0.177\n",
      "epoch: 43 of 250\n",
      "training_loss: 0.0184, acc: 0.873, ROC: 0.522, f1: 0.112\n",
      "epoch: 44 of 250\n",
      "training_loss: 0.0194, acc: 0.843, ROC: 0.513, f1: 0.113\n",
      "epoch: 45 of 250\n",
      "training_loss: 0.0117, acc: 0.857, ROC: 0.553, f1: 0.183\n",
      "epoch: 46 of 250\n",
      "training_loss: 0.0160, acc: 0.856, ROC: 0.550, f1: 0.172\n",
      "epoch: 47 of 250\n",
      "training_loss: 0.0163, acc: 0.845, ROC: 0.534, f1: 0.153\n",
      "epoch: 48 of 250\n",
      "training_loss: 0.0159, acc: 0.853, ROC: 0.538, f1: 0.160\n",
      "epoch: 49 of 250\n",
      "training_loss: 0.0116, acc: 0.873, ROC: 0.568, f1: 0.191\n",
      "epoch: 50 of 250\n",
      "training_loss: 0.0088, acc: 0.862, ROC: 0.519, f1: 0.115\n",
      "epoch: 51 of 250\n",
      "training_loss: 0.0158, acc: 0.850, ROC: 0.533, f1: 0.148\n",
      "epoch: 52 of 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.0158, acc: 0.864, ROC: 0.584, f1: 0.253\n",
      "epoch: 53 of 250\n",
      "training_loss: 0.0117, acc: 0.859, ROC: 0.549, f1: 0.185\n",
      "epoch: 54 of 250\n",
      "training_loss: 0.0156, acc: 0.866, ROC: 0.531, f1: 0.141\n",
      "epoch: 55 of 250\n",
      "training_loss: 0.0107, acc: 0.849, ROC: 0.527, f1: 0.137\n",
      "epoch: 56 of 250\n",
      "training_loss: 0.0129, acc: 0.868, ROC: 0.543, f1: 0.165\n",
      "epoch: 57 of 250\n",
      "training_loss: 0.0332, acc: 0.793, ROC: 0.600, f1: 0.225\n",
      "epoch: 58 of 250\n",
      "training_loss: 0.0142, acc: 0.858, ROC: 0.562, f1: 0.211\n",
      "epoch: 59 of 250\n",
      "training_loss: 0.0168, acc: 0.864, ROC: 0.552, f1: 0.181\n",
      "epoch: 60 of 250\n",
      "training_loss: 0.0058, acc: 0.857, ROC: 0.536, f1: 0.154\n",
      "epoch: 61 of 250\n",
      "training_loss: 0.0095, acc: 0.857, ROC: 0.551, f1: 0.183\n",
      "epoch: 62 of 250\n",
      "training_loss: 0.0099, acc: 0.859, ROC: 0.558, f1: 0.203\n",
      "epoch: 63 of 250\n",
      "training_loss: 0.0161, acc: 0.854, ROC: 0.527, f1: 0.131\n",
      "epoch: 64 of 250\n",
      "training_loss: 0.0072, acc: 0.859, ROC: 0.553, f1: 0.185\n",
      "epoch: 65 of 250\n",
      "training_loss: 0.0105, acc: 0.859, ROC: 0.576, f1: 0.221\n",
      "epoch: 66 of 250\n",
      "training_loss: 0.0196, acc: 0.884, ROC: 0.583, f1: 0.256\n",
      "epoch: 67 of 250\n",
      "training_loss: 0.0167, acc: 0.878, ROC: 0.555, f1: 0.187\n",
      "epoch: 68 of 250\n",
      "training_loss: 0.0093, acc: 0.867, ROC: 0.546, f1: 0.164\n",
      "epoch: 69 of 250\n",
      "training_loss: 0.0069, acc: 0.869, ROC: 0.576, f1: 0.216\n",
      "epoch: 70 of 250\n",
      "training_loss: 0.0078, acc: 0.861, ROC: 0.552, f1: 0.178\n",
      "epoch: 71 of 250\n",
      "training_loss: 0.0054, acc: 0.861, ROC: 0.563, f1: 0.197\n",
      "epoch: 72 of 250\n",
      "training_loss: 0.0055, acc: 0.875, ROC: 0.536, f1: 0.150\n",
      "epoch: 73 of 250\n",
      "training_loss: 0.0067, acc: 0.879, ROC: 0.549, f1: 0.166\n",
      "epoch: 74 of 250\n",
      "training_loss: 0.0126, acc: 0.869, ROC: 0.561, f1: 0.196\n",
      "epoch: 75 of 250\n",
      "training_loss: 0.0087, acc: 0.862, ROC: 0.549, f1: 0.169\n",
      "epoch: 76 of 250\n",
      "training_loss: 0.0064, acc: 0.842, ROC: 0.555, f1: 0.194\n",
      "epoch: 77 of 250\n",
      "training_loss: 0.0096, acc: 0.880, ROC: 0.525, f1: 0.118\n",
      "epoch: 78 of 250\n",
      "training_loss: 0.0075, acc: 0.867, ROC: 0.582, f1: 0.222\n",
      "epoch: 79 of 250\n",
      "training_loss: 0.0094, acc: 0.867, ROC: 0.563, f1: 0.204\n",
      "epoch: 80 of 250\n",
      "training_loss: 0.0142, acc: 0.843, ROC: 0.567, f1: 0.195\n",
      "epoch: 81 of 250\n",
      "training_loss: 0.0083, acc: 0.868, ROC: 0.517, f1: 0.108\n",
      "epoch: 82 of 250\n",
      "training_loss: 0.0077, acc: 0.833, ROC: 0.581, f1: 0.237\n",
      "epoch: 83 of 250\n",
      "training_loss: 0.0176, acc: 0.858, ROC: 0.584, f1: 0.245\n",
      "epoch: 84 of 250\n",
      "training_loss: 0.0133, acc: 0.899, ROC: 0.570, f1: 0.217\n",
      "epoch: 85 of 250\n",
      "training_loss: 0.0035, acc: 0.820, ROC: 0.553, f1: 0.204\n",
      "epoch: 86 of 250\n",
      "training_loss: 0.0091, acc: 0.871, ROC: 0.529, f1: 0.134\n",
      "epoch: 87 of 250\n",
      "training_loss: 0.0065, acc: 0.849, ROC: 0.542, f1: 0.156\n",
      "epoch: 88 of 250\n",
      "training_loss: 0.0101, acc: 0.855, ROC: 0.575, f1: 0.199\n",
      "epoch: 89 of 250\n",
      "training_loss: 0.0073, acc: 0.848, ROC: 0.575, f1: 0.240\n",
      "epoch: 90 of 250\n",
      "training_loss: 0.0041, acc: 0.861, ROC: 0.601, f1: 0.280\n",
      "epoch: 91 of 250\n",
      "training_loss: 0.0066, acc: 0.843, ROC: 0.532, f1: 0.151\n",
      "epoch: 92 of 250\n",
      "training_loss: 0.0065, acc: 0.818, ROC: 0.563, f1: 0.209\n",
      "epoch: 93 of 250\n",
      "training_loss: 0.0104, acc: 0.859, ROC: 0.517, f1: 0.113\n",
      "epoch: 94 of 250\n",
      "training_loss: 0.0128, acc: 0.870, ROC: 0.580, f1: 0.226\n",
      "epoch: 95 of 250\n",
      "training_loss: 0.0038, acc: 0.867, ROC: 0.533, f1: 0.131\n",
      "epoch: 96 of 250\n",
      "training_loss: 0.0065, acc: 0.870, ROC: 0.540, f1: 0.156\n",
      "epoch: 97 of 250\n",
      "training_loss: 0.0025, acc: 0.856, ROC: 0.545, f1: 0.172\n",
      "epoch: 98 of 250\n",
      "training_loss: 0.0042, acc: 0.839, ROC: 0.562, f1: 0.191\n",
      "epoch: 99 of 250\n",
      "training_loss: 0.0058, acc: 0.845, ROC: 0.553, f1: 0.180\n",
      "epoch: 100 of 250\n",
      "training_loss: 0.0069, acc: 0.858, ROC: 0.610, f1: 0.276\n",
      "epoch: 101 of 250\n",
      "training_loss: 0.0020, acc: 0.830, ROC: 0.498, f1: 0.086\n",
      "epoch: 102 of 250\n",
      "training_loss: 0.0081, acc: 0.912, ROC: 0.564, f1: 0.185\n",
      "epoch: 103 of 250\n",
      "training_loss: 0.0068, acc: 0.866, ROC: 0.526, f1: 0.130\n",
      "epoch: 104 of 250\n",
      "training_loss: 0.0169, acc: 0.847, ROC: 0.534, f1: 0.155\n",
      "epoch: 105 of 250\n",
      "training_loss: 0.0109, acc: 0.844, ROC: 0.565, f1: 0.212\n",
      "epoch: 106 of 250\n",
      "training_loss: 0.0095, acc: 0.870, ROC: 0.580, f1: 0.226\n",
      "epoch: 107 of 250\n",
      "training_loss: 0.0054, acc: 0.853, ROC: 0.525, f1: 0.130\n",
      "epoch: 108 of 250\n",
      "training_loss: 0.0050, acc: 0.838, ROC: 0.567, f1: 0.221\n",
      "epoch: 109 of 250\n",
      "training_loss: 0.0072, acc: 0.863, ROC: 0.527, f1: 0.127\n",
      "epoch: 110 of 250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "for clusters in [3, 5, 7, 9, 11]:\n",
    "    for i, mod in enumerate(models):\n",
    "        \n",
    "        cluster_model = KMeans(n_clusters=clusters, random_state=0)\n",
    "        labels15 = pd.DataFrame(cluster_model.fit_predict(X15))\n",
    "        labels16 = pd.DataFrame(cluster_model.predict(X16))\n",
    "\n",
    "        labels15[\"X\"] = X15.index\n",
    "        labels16[\"X\"] = X16.index\n",
    "\n",
    "\n",
    "        labels15 = labels15.set_index(0)\n",
    "        labels16 = labels16.set_index(0)\n",
    "        \n",
    "        mods = {}\n",
    "        for i in labels15.index.unique():\n",
    "            model = mod(str(i))\n",
    "            x = X_train[labels15.loc[i][\"X\"]]\n",
    "            y = Y_train[labels15.loc[i][\"X\"]]\n",
    "            \n",
    "            x_val = X_val[labels16.loc[i][\"X\"]]\n",
    "            y_val = Y_val[labels16.loc[i][\"X\"]]\n",
    "            print(x.shape)\n",
    "            oversample = SMOTE()\n",
    "            x, y = oversample.fit_resample(x.reshape(x.shape[0], -1), y)\n",
    "            x = x.reshape(-1, 39, 11, 11)\n",
    "\n",
    "            \n",
    "            hists = train_model(model, x, y, x_val, y_val, 250)\n",
    "            mods[i] = model\n",
    "        print(mods)\n",
    "            \n",
    "        n_function = neighbor_part(mods, X_train, ID_train, wt, labels15)\n",
    "\n",
    "        oversample = SMOTE()\n",
    "        x, y = oversample.fit_resample(n_function, Y_train)\n",
    "\n",
    "        clf_bagger = RandomForestClassifier(max_depth = 12, oob_score = True)\n",
    "        clf_bagger.fit(x,y)\n",
    "\n",
    "        n_function = neighbor_part(mods, X_val, ID_val, wv, labels16)\n",
    "        preds = clf_bagger.predict(n_function)\n",
    "        totacc = accuracy_score(Y_val, preds)\n",
    "        totf1 = f1_score(Y_val, preds)\n",
    "        totROC = roc_auc_score(Y_val, preds)\n",
    "\n",
    "\n",
    "        with open(\"../results/CNN/model\" + str(i) +\"_\"+ str(clusters) + \"cluster.csv\", \"a+\") as f:\n",
    "            f.write(\"loss;acc;ROC;f1_score\\n\")\n",
    "            f.write(str(hists[2]) + \";\" + str(hists[0]) + \";\" + str(hists[1]) + \";\" + str(hists[3]) + \"\\n\")\n",
    "            f.write(\"--;\" + str(totacc) + \";\" + str(totROC) + \";\" + str(totf1))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a06f425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(39, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Flatten()\n",
      "    (8): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (9): ReLU()\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mods[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3768ab",
   "metadata": {},
   "source": [
    "# Neighbor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fc9de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "import libpysal\n",
    "\n",
    "def load_data(year):\n",
    "    os.getcwd()\n",
    "    df = pd.DataFrame()\n",
    "    path = \"../Data/filled/\" + str(year) + \"/\"\n",
    "    for filename in os.listdir(path):\n",
    "        df1 = pd.read_csv(path + filename)\n",
    "        if df1.geometry.isna().any():\n",
    "            print(filename)\n",
    "        df = pd.concat([df, df1])\n",
    "    \n",
    "    df = gpd.GeoDataFrame(df)\n",
    "    df.geometry = df.geometry.apply(shapely.wkt.loads)\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.drop([\"Unnamed: 0\", \"index\"], axis = 1)\n",
    "    return df\n",
    "\n",
    "df15 = load_data(2015)\n",
    "df16 = load_data(2016)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X15 = df15[df15.columns[:-4]]\n",
    "Y15 = df15.y\n",
    "\n",
    "X16 = df16[df16.columns[:-4]]\n",
    "Y16 = df16.y\n",
    "\n",
    "X15 = pd.DataFrame(scaler.fit_transform(X15))\n",
    "X16 = pd.DataFrame(scaler.transform(X16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7edf9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df15 = load_data(2015)\n",
    "df16 = load_data(2016)\n",
    "wt = libpysal.weights.DistanceBand.from_dataframe(df15, threshold=150, binary = True, silence_warnings = True)\n",
    "wv = libpysal.weights.DistanceBand.from_dataframe(df16, threshold=150, binary = True, silence_warnings = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4589a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X15 = df15[df15.columns[:-4]]\n",
    "Y15 = df15.y\n",
    "\n",
    "X16 = df16[df16.columns[:-4]]\n",
    "Y16 = df16.y\n",
    "\n",
    "X15 = pd.DataFrame(scaler.fit_transform(X15))\n",
    "X16 = pd.DataFrame(scaler.transform(X16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a124afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = KMeans(n_clusters=5, random_state=0)\n",
    "labels15 = pd.DataFrame(cluster_model.fit_predict(X15))\n",
    "labels16 = pd.DataFrame(cluster_model.predict(X16))\n",
    "\n",
    "labels15[\"X\"] = X15.index\n",
    "labels16[\"X\"] = X16.index\n",
    "\n",
    "\n",
    "labels15 = labels15.set_index(0)\n",
    "labels16 = labels16.set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0faebf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cbs_id_koppel.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c567c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# br.C28992R100 = br.C28992R100.map(b) # change C28992code for id\n",
    "labels15.C28 = labels15.C28.map(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9745a58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>C28</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2748208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2748211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2748212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2748245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2748263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56362</td>\n",
       "      <td>1413137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56363</td>\n",
       "      <td>1413181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56364</td>\n",
       "      <td>1414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56365</td>\n",
       "      <td>1414808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56366</td>\n",
       "      <td>1416469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56367 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X      C28\n",
       "0                 \n",
       "0       0  2748208\n",
       "0       1  2748211\n",
       "0       2  2748212\n",
       "0       3  2748245\n",
       "0       4  2748263\n",
       "..    ...      ...\n",
       "0   56362  1413137\n",
       "0   56363  1413181\n",
       "0   56364  1414800\n",
       "0   56365  1414808\n",
       "0   56366  1416469\n",
       "\n",
       "[56367 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a559bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56367, 39, 11, 11)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80e2c76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2893058., 2870455., 2868833., ..., 1311910., 1176428., 1314791.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b0a18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preds = pd.DataFrame(np.array(preds.detach())).set_index(ID_train)\n",
    "neighbors = [w15.neighbors[x] for x in ID_train]\n",
    "transitions = [preds.loc[x].values for x in neighbors]\n",
    "\n",
    "\n",
    "\n",
    "n_function = np.zeros((len(preds), w15.max_neighbors + 1))\n",
    "for i, (t, idx) in enumerate(zip(transitions, ID_train)):\n",
    "    n_function[i, 1:len(t) + 1] = t.squeeze()\n",
    "    n_function[i, 0] = preds.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f78e226c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5] END .......max_depth=6, oob_score=True;, score=0.641 total time=   4.8s\n",
      "[CV 2/5] END .......max_depth=6, oob_score=True;, score=0.649 total time=   4.6s\n",
      "[CV 3/5] END .......max_depth=6, oob_score=True;, score=0.663 total time=   4.7s\n",
      "[CV 4/5] END .......max_depth=6, oob_score=True;, score=0.619 total time=   4.8s\n",
      "[CV 5/5] END .......max_depth=6, oob_score=True;, score=0.681 total time=   4.7s\n",
      "[CV 1/5] END ......max_depth=6, oob_score=False;, score=0.639 total time=   4.1s\n",
      "[CV 2/5] END ......max_depth=6, oob_score=False;, score=0.652 total time=   4.3s\n",
      "[CV 3/5] END ......max_depth=6, oob_score=False;, score=0.663 total time=   4.0s\n",
      "[CV 4/5] END ......max_depth=6, oob_score=False;, score=0.619 total time=   3.9s\n",
      "[CV 5/5] END ......max_depth=6, oob_score=False;, score=0.682 total time=   4.4s\n",
      "[CV 1/5] END .......max_depth=8, oob_score=True;, score=0.647 total time=   5.4s\n",
      "[CV 2/5] END .......max_depth=8, oob_score=True;, score=0.663 total time=   5.5s\n",
      "[CV 3/5] END .......max_depth=8, oob_score=True;, score=0.673 total time=   5.6s\n",
      "[CV 4/5] END .......max_depth=8, oob_score=True;, score=0.632 total time=   5.4s\n",
      "[CV 5/5] END .......max_depth=8, oob_score=True;, score=0.688 total time=   6.6s\n",
      "[CV 1/5] END ......max_depth=8, oob_score=False;, score=0.647 total time=   5.1s\n",
      "[CV 2/5] END ......max_depth=8, oob_score=False;, score=0.663 total time=   5.6s\n",
      "[CV 3/5] END ......max_depth=8, oob_score=False;, score=0.673 total time=   6.3s\n",
      "[CV 4/5] END ......max_depth=8, oob_score=False;, score=0.632 total time=   5.3s\n",
      "[CV 5/5] END ......max_depth=8, oob_score=False;, score=0.688 total time=   5.6s\n",
      "[CV 1/5] END ......max_depth=10, oob_score=True;, score=0.659 total time=   6.8s\n",
      "[CV 2/5] END ......max_depth=10, oob_score=True;, score=0.671 total time=   7.0s\n",
      "[CV 3/5] END ......max_depth=10, oob_score=True;, score=0.682 total time=   6.9s\n",
      "[CV 4/5] END ......max_depth=10, oob_score=True;, score=0.645 total time=   6.8s\n",
      "[CV 5/5] END ......max_depth=10, oob_score=True;, score=0.697 total time=   7.0s\n",
      "[CV 1/5] END .....max_depth=10, oob_score=False;, score=0.657 total time=   6.1s\n",
      "[CV 2/5] END .....max_depth=10, oob_score=False;, score=0.672 total time=   6.0s\n",
      "[CV 3/5] END .....max_depth=10, oob_score=False;, score=0.682 total time=   6.1s\n",
      "[CV 4/5] END .....max_depth=10, oob_score=False;, score=0.645 total time=   6.2s\n",
      "[CV 5/5] END .....max_depth=10, oob_score=False;, score=0.696 total time=   6.2s\n",
      "[CV 1/5] END ......max_depth=12, oob_score=True;, score=0.667 total time=   7.6s\n",
      "[CV 2/5] END ......max_depth=12, oob_score=True;, score=0.681 total time=   7.6s\n",
      "[CV 3/5] END ......max_depth=12, oob_score=True;, score=0.690 total time=   7.5s\n",
      "[CV 4/5] END ......max_depth=12, oob_score=True;, score=0.657 total time=   7.4s\n",
      "[CV 5/5] END ......max_depth=12, oob_score=True;, score=0.703 total time=   7.6s\n",
      "[CV 1/5] END .....max_depth=12, oob_score=False;, score=0.667 total time=   6.8s\n",
      "[CV 2/5] END .....max_depth=12, oob_score=False;, score=0.681 total time=   6.9s\n",
      "[CV 3/5] END .....max_depth=12, oob_score=False;, score=0.691 total time=   6.8s\n",
      "[CV 4/5] END .....max_depth=12, oob_score=False;, score=0.657 total time=   7.1s\n",
      "[CV 5/5] END .....max_depth=12, oob_score=False;, score=0.704 total time=   7.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [6, 8, 10, 12],\n",
       "                         'oob_score': [True, False]},\n",
       "             scoring='balanced_accuracy', verbose=3)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"max_depth\": [6, 8, 10, 12], \"oob_score\" : [True, False]}\n",
    "clf_bagger = GridSearchCV(RandomForestClassifier(), params, cv = 5, scoring = \"balanced_accuracy\",\n",
    "                               verbose = 3)\n",
    "oversample = SMOTE()\n",
    "x, y = oversample.fit_resample(n_function, Y_train)\n",
    "\n",
    "clf_bagger.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "08a83af4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 12, 'oob_score': False}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_bagger.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(preds.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b249875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.from_numpy(X_val).float()\n",
    "preds = model(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "91e2c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w16.pickle', 'rb') as handle:\n",
    "    w16 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49d94624",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(np.array(preds.detach())).set_index(ID_val)\n",
    "neighbors = [w16.neighbors[x] for x in ID_val]\n",
    "transitions = [preds.loc[x].values for x in neighbors]\n",
    "\n",
    "\n",
    "\n",
    "n_function = np.zeros((len(preds), w16.max_neighbors + 1))\n",
    "for i, (t, idx) in enumerate(zip(transitions, ID_val)):\n",
    "    n_function[i, 1:len(t) + 1] = t.squeeze()\n",
    "    n_function[i, 0] = preds.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee759569",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf_bagger.predict(n_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0f66f06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57050"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8c62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71e36a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5448808332705778"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_bagger.score(n_function, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "708da41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22256568778979907"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(Y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d5b6b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1262683201803833"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(Y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "653623ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4529"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Y_val == 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785763ff",
   "metadata": {},
   "source": [
    "# No bagger on the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "efe1d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(np.array(preds.detach())).set_index(ID_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "ba4f5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[\"y\"] = Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "da369a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.sort_values(by=[0], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2288054e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3699708.0</th>\n",
       "      <td>9.999919e-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206980.0</th>\n",
       "      <td>9.999806e-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206983.0</th>\n",
       "      <td>9.999349e-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206982.0</th>\n",
       "      <td>9.999343e-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210563.0</th>\n",
       "      <td>9.999300e-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654130.0</th>\n",
       "      <td>3.897061e-21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113362.0</th>\n",
       "      <td>1.054448e-21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705586.0</th>\n",
       "      <td>2.995088e-22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703981.0</th>\n",
       "      <td>6.478963e-23</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116824.0</th>\n",
       "      <td>9.653986e-25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57050 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0    y\n",
       "3699708.0  9.999919e-01  0.0\n",
       "2206980.0  9.999806e-01  0.0\n",
       "2206983.0  9.999349e-01  0.0\n",
       "2206982.0  9.999343e-01  0.0\n",
       "2210563.0  9.999300e-01  0.0\n",
       "...                 ...  ...\n",
       "2654130.0  3.897061e-21  0.0\n",
       "2113362.0  1.054448e-21  0.0\n",
       "3705586.0  2.995088e-22  0.0\n",
       "3703981.0  6.478963e-23  0.0\n",
       "2116824.0  9.653986e-25  1.0\n",
       "\n",
       "[57050 rows x 2 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "70c91604",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0].iloc[0:4529] = 1\n",
    "preds[0].iloc[4529:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "4426a7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17244424817840584"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(preds[\"y\"], preds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e2e24821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17244424817840584"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(preds[\"y\"], preds[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
